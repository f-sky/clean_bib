
@misc{181110136FilterReg,
  title = {[1811.10136] {{FilterReg}}: {{Robust}} and {{Efficient Probabilistic Point}}-{{Set Registration}} Using {{Gaussian Filter}} and {{Twist Parameterization}}},
  file = {/Users/sunjiaming/Zotero/storage/WL78KZPG/1811.html},
  howpublished = {https://arxiv.org/abs/1811.10136}
}

@misc{181207976MIDFusion,
  title = {[1812.07976] {{MID}}-{{Fusion}}: {{Octree}}-Based {{Object}}-{{Level Multi}}-{{Instance Dynamic SLAM}}},
  file = {/Users/sunjiaming/Zotero/storage/4VMPM2KH/1812.html},
  howpublished = {https://arxiv.org/abs/1812.07976}
}

@misc{190403483SDRSAC,
  title = {[1904.03483] {{SDRSAC}}: {{Semidefinite}}-{{Based Randomized Approach}} for {{Robust Point Cloud Registration}} without {{Correspondences}}},
  file = {/Users/sunjiaming/Zotero/storage/8TEH2H6Z/1904.html},
  howpublished = {https://arxiv.org/abs/1904.03483}
}

@article{aanaesLargeScaleDataMultipleView2016,
  title = {Large-{{Scale Data}} for {{Multiple}}-{{View Stereopsis}}},
  author = {Aan{\ae}s, Henrik and Jensen, Rasmus Ramsb{\o}l and Vogiatzis, George and Tola, Engin and Dahl, Anders Bjorholm},
  year = {2016},
  month = nov,
  volume = {120},
  pages = {153--168},
  issn = {1573-1405},
  doi = {10.1007/s11263-016-0902-9},
  abstract = {The seminal multiple-view stereo benchmark evaluations from Middlebury and by Strecha et al. have played a major role in propelling the development of multi-view stereopsis (MVS) methodology. The somewhat small size and variability of these data sets, however, limit their scope and the conclusions that can be derived from them. To facilitate further development within MVS, we here present a new and varied data set consisting of 80 scenes, seen from 49 or 64 accurate camera positions. This is accompanied by accurate structured light scans for reference and evaluation. In addition all images are taken under seven different lighting conditions. As a benchmark and to validate the use of our data set for obtaining reasonable and statistically significant findings about MVS, we have applied the three state-of-the-art MVS algorithms by Campbell et al., Furukawa et al., and Tola et al. to the data set. To do this we have extended the evaluation protocol from the Middlebury evaluation, necessitated by the more complex geometry of some of our scenes. The data set and accompanying evaluation framework are made freely available online. Based on this evaluation, we are able to observe several characteristics of state-of-the-art MVS, e.g. that there is a tradeoff between the quality of the reconstructed 3D points (accuracy) and how much of an object's surface is captured (completeness). Also, several issues that we hypothesized would challenge MVS, such as specularities and changing lighting conditions did not pose serious problems. Our study finds that the two most pressing issues for MVS are lack of texture and meshing (forming 3D points into closed triangulated surfaces).},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large-Scale Data for Multiple-View Stereopsis-Aanæs et al-2016.pdf},
  journal = {International Journal of Computer Vision},
  keywords = {neufu_paper},
  language = {en},
  number = {2}
}

@article{abbasGeometricApproachObtain2019,
  title = {A {{Geometric Approach}} to {{Obtain}} a {{Bird}}'s {{Eye View}} from an {{Image}}},
  author = {Abbas, Ammar and Zisserman, Andrew},
  year = {2019},
  month = may,
  abstract = {The objective of this paper is to rectify any monocular image by computing a homography matrix that transforms it to a bird's eye (overhead) view. We make the following contributions: (i) we show that the homography matrix can be parameterised with only four parameters that specify the horizon line and the vertical vanishing point, or only two if the field of view or focal length is known; (ii) We introduce a novel representation for the geometry of a line or point (which can be at infinity) that is suitable for regression with a convolutional neural network (CNN); (iii) We introduce a large synthetic image dataset with ground truth for the orthogonal vanishing points, that can be used for training a CNN to predict these geometric entities; and finally (iv) We achieve state-of-the-art results on horizon detection, with 74.52\% AUC on the Horizon Lines in the Wild dataset. Our method is fast and robust, and can be used to remove perspective distortion from videos in real time.},
  archivePrefix = {arXiv},
  eprint = {1905.02231},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Geometric Approach to Obtain a Bird's Eye View from an Image-Abbas_Zisserman-2019.pdf;/Users/sunjiaming/Zotero/storage/4CRSIVE3/1905.html},
  journal = {arXiv:1905.02231 [cs]},
  primaryClass = {cs}
}

@inproceedings{abdulrachman3DLIDARMultiObject,
  title = {{{3D}}-{{LIDAR Multi Object Tracking}} for {{Autonomous Driving}}},
  author = {{Abdul Rachman}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-LIDAR Multi Object Tracking for Autonomous Driving-Abdul Rachman-.pdf}
}

@article{achlioptasLearningRepresentationsGenerative2017,
  title = {Learning {{Representations}} and {{Generative Models}} for {{3D Point Clouds}}},
  author = {Achlioptas, Panos and Diamanti, Olga and Mitliagkas, Ioannis and Guibas, Leonidas},
  year = {2017},
  month = jul,
  abstract = {Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.},
  archivePrefix = {arXiv},
  eprint = {1707.02392},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Representations and Generative Models for 3D Point Clouds-Achlioptas et al-2017.pdf;/Users/sunjiaming/Zotero/storage/VNLHEWLD/1707.html},
  journal = {arXiv:1707.02392 [cs]},
  keywords = {3d feature learning},
  primaryClass = {cs}
}

@article{agrawalDifferentiableConvexOptimization2019,
  title = {Differentiable {{Convex Optimization Layers}}},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
  year = {2019},
  month = oct,
  abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
  archivePrefix = {arXiv},
  eprint = {1910.12430},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Differentiable Convex Optimization Layers-Agrawal et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Z5JL8JUL/1910.html},
  journal = {arXiv:1910.12430 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@inproceedings{ajanovicSearchBasedOptimalMotion2018,
  title = {Search-{{Based Optimal Motion Planning}} for {{Automated Driving}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Ajanovic, Z. and Lacevic, B. and Shyrokau, B. and Stolz, M. and Horn, M.},
  year = {2018},
  month = oct,
  pages = {4523--4530},
  doi = {10.1109/IROS.2018.8593813},
  abstract = {This paper presents a framework for fast and robust motion planning designed to facilitate automated driving. The framework allows for real-time computation even for horizons of several hundred meters and thus enabling automated driving in urban conditions. This is achieved through several features. Firstly, a convenient geometrical representation of both the search space and driving constraints enables the use of classical path planning approach. Thus, a wide variety of constraints can be tackled simultaneously (other vehicles, traffic lights, etc.). Secondly, an exact cost-to-go map, obtained by solving a relaxed problem, is then used by A*-based algorithm with model predictive flavour in order to compute the optimal motion trajectory. The algorithm takes into account both distance and time horizons. The approach is validated within a simulation study with realistic traffic scenarios. We demonstrate the capability of the algorithm to devise plans both in fast and slow driving conditions, even when full stop is required.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Search-Based Optimal Motion Planning for Automated Driving-Ajanovic et al-2018.pdf;/Users/sunjiaming/Zotero/storage/BR6WCYIK/8593813.html},
  keywords = {planning}
}

@article{alhaijaAugmentedRealityMeets2017,
  title = {Augmented {{Reality Meets Computer Vision}} : {{Efficient Data Generation}} for {{Urban Driving Scenes}}},
  shorttitle = {Augmented {{Reality Meets Computer Vision}}},
  author = {Alhaija, Hassan Abu and Mustikovela, Siva Karthik and Mescheder, Lars and Geiger, Andreas and Rother, Carsten},
  year = {2017},
  month = aug,
  abstract = {The success of deep learning in computer vision is based on availability of large annotated datasets. To lower the need for hand labeled images, virtually rendered 3D worlds have recently gained popularity. Creating realistic 3D content is challenging on its own and requires significant human effort. In this work, we propose an alternative paradigm which combines real and synthetic data for learning semantic instance segmentation and object detection models. Exploiting the fact that not all aspects of the scene are equally important for this task, we propose to augment real-world imagery with virtual objects of the target category. Capturing real-world images at large scale is easy and cheap, and directly provides real background appearances without the need for creating complex 3D models of the environment. We present an efficient procedure to augment real images with virtual objects. This allows us to create realistic composite images which exhibit both realistic background appearance and a large number of complex object arrangements. In contrast to modeling complete 3D environments, our augmentation approach requires only a few user interactions in combination with 3D shapes of the target object. Through extensive experimentation, we conclude the right set of parameters to produce augmented data which can maximally enhance the performance of instance segmentation models. Further, we demonstrate the utility of our approach on training standard deep models for semantic instance segmentation and object detection of cars in outdoor driving scenes. We test the models trained on our augmented data on the KITTI 2015 dataset, which we have annotated with pixel-accurate ground truth, and on Cityscapes dataset. Our experiments demonstrate that models trained on augmented imagery generalize better than those trained on synthetic data or models trained on limited amount of annotated real data.},
  archivePrefix = {arXiv},
  eprint = {1708.01566},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Augmented Reality Meets Computer Vision -Alhaija et al-2017.pdf;/Users/sunjiaming/Zotero/storage/U52C8HYD/1708.html},
  journal = {arXiv:1708.01566 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{alhashimHighQualityMonocular2018,
  title = {High {{Quality Monocular Depth Estimation}} via {{Transfer Learning}}},
  author = {Alhashim, Ibraheem and Wonka, Peter},
  year = {2018},
  month = dec,
  abstract = {Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.},
  archivePrefix = {arXiv},
  eprint = {1812.11941},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/High Quality Monocular Depth Estimation via Transfer Learning-Alhashim_Wonka-2018.pdf;/Users/sunjiaming/Zotero/storage/4K84MTDD/1812.html},
  journal = {arXiv:1812.11941 [cs]},
  primaryClass = {cs}
}

@article{amosDifferentiableCrossEntropyMethod2020,
  title = {The {{Differentiable Cross}}-{{Entropy Method}}},
  author = {Amos, Brandon and Yarats, Denis},
  year = {2020},
  month = aug,
  abstract = {We study the cross-entropy method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. DCEM enables us to fine-tune CEM-based controllers with policy optimization.},
  archivePrefix = {arXiv},
  eprint = {1909.12830},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Differentiable Cross-Entropy Method-Amos_Yarats-2020.pdf;/Users/sunjiaming/Zotero/storage/JZH56V35/1909.html},
  journal = {arXiv:1909.12830 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{amosLimitedMultiLabelProjection2019,
  title = {The {{Limited Multi}}-{{Label Projection Layer}}},
  author = {Amos, Brandon and Koltun, Vladlen and Kolter, J. Zico},
  year = {2019},
  month = jun,
  abstract = {We propose the Limited Multi-Label (LML) projection layer as a new primitive operation for end-to-end learning systems. The LML layer provides a probabilistic way of modeling multi-label predictions limited to having exactly k labels. We derive efficient forward and backward passes for this layer and show how the layer can be used to optimize the top-k recall for multi-label tasks with incomplete label information. We evaluate LML layers on top-k CIFAR-100 classification and scene graph generation. We show that LML layers add a negligible amount of computational overhead, strictly improve the model's representational capacity, and improve accuracy. We also revisit the truncated top-k entropy method as a competitive baseline for top-k classification.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Limited Multi-Label Projection Layer-Amos et al-22.pdf;/Users/sunjiaming/Zotero/storage/RDCLYRKA/1906.html;/Users/sunjiaming/Zotero/storage/SNAGWBAK/1906.html},
  language = {en}
}

@article{amosOptNetDifferentiableOptimization2019,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  year = {2019},
  month = oct,
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
  archivePrefix = {arXiv},
  eprint = {1703.00443},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/OptNet-Amos_Kolter-2019.pdf;/Users/sunjiaming/Zotero/storage/H33KG65S/1703.html},
  journal = {arXiv:1703.00443 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@inproceedings{anonymousImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied...},
  file = {/Users/sunjiaming/Zotero/storage/JFQN7XKL/Anonymous - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf;/Users/sunjiaming/Zotero/storage/FUSUKS9H/forum.html},
  language = {en}
}

@inproceedings{anonymousLambdaNetworksModelingLongrange2020,
  title = {{{LambdaNetworks}}: {{Modeling}} Long-Range {{Interactions}} without {{Attention}}},
  shorttitle = {{{LambdaNetworks}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {We present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda...},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LambdaNetworks-Anonymous-2020.pdf;/Users/sunjiaming/Zotero/storage/D47YLM7B/forum.html;/Users/sunjiaming/Zotero/storage/LG4K5QQ6/forum.html},
  language = {en}
}

@inproceedings{anonymousLongRangeArena2020,
  title = {Long {{Range Arena}} : {{A Benchmark}} for {{Efficient Transformers}}},
  shorttitle = {Long {{Range Arena}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been...},
  file = {/Users/sunjiaming/Zotero/storage/8KPMFKR5/Anonymous - 2020 - Long Range Arena  A Benchmark for Efficient Trans.pdf;/Users/sunjiaming/Zotero/storage/Z9JFZVWU/forum.html},
  language = {en}
}

@article{ansariEarthAinFlat2018,
  title = {The {{Earth}} Ain't {{Flat}}: {{Monocular Reconstruction}} of {{Vehicles}} on {{Steep}} and {{Graded Roads}} from a {{Moving Camera}}},
  shorttitle = {The {{Earth}} Ain't {{Flat}}},
  author = {Ansari, Junaid Ahmed and Sharma, Sarthak and Majumdar, Anshuman and Murthy, J. Krishna and Krishna, K. Madhava},
  year = {2018},
  month = mar,
  abstract = {Accurate localization of other traffic participants is a vital task in autonomous driving systems. State-of-the-art systems employ a combination of sensing modalities such as RGB cameras and LiDARs for localizing traffic participants, but most such demonstrations have been confined to plain roads. We demonstrate, to the best of our knowledge, the first results for monocular object localization and shape estimation on surfaces that do not share the same plane with the moving monocular camera. We approximate road surfaces by local planar patches and use semantic cues from vehicles in the scene to initialize a local bundle-adjustment like procedure that simultaneously estimates the pose and shape of the vehicles, and the orientation of the local ground plane on which the vehicle stands as well. We evaluate the proposed approach on the KITTI and SYNTHIA-SF benchmarks, for a variety of road plane configurations. The proposed approach significantly improves the state-of-the-art for monocular object localization on arbitrarily-shaped roads.},
  archivePrefix = {arXiv},
  eprint = {1803.02057},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/5JFKPSGM/Ansari et al. - 2018 - The Earth ain't Flat Monocular Reconstruction of .pdf},
  journal = {arXiv:1803.02057 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{aokiPointNetLKRobustEfficient2019,
  title = {{{PointNetLK}}: {{Robust}} \& {{Efficient Point Cloud Registration}} Using {{PointNet}}},
  shorttitle = {{{PointNetLK}}},
  author = {Aoki, Yasuhiro and Goforth, Hunter and Srivatsan, Rangaprasad Arun and Lucey, Simon},
  year = {2019},
  month = mar,
  abstract = {PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent extensions are state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable "imaging" function. As a consequence, classical vision algorithms for image alignment can be applied on the problem - namely the Lucas \& Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency - opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK.},
  archivePrefix = {arXiv},
  eprint = {1903.05711},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointNetLK-Aoki et al-2019.pdf;/Users/sunjiaming/Zotero/storage/5SFVD95W/1903.html},
  journal = {arXiv:1903.05711 [cs]},
  primaryClass = {cs}
}

@article{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = jan,
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  archivePrefix = {arXiv},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Wasserstein GAN-Arjovsky et al-2017.pdf;/Users/sunjiaming/Zotero/storage/Z7GBI6LP/1701.html},
  journal = {arXiv:1701.07875 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{armeni3DSceneGraph2019,
  title = {{{3D Scene Graph}}: {{A Structure}} for {{Unified Semantics}}, {{3D Space}}, and {{Camera}}},
  shorttitle = {{{3D Scene Graph}}},
  author = {Armeni, Iro and He, Zhi-Yang and Gwak, JunYoung and Zamir, Amir R. and Fischer, Martin and Malik, Jitendra and Savarese, Silvio},
  year = {2019},
  month = oct,
  abstract = {A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, texture, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, and other attributes), rooms (e.g., scene category, volume, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities. However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations.},
  archivePrefix = {arXiv},
  eprint = {1910.02527},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Scene Graph-Armeni et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XCNMKJIY/1910.html},
  journal = {arXiv:1910.02527 [cs]},
  primaryClass = {cs}
}

@article{armeniJoint2D3DSemanticData,
  title = {Joint {{2D}}-{{3D}}-{{Semantic Data}} for {{Indoor Scene Understanding}}},
  author = {Armeni, Iro and Sax, Alexander and Zamir, Amir R and Savarese, Silvio},
  pages = {9},
  abstract = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000 m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{$\smwhtcircle$} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.},
  file = {/Users/sunjiaming/Zotero/storage/WX99K899/Armeni et al. - Joint 2D-3D-Semantic Data for Indoor Scene Underst.pdf},
  keywords = {dataset},
  language = {en}
}

@article{arnabExploitingTemporalContext2019,
  title = {Exploiting Temporal Context for {{3D}} Human Pose Estimation in the Wild},
  author = {Arnab, Anurag and Doersch, Carl and Zisserman, Andrew},
  year = {2019},
  month = may,
  abstract = {We present a bundle-adjustment-based algorithm for recovering accurate 3D human pose and meshes from monocular videos. Unlike previous algorithms which operate on single frames, we show that reconstructing a person over an entire sequence gives extra constraints that can resolve ambiguities. This is because videos often give multiple views of a person, yet the overall body shape does not change and 3D positions vary slowly. Our method improves not only on standard mocap-based datasets like Human 3.6M -- where we show quantitative improvements -- but also on challenging in-the-wild datasets such as Kinetics. Building upon our algorithm, we present a new dataset of more than 3 million frames of YouTube videos from Kinetics with automatically generated 3D poses and meshes. We show that retraining a single-frame 3D pose estimator on this data improves accuracy on both real-world and mocap data by evaluating on the 3DPW and HumanEVA datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.04266},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Exploiting temporal context for 3D human pose estimation in the wild-Arnab et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9M2X7YGQ/1905.html},
  journal = {arXiv:1905.04266 [cs]},
  primaryClass = {cs}
}

@article{asanoSelflabellingSimultaneousClustering2019,
  title = {Self-Labelling via Simultaneous Clustering and Representation Learning},
  author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
  year = {2019},
  month = nov,
  abstract = {Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Compared to the best previous method in this class, namely DeepCluster, our formulation minimizes a single objective function for both representation learning and clustering; it also significantly outperforms DeepCluster in standard benchmarks and reaches state of the art for learning a ResNet-50 self-supervisedly.},
  archivePrefix = {arXiv},
  eprint = {1911.05371},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-labelling via simultaneous clustering and representation learning-Asano et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AAPK3XV5/1911.html},
  journal = {arXiv:1911.05371 [cs]},
  primaryClass = {cs}
}

@article{asselinDeepSVBRDFEstimation2020,
  title = {Deep {{SVBRDF Estimation}} on {{Real Materials}}},
  author = {Asselin, Louis-Philippe and Laurendeau, Denis and Lalonde, Jean-Fran{\c c}ois},
  year = {2020},
  month = oct,
  abstract = {Recent work has demonstrated that deep learning approaches can successfully be used to recover accurate estimates of the spatially-varying BRDF (SVBRDF) of a surface from as little as a single image. Closer inspection reveals, however, that most approaches in the literature are trained purely on synthetic data, which, while diverse and realistic, is often not representative of the richness of the real world. In this paper, we show that training such networks exclusively on synthetic data is insufficient to achieve adequate results when tested on real data. Our analysis leverages a new dataset of real materials obtained with a novel portable multi-light capture apparatus. Through an extensive series of experiments and with the use of a novel deep learning architecture, we explore two strategies for improving results on real data: finetuning, and a per-material optimization procedure. We show that adapting network weights to real data is of critical importance, resulting in an approach which significantly outperforms previous methods for SVBRDF estimation on real materials. Dataset and code are available at https://lvsn.github.io/real-svbrdf},
  archivePrefix = {arXiv},
  eprint = {2010.04143},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/2VW48NP8/Asselin et al. - 2020 - Deep SVBRDF Estimation on Real Materials.pdf;/Users/sunjiaming/Zotero/storage/CQDX4C6Y/2010.html},
  journal = {arXiv:2010.04143 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{atanasovUnifyingViewGeometry2018,
  title = {A {{Unifying View}} of {{Geometry}}, {{Semantics}}, and {{Data Association}} in {{SLAM}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Atanasov, Nikolay and Bowman, Sean L. and Daniilidis, Kostas and Pappas, George J.},
  year = {2018},
  month = jul,
  pages = {5204--5208},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/722},
  abstract = {Traditional approaches for simultaneous localization and mapping (SLAM) rely on geometric features such as points, lines, and planes to infer the environment structure. They make hard decisions about the (data) association between observed features and mapped landmarks to update the environment model. This paper makes two contributions to the state of the art in SLAM. First, it generalizes the purely geometric model by introducing semantically meaningful objects, represented as structured models of mid-level part features. Second, instead of making hard, potentially wrong associations between semantic features and objects, it shows that SLAM inference can be performed efficiently with probabilistic data association. The approach not only allows building meaningful maps (containing doors, chairs, cars, etc.) but also offers significant advantages in ambiguous environments.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A Unifying View of Geometry, Semantics, and Data Association in SLAM-Atanasov et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A Unifying View of Geometry, Semantics, and Data Association in SLAM-Atanasov et al-22.pdf},
  isbn = {978-0-9992411-2-7},
  keywords = {semantic slam,slam},
  language = {en}
}

@article{atteneLightweightApproachRepairing2010,
  title = {A Lightweight Approach to Repairing Digitized Polygon Meshes},
  author = {Attene, Marco},
  year = {2010},
  month = nov,
  volume = {26},
  pages = {1393--1406},
  issn = {1432-2315},
  doi = {10.1007/s00371-010-0416-3},
  abstract = {When designing novel algorithms for geometric processing and analysis, researchers often assume that the input conforms to several requirements. On the other hand, polygon meshes obtained from acquisition of real-world objects typically exhibit several defects, and thus are not appropriate for a widespread exploitation.},
  journal = {The Visual Computer},
  language = {en},
  number = {11}
}

@article{atzmonSALSignAgnostic2020,
  title = {{{SAL}}++: {{Sign Agnostic Learning}} with {{Derivatives}}},
  shorttitle = {{{SAL}}++},
  author = {Atzmon, Matan and Lipman, Yaron},
  year = {2020},
  month = jun,
  abstract = {Learning 3D geometry directly from raw data, such as point clouds, triangle soups, or un-oriented meshes is still a challenging task that feeds many downstream computer vision and graphics applications. In this paper, we introduce SAL++: a method for learning implicit neural representations of shapes directly from such raw data. We build upon the recent sign agnostic learning (SAL) approach and generalize it to include derivative data in a sign agnostic manner. In more detail, given the unsigned distance function to the input raw data, we suggest a novel sign agnostic regression loss, incorporating both pointwise values and gradients of the unsigned distance function. Optimizing this loss leads to a signed implicit function solution, the zero level set of which is a high quality, valid manifold approximation to the input 3D data. We demonstrate the efficacy of SAL++ shape space learning from two challenging datasets: ShapeNet that contains inconsistent orientation and non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups). On both these datasets, we present state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {2006.05400},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SAL++-Atzmon_Lipman-2020.pdf},
  journal = {arXiv:2006.05400 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@misc{AugmentedRealityApple,
  title = {Augmented {{Reality}} - {{Apple Developer}}},
  file = {/Users/sunjiaming/Zotero/storage/FTBKZ2QV/augmented-reality.html},
  howpublished = {https://developer.apple.com/augmented-reality/},
  keywords = {neufu_paper}
}

@article{avetisyanEndtoEndCADModel2019,
  title = {End-to-{{End CAD Model Retrieval}} and {{9DoF Alignment}} in {{3D Scans}}},
  author = {Avetisyan, Armen and Dai, Angela and Nie{\ss}ner, Matthias},
  year = {2019},
  month = jun,
  abstract = {We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by \$19.04\textbackslash\%\$ for CAD model alignment to scans, with \$\textbackslash approx 250\textbackslash times\$ faster runtime than previous data-driven approaches.},
  archivePrefix = {arXiv},
  eprint = {1906.04201},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans-Avetisyan et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WWFI3CUA/1906.html},
  journal = {arXiv:1906.04201 [cs]},
  primaryClass = {cs}
}

@article{avetisyanScan2CADLearningCAD2018,
  title = {{{Scan2CAD}}: {{Learning CAD Model Alignment}} in {{RGB}}-{{D Scans}}},
  shorttitle = {{{Scan2CAD}}},
  author = {Avetisyan, Armen and Dahnert, Manuel and Dai, Angela and Savva, Manolis and Chang, Angel X. and Nie{\ss}ner, Matthias},
  year = {2018},
  month = nov,
  abstract = {We present Scan2CAD, a novel data-driven method that learns to align clean 3D CAD models from a shape database to the noisy and incomplete geometry of a commodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method takes as input a set of CAD models, and predicts a 9DoF pose that aligns each model to the underlying scan geometry. To tackle this problem, we create a new scan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoint pairs between 14225 CAD models from ShapeNet and their counterpart objects in the scans. Our method selects a set of representative keypoints in a 3D scan for which we find correspondences to the CAD geometry. To this end, we design a novel 3D CNN architecture that learns a joint embedding between real and synthetic objects, and from this predicts a correspondence heatmap. Based on these correspondence heatmaps, we formulate a variational energy minimization that aligns a given set of CAD models to the reconstruction. We evaluate our approach on our newly introduced Scan2CAD benchmark where we outperform both handcrafted feature descriptor as well as state-of-the-art CNN based methods by 21.39\%.},
  archivePrefix = {arXiv},
  eprint = {1811.11187},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scan2CAD-Avetisyan et al-2018.pdf;/Users/sunjiaming/Zotero/storage/HEMJAVM9/1811.html},
  journal = {arXiv:1811.11187 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{avetisyanSceneCADPredictingObject,
  title = {{{SceneCAD}}: {{Predicting Object Alignments}} and {{Layouts}} in {{RGB}}-{{D Scans}}},
  author = {Avetisyan, Armen and Khanova, Tatiana and Choy, Christopher and Dash, Denver and Dai, Angela and Nie{\ss}ner, Matthias},
  pages = {19},
  abstract = {We present a novel approach to reconstructing lightweight, CAD-based representations of scanned 3D environments from commodity RGB-D sensors. Our key idea is to jointly optimize for both CAD model alignments as well as layout estimations of the scanned scene, explicitly modeling inter-relationships between objects-to-objects and objects-to-layout. Since object arrangement and scene layout are intrinsically coupled, we show that treating the problem jointly significantly helps to produce globally-consistent representations of a scene. Object CAD models are aligned to the scene by establishing dense correspondences between geometry, and we introduce a hierarchical layout prediction approach to estimate layout planes from corners and edges of the scene. To this end, we propose a message-passing graph neural network to model the inter-relationships between objects and layout, guiding generation of a globally object alignment in a scene. By considering the global scene layout, we achieve significantly improved CAD alignments compared to state-of-the-art methods, improving from 41.83\% to 58.41\% alignment accuracy on SUNCG and from 50.05\% to 61.24\% on ScanNet, respectively. The resulting CAD-based representations makes our method well-suited for applications in content creation such as augmented- or virtual reality.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SceneCAD-Avetisyan et al-.pdf},
  language = {en}
}

@article{azinovicInversePathTracing2019,
  title = {Inverse {{Path Tracing}} for {{Joint Material}} and {{Lighting Estimation}}},
  author = {Azinovi{\'c}, Dejan and Li, Tzu-Mao and Kaplanyan, Anton and Nie{\ss}ner, Matthias},
  year = {2019},
  month = mar,
  abstract = {Modern computer vision algorithms have brought significant advancement to 3D geometry reconstruction. However, illumination and material reconstruction remain less studied, with current approaches assuming very simplified models for materials and illumination. We introduce Inverse Path Tracing, a novel approach to jointly estimate the material properties of objects and light sources in indoor scenes by using an invertible light transport simulation. We assume a coarse geometry scan, along with corresponding images and camera poses. The key contribution of this work is an accurate and simultaneous retrieval of light sources and physically based material properties (e.g., diffuse reflectance, specular reflectance, roughness, etc.) for the purpose of editing and re-rendering the scene under new conditions. To this end, we introduce a novel optimization method using a differentiable Monte Carlo renderer that computes derivatives with respect to the estimated unknown illumination and material properties. This enables joint optimization for physically correct light transport and material models using a tailored stochastic gradient descent.},
  archivePrefix = {arXiv},
  eprint = {1903.07145},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Inverse Path Tracing for Joint Material and Lighting Estimation-Azinović et al-2019.pdf},
  journal = {arXiv:1903.07145 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{babbObjectSpatialTemporal2010,
  title = {Object, Spatial, and Temporal Memory: {{A}} Behavioral Analysis of Visual Scenes Using a What, Where, and When Paradigm},
  author = {Babb, Stephanie J and Johnson, Ruth M},
  year = {2010},
  volume = {26},
  pages = {15},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object, spatial, and temporal memory-Babb_Johnson-2010.pdf},
  journal = {Current psychology letters},
  language = {en},
  number = {2}
}

@article{badkiMeshletPriors3D2020,
  title = {Meshlet {{Priors}} for {{3D Mesh Reconstruction}}},
  author = {Badki, Abhishek and Gallo, Orazio and Kautz, Jan and Sen, Pradeep},
  year = {2020},
  month = jan,
  abstract = {Estimating a mesh from an unordered set of sparse, noisy 3D points is a challenging problem that requires carefully selected priors. Existing hand-crafted priors, such as smoothness regularizers, impose an undesirable trade-off between attenuating noise and preserving local detail. Recent deep-learning approaches produce impressive results by learning priors directly from the data. However, the priors are learned at the object level, which makes these algorithms class-specific, and even sensitive to the pose of the object. We introduce meshlets, small patches of mesh that we use to learn local shape priors. Meshlets act as a dictionary of local features and thus allow to use learned priors to reconstruct object meshes in any pose and from unseen classes, even when the noise is large and the samples sparse.},
  archivePrefix = {arXiv},
  eprint = {2001.01744},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Meshlet Priors for 3D Mesh Reconstruction-Badki et al-22.pdf;/Users/sunjiaming/Zotero/storage/ZZ9KX5QM/2001.html},
  journal = {arXiv:2001.01744 [cs]},
  primaryClass = {cs}
}

@article{badkiMeshletPriors3D2020a,
  title = {Meshlet {{Priors}} for {{3D Mesh Reconstruction}}},
  author = {Badki, Abhishek and Gallo, Orazio and Kautz, Jan and Sen, Pradeep},
  year = {2020},
  month = jun,
  abstract = {Estimating a mesh from an unordered set of sparse, noisy 3D points is a challenging problem that requires carefully selected priors. Existing hand-crafted priors, such as smoothness regularizers, impose an undesirable trade-off between attenuating noise and preserving local detail. Recent deep-learning approaches produce impressive results by learning priors directly from the data. However, the priors are learned at the object level, which makes these algorithms class-specific and even sensitive to the pose of the object. We introduce meshlets, small patches of mesh that we use to learn local shape priors. Meshlets act as a dictionary of local features and thus allow to use learned priors to reconstruct object meshes in any pose and from unseen classes, even when the noise is large and the samples sparse.},
  archivePrefix = {arXiv},
  eprint = {2001.01744},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Meshlet Priors for 3D Mesh Reconstruction-Badki et al-3.pdf;/Users/sunjiaming/Zotero/storage/LKC4FH8N/Badki et al. - 2020 - Meshlet Priors for 3D Mesh Reconstruction.pdf;/Users/sunjiaming/Zotero/storage/PZJKJRCI/2001.html},
  journal = {arXiv:2001.01744 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{ballesterDOTDynamicObject2020,
  title = {{{DOT}}: {{Dynamic Object Tracking}} for {{Visual SLAM}}},
  shorttitle = {{{DOT}}},
  author = {Ballester, Irene and Fontan, Alejandro and Civera, Javier and Strobl, Klaus H. and Triebel, Rudolph},
  year = {2020},
  month = sep,
  abstract = {In this paper we present DOT (Dynamic Object Tracking), a front-end that added to existing SLAM systems can significantly improve their robustness and accuracy in highly dynamic environments. DOT combines instance segmentation and multi-view geometry to generate masks for dynamic objects in order to allow SLAM systems based on rigid scene models to avoid such image areas in their optimizations. To determine which objects are actually moving, DOT segments first instances of potentially dynamic objects and then, with the estimated camera motion, tracks such objects by minimizing the photometric reprojection error. This short-term tracking improves the accuracy of the segmentation with respect to other approaches. In the end, only actually dynamic masks are generated. We have evaluated DOT with ORB-SLAM 2 in three public datasets. Our results show that our approach improves significantly the accuracy and robustness of ORB-SLAM 2, especially in highly dynamic scenes.},
  archivePrefix = {arXiv},
  eprint = {2010.00052},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DOT-Ballester et al-2020.pdf;/Users/sunjiaming/Zotero/storage/3HNFXFY6/2010.html},
  journal = {arXiv:2010.00052 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{balntasHPatchesBenchmarkEvaluation2017,
  title = {{{HPatches}}: {{A}} Benchmark and Evaluation of Handcrafted and Learned Local Descriptors},
  shorttitle = {{{HPatches}}},
  author = {Balntas, Vassileios and Lenc, Karel and Vedaldi, Andrea and Mikolajczyk, Krystian},
  year = {2017},
  month = apr,
  abstract = {In this paper, we propose a novel benchmark for evaluating local image descriptors. We demonstrate that the existing datasets and evaluation protocols do not specify unambiguously all aspects of evaluation, leading to ambiguities and inconsistencies in results reported in the literature. Furthermore, these datasets are nearly saturated due to the recent improvements in local descriptors obtained by learning them from large annotated datasets. Therefore, we introduce a new large dataset suitable for training and testing modern descriptors, together with strictly defined evaluation protocols in several tasks such as matching, retrieval and classification. This allows for more realistic, and thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors can boost their performance to the level of deep learning based descriptors within a realistic benchmarks evaluation.},
  archivePrefix = {arXiv},
  eprint = {1704.05939},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/HPatches-Balntas et al-2017.pdf;/Users/sunjiaming/Zotero/storage/LBEGMQFR/1704.html},
  journal = {arXiv:1704.05939 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{balntasPoseGuidedRGBD2017,
  title = {Pose {{Guided RGBD Feature Learning}} for {{3D Object Pose Estimation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Balntas, Vassileios and Doumanoglou, Andreas and Sahin, Caner and Sock, Juil and Kouskouridas, Rigas and Kim, Tae-Kyun},
  year = {2017},
  month = oct,
  pages = {3876--3884},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.416},
  abstract = {In this paper we examine the effects of using object poses as guidance to learning robust features for 3D object pose estimation. Previous works have focused on learning feature embeddings based on metric learning with triplet comparisons and rely only on the qualitative distinction of similar and dissimilar pose labels. In contrast, we consider the exact pose differences between the training samples, and aim to learn embeddings such that the distances in the pose label space are proportional to the distances in the feature space. However, since it is less desirable to force the pose-feature correlation when objects are symmetric, we discuss the use of weights that reflect object symmetry when measuring the pose distances. Furthermore, end-toend pose regression is investigated and is shown to further boost the discriminative power of feature learning, improving pose recognition accuracies. Experimental results show that the features that are learnt guided by poses, are significantly more discriminative than the ones learned in the traditional way, outperforming state-of-the-art works. Finally, we measure the generalisation capacity of pose guided feature learning in previously unseen scenes containing objects under different occlusion levels, and we show that it adapts well to novel tasks.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pose Guided RGBD Feature Learning for 3D Object Pose Estimation-Balntas et al-2017.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@article{bansalChauffeurNetLearningDrive2018,
  title = {{{ChauffeurNet}}: {{Learning}} to {{Drive}} by {{Imitating}} the {{Best}} and {{Synthesizing}} the {{Worst}}},
  shorttitle = {{{ChauffeurNet}}},
  author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
  year = {2018},
  month = dec,
  abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.},
  archivePrefix = {arXiv},
  eprint = {1812.03079},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ChauffeurNet-Bansal et al-2018.pdf;/Users/sunjiaming/Zotero/storage/GYBZ9EEM/1812.html},
  journal = {arXiv:1812.03079 [cs]},
  keywords = {motion prediction},
  primaryClass = {cs}
}

@article{bar-haimScopeFlowDynamicScene2020,
  title = {{{ScopeFlow}}: {{Dynamic Scene Scoping}} for {{Optical Flow}}},
  shorttitle = {{{ScopeFlow}}},
  author = {{Bar-Haim}, Aviram and Wolf, Lior},
  year = {2020},
  month = feb,
  abstract = {We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol. Using a low parameters off-the-shelf model, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10\%. The method also surpasses all similar architecture variants by more than 12\% and 19.7\% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.},
  archivePrefix = {arXiv},
  eprint = {2002.10770},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ScopeFlow-Bar-Haim_Wolf-2020.pdf;/Users/sunjiaming/Zotero/storage/ZU9S7ADV/2002.html},
  journal = {arXiv:2002.10770 [cs]},
  primaryClass = {cs}
}

@article{barathMAGSACMarginalizingSample2018,
  title = {{{MAGSAC}}: Marginalizing Sample Consensus},
  shorttitle = {{{MAGSAC}}},
  author = {Barath, Daniel and Noskova, Jana and Matas, Jiri},
  year = {2018},
  month = mar,
  abstract = {A method called, sigma-consensus, is proposed to eliminate the need for a user-defined inlier-outlier threshold in RANSAC. Instead of estimating the noise sigma, it is marginalized over a range of noise scales. The optimized model is obtained by weighted least-squares fitting where the weights come from the marginalization over sigma of the point likelihoods of being inliers. A new quality function is proposed not requiring sigma and, thus, a set of inliers to determine the model quality. Also, a new termination criterion for RANSAC is built on the proposed marginalization approach. Applying sigma-consensus, MAGSAC is proposed with no need for a user-defined sigma and improving the accuracy of robust estimation significantly. It is superior to the state-of-the-art in terms of geometric accuracy on publicly available real-world datasets for epipolar geometry (F and E) and homography estimation. In addition, applying sigma-consensus only once as a post-processing step to the RANSAC output always improved the model quality on a wide range of vision problems without noticeable deterioration in processing time, adding a few milliseconds. The source code is at https://github.com/danini/magsac.},
  archivePrefix = {arXiv},
  eprint = {1803.07469},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MAGSAC-Barath et al-2018.pdf;/Users/sunjiaming/Zotero/storage/VWQ69EDM/1803.html},
  journal = {arXiv:1803.07469 [cs]},
  primaryClass = {cs}
}

@book{barfootStateEstimationRobotics2017,
  title = {State {{Estimation}} for {{Robotics}}},
  author = {Barfoot, Timothy D.},
  year = {2017},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781316671528},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/State Estimation for Robotics-Barfoot-22.pdf},
  isbn = {978-1-316-67152-8},
  language = {en}
}

@article{barronGeneralAdaptiveRobust2017,
  title = {A {{General}} and {{Adaptive Robust Loss Function}}},
  author = {Barron, Jonathan T.},
  year = {2017},
  month = jan,
  abstract = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
  archivePrefix = {arXiv},
  eprint = {1701.03077},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A General and Adaptive Robust Loss Function-Barron-2017.pdf},
  journal = {arXiv:1701.03077 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{baserFANTrack3DMultiObject2019,
  title = {{{FANTrack}}: {{3D Multi}}-{{Object Tracking}} with {{Feature Association Network}}},
  shorttitle = {{{FANTrack}}},
  author = {Baser, Erkan and Balasubramanian, Venkateshwaran and Bhattacharyya, Prarthana and Czarnecki, Krzysztof},
  year = {2019},
  month = may,
  abstract = {We propose a data-driven approach to online multi-object tracking (MOT) that uses a convolutional neural network (CNN) for data association in a tracking-by-detection framework. The problem of multi-target tracking aims to assign noisy detections to a-priori unknown and time-varying number of tracked objects across a sequence of frames. A majority of the existing solutions focus on either tediously designing cost functions or formulating the task of data association as a complex optimization problem that can be solved effectively. Instead, we exploit the power of deep learning to formulate the data association problem as inference in a CNN. To this end, we propose to learn a similarity function that combines cues from both image and spatial features of objects. Our solution learns to perform global assignments in 3D purely from data, handles noisy detections and a varying number of targets, and is easy to train. We evaluate our approach on the challenging KITTI dataset and show competitive results. Our code is available at https://git.uwaterloo.ca/wise-lab/fantrack.},
  archivePrefix = {arXiv},
  eprint = {1905.02843},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Baser et al_2019_FANTrack.pdf;/Users/sunjiaming/Zotero/storage/QGPWIJ82/1905.html},
  journal = {arXiv:1905.02843 [cs]},
  primaryClass = {cs}
}

@article{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archivePrefix = {arXiv},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/EWAYSS7F/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/Users/sunjiaming/Zotero/storage/DL33VAKU/1806.html},
  journal = {arXiv:1806.01261 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bednarikShapeReconstructionLearning2019,
  title = {Shape {{Reconstruction}} by {{Learning Differentiable Surface Representations}}},
  author = {Bednarik, Jan and Parashar, Shaifali and Gundogdu, Erhan and Salzmann, Mathieu and Fua, Pascal},
  year = {2019},
  month = nov,
  abstract = {Generative models that produce point clouds have emerged as a powerful tool to represent 3D surfaces, and the best current ones rely on learning an ensemble of parametric representations. Unfortunately, they offer no control over the deformations of the surface patches that form the ensemble and thus fail to prevent them from either overlapping or collapsing into single points or lines. As a consequence, computing shape properties such as surface normals and curvatures becomes difficult and unreliable. In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap. Furthermore, this lets us reliably compute quantities such as surface normals and curvatures. We will demonstrate on several tasks that this yields more accurate surface reconstructions than the state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches.},
  archivePrefix = {arXiv},
  eprint = {1911.11227},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Shape Reconstruction by Learning Differentiable Surface Representations-Bednarik et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TUBSEJIT/1911.html},
  journal = {arXiv:1911.11227 [cs]},
  primaryClass = {cs}
}

@article{behlPointFlowNetLearningRepresentations2018,
  title = {{{PointFlowNet}}: {{Learning Representations}} for {{Rigid Motion Estimation}} from {{Point Clouds}}},
  shorttitle = {{{PointFlowNet}}},
  author = {Behl, Aseem and Paschalidou, Despoina and Donn{\'e}, Simon and Geiger, Andreas},
  year = {2018},
  month = jun,
  abstract = {Despite significant progress in image-based 3D scene flow estimation, the performance of such approaches has not yet reached the fidelity required by many applications. Simultaneously, these applications are often not restricted to image-based estimation: laser scanners provide a popular alternative to traditional cameras, for example in the context of self-driving cars, as they directly yield a 3D point cloud. In this paper, we propose to estimate 3D motion from such unstructured point clouds using a deep neural network. In a single forward pass, our model jointly predicts 3D scene flow as well as the 3D bounding box and rigid body motion of objects in the scene. While the prospect of estimating 3D scene flow from unstructured point clouds is promising, it is also a challenging task. We show that the traditional global representation of rigid body motion prohibits inference by CNNs, and propose a translation equivariant representation to circumvent this problem. For training our deep network, a large dataset is required. Because of this, we augment real scans from KITTI with virtual objects, realistically modeling occlusions and simulating sensor noise. A thorough comparison with classic and learning-based techniques highlights the robustness of the proposed approach.},
  archivePrefix = {arXiv},
  eprint = {1806.02170},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointFlowNet-Behl et al-2018.pdf;/Users/sunjiaming/Zotero/storage/5GSLBEMK/1806.html},
  journal = {arXiv:1806.02170 [cs]},
  primaryClass = {cs}
}

@article{bemanaXFieldsImplicitNeural,
  title = {X-{{Fields}}: {{Implicit Neural View}}-, {{Light}}- and {{Time}}-{{Image Interpolation}}},
  author = {Bemana, Mojtaba and Myszkowski, Karol and Seidel, Hans-Peter and Ritschel, Tobias},
  volume = {39},
  pages = {15},
  file = {/Users/sunjiaming/Zotero/storage/J8DPZSBC/Bemana et al. - X-Fields Implicit Neural View-, Light- and Time-I.pdf},
  language = {en},
  number = {6}
}

@article{bergmannTrackingBellsWhistles2019,
  title = {Tracking without Bells and Whistles},
  author = {Bergmann, Philipp and Meinhardt, Tim and {Leal-Taixe}, Laura},
  year = {2019},
  month = aug,
  abstract = {The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.},
  archivePrefix = {arXiv},
  eprint = {1903.05625},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tracking without bells and whistles-Bergmann et al-2019.pdf;/Users/sunjiaming/Zotero/storage/GBCEWCNE/1903.html},
  journal = {arXiv:1903.05625 [cs]},
  primaryClass = {cs}
}

@article{bernardinEvaluatingMultipleObject2008,
  ids = {bernardinEvaluatingMultipleObject2008a},
  title = {Evaluating {{Multiple Object Tracking Performance}}: {{The CLEAR MOT Metrics}}},
  shorttitle = {Evaluating {{Multiple Object Tracking Performance}}},
  author = {Bernardin, Keni and Stiefelhagen, Rainer},
  year = {2008},
  volume = {2008},
  pages = {1--10},
  issn = {1687-5176, 1687-5281},
  doi = {10.1155/2008/246309},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Evaluating Multiple Object Tracking Performance-Bernardin_Stiefelhagen-2008.pdf;/Users/sunjiaming/Zotero/storage/Z8AK4936/Bernardin and Stiefelhagen - 2008 - Evaluating Multiple Object Tracking Performance T.pdf},
  journal = {EURASIP Journal on Image and Video Processing},
  keywords = {2d tracking},
  language = {en}
}

@article{bertasiusLearningTemporalPose2019,
  title = {Learning {{Temporal Pose Estimation}} from {{Sparsely}}-{{Labeled Videos}}},
  author = {Bertasius, Gedas and Feichtenhofer, Christoph and Tran, Du and Shi, Jianbo and Torresani, Lorenzo},
  year = {2019},
  month = dec,
  abstract = {Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7\% mAP vs 83.8\% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.},
  archivePrefix = {arXiv},
  eprint = {1906.04016},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Temporal Pose Estimation from Sparsely-Labeled Videos-Bertasius et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VW77LNL3/1906.html},
  journal = {arXiv:1906.04016 [cs]},
  primaryClass = {cs}
}

@article{bewleyRangeConditionedDilated2020,
  title = {Range {{Conditioned Dilated Convolutions}} for {{Scale Invariant 3D Object Detection}}},
  author = {Bewley, Alex and Sun, Pei and Mensink, Thomas and Anguelov, Dragomir and Sminchisescu, Cristian},
  year = {2020},
  month = may,
  abstract = {This paper presents a novel 3D object detection framework that processes LiDAR data directly on a representation of the sensor's native range images. When operating in the range image view, one faces learning challenges, including occlusion and considerable scale variation, limiting the obtainable accuracy. To address these challenges, a range-conditioned dilated block (RCD) is proposed to dynamically adjust a continuous dilation rate as a function of the measured range, achieving scale invariance. Furthermore, soft range gating helps mitigate the effect of occlusion. An end-to-end trained box-refinement network brings additional performance improvements in occluded areas, and produces more accurate bounding box predictions. On the challenging Waymo Open Dataset, our improved range-based detector outperforms state of the art at long range detection. Our framework is superior to prior multiview, voxel-based methods over all ranges, setting a new baseline for range-based 3D detection on this large scale public dataset.},
  archivePrefix = {arXiv},
  eprint = {2005.09927},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Range Conditioned Dilated Convolutions for Scale Invariant 3D Object Detection-Bewley et al-2020.pdf;/Users/sunjiaming/Zotero/storage/TF5CEGB7/2005.html},
  journal = {arXiv:2005.09927 [cs]},
  primaryClass = {cs}
}

@inproceedings{bewleySimpleOnlineRealtime2016,
  title = {Simple Online and Realtime Tracking},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  year = {2016},
  month = sep,
  pages = {3464--3468},
  publisher = {{IEEE}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1109/ICIP.2016.7533003},
  abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  file = {/Users/sunjiaming/Zotero/storage/GYHX62IW/Bewley et al. - 2016 - Simple online and realtime tracking.pdf},
  isbn = {978-1-4673-9961-6},
  language = {en}
}

@article{bhowmikReinforcedFeaturePoints2019,
  title = {Reinforced {{Feature Points}}: {{Optimizing Feature Detection}} and {{Description}} for a {{High}}-{{Level Task}}},
  shorttitle = {Reinforced {{Feature Points}}},
  author = {Bhowmik, Aritra and Gumhold, Stefan and Rother, Carsten and Brachmann, Eric},
  year = {2019},
  month = dec,
  abstract = {We address a core problem of computer vision: Detection and description of 2D feature points for image matching. For a long time, hand-crafted designs, like the seminal SIFT algorithm, were unsurpassed in accuracy and efficiency. Recently, learned feature detectors emerged that implement detection and description using neural networks. Training these networks usually resorts to optimizing low-level matching scores, often pre-defining sets of image patches which should or should not match, or which should or should not contain key points. Unfortunately, increased accuracy for these low-level matching scores does not necessarily translate to better performance in high-level vision tasks. We propose a new training methodology which embeds the feature detector in a complete vision pipeline, and where the learnable parameters are trained in an end-to-end fashion. We overcome the discrete nature of key point selection and descriptor matching using principles from reinforcement learning. As an example, we address the task of relative pose estimation between a pair of images. We demonstrate that the accuracy of a state-of-the-art learning-based feature detector can be increased when trained for the task it is supposed to solve at test time. Our training methodology poses little restrictions on the task to learn, and works for any architecture which predicts key point heat maps, and descriptors for key point locations.},
  archivePrefix = {arXiv},
  eprint = {1912.00623},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reinforced Feature Points-Bhowmik et al-22.pdf;/Users/sunjiaming/Zotero/storage/ID74RQRF/1912.html},
  journal = {arXiv:1912.00623 [cs]},
  primaryClass = {cs}
}

@article{biancoBenchmarkAnalysisRepresentative2018,
  title = {Benchmark {{Analysis}} of {{Representative Deep Neural Network Architectures}}},
  author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
  year = {2018},
  volume = {4},
  pages = {9},
  abstract = {This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Benchmark Analysis of Representative Deep Neural Network Architectures-Bianco et al-2018.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{bianGMSGridbasedMotion,
  title = {{{GMS}}: {{Grid}}-Based {{Motion Statistics}} for {{Fast}}, {{Ultra}}-{{Robust Feature Correspondence}}},
  author = {Bian, JiaWang and Lin, Wen-Yan and Matsushita, Yasuyuki and Yeung, Sai-Kit and Nguyen, Tan-Dat and Cheng, Ming-Ming},
  pages = {10},
  abstract = {Incorporating smoothness constraints into feature matching is known to enable ultra-robust matching. However, such formulations are both complex and slow, making them unsuitable for video applications. This paper proposes GMS (Grid-based Motion Statistics), a simple means of encapsulating motion smoothness as the statistical likelihood of a certain number of matches in a region. GMS enables translation of high match numbers into high match quality. This provides a real-time, ultra-robust correspondence system. Evaluation on videos, with low textures, blurs and wide-baselines show GMS consistently out-performs other real-time matchers and can achieve parity with more sophisticated, much slower techniques.},
  file = {/Users/sunjiaming/Zotero/storage/XUTTDW46/Bian et al. - GMS Grid-based Motion Statistics for Fast, Ultra-.pdf},
  language = {en}
}

@article{bianUnsupervisedScaleconsistentDepth2019,
  title = {Unsupervised {{Scale}}-Consistent {{Depth}} and {{Ego}}-Motion {{Learning}} from {{Monocular Video}}},
  author = {Bian, Jia-Wang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  year = {2019},
  month = aug,
  abstract = {Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions, and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Extensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the standard KITTI and Make3D datasets. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the state-of-the-art model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using monocular video snippets can predict globally scale-consistent camera trajectories over a long video sequence.},
  archivePrefix = {arXiv},
  eprint = {1908.10553},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video-Bian et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RCUWDHIE/1908.html},
  journal = {arXiv:1908.10553 [cs]},
  primaryClass = {cs}
}

@article{biDeep3DCapture2020a,
  title = {Deep {{3D Capture}}: {{Geometry}} and {{Reflectance}} from {{Sparse Multi}}-{{View Images}}},
  shorttitle = {Deep {{3D Capture}}},
  author = {Bi, Sai and Xu, Zexiang and Sunkavalli, Kalyan and Kriegman, David and Ramamoorthi, Ravi},
  year = {2020},
  month = mar,
  abstract = {We introduce a novel learning-based method to reconstruct the high-quality geometry and complex, spatially-varying BRDF of an arbitrary object from a sparse set of only six images captured by wide-baseline cameras under collocated point lighting. We first estimate per-view depth maps using a deep multi-view stereo network; these depth maps are used to coarsely align the different views. We propose a novel multi-view reflectance estimation network architecture that is trained to pool features from these coarsely aligned images and predict per-view spatially-varying diffuse albedo, surface normals, specular roughness and specular albedo. We do this by jointly optimizing the latent space of our multi-view reflectance network to minimize the photometric error between images rendered with our predictions and the input images. While previous state-of-the-art methods fail on such sparse acquisition setups, we demonstrate, via extensive experiments on synthetic and real data, that our method produces high-quality reconstructions that can be used to render photorealistic images.},
  archivePrefix = {arXiv},
  eprint = {2003.12642},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep 3D Capture-Bi et al-22.pdf;/Users/sunjiaming/Zotero/storage/J2Z7T5GA/2003.html},
  journal = {arXiv:2003.12642 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{biDeepReflectanceVolumes,
  title = {Deep {{Reflectance Volumes}}: {{Relightable Reconstructions}} from {{Multi}}-{{View Photometric Images}}},
  author = {Bi, Sai and Xu, Zexiang and Sunkavalli, Kalyan and Ha{\textasciicaron}san, Milo{\textasciicaron}s and Kriegman, David and Ramamoorthi, Ravi},
  pages = {18},
  abstract = {We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Reﬂectance Volumes-Bi et al-.pdf},
  language = {en}
}

@article{biDeepReflectanceVolumes2020,
  title = {Deep {{Reflectance Volumes}}: {{Relightable Reconstructions}} from {{Multi}}-{{View Photometric Images}}},
  shorttitle = {Deep {{Reflectance Volumes}}},
  author = {Bi, Sai and Xu, Zexiang and Sunkavalli, Kalyan and Ha{\v s}an, Milo{\v s} and {Hold-Geoffroy}, Yannick and Kriegman, David and Ramamoorthi, Ravi},
  year = {2020},
  month = jul,
  abstract = {We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.},
  archivePrefix = {arXiv},
  eprint = {2007.09892},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Reflectance Volumes-Bi et al-2020.pdf;/Users/sunjiaming/Zotero/storage/AFECL69J/Bi et al. - 2020 - Deep Reflectance Volumes Relightable Reconstructi.pdf;/Users/sunjiaming/Zotero/storage/87MRTTMD/2007.html},
  journal = {arXiv:2007.09892 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{biNeuralReflectanceFields2020,
  title = {Neural {{Reflectance Fields}} for {{Appearance Acquisition}}},
  author = {Bi, Sai and Xu, Zexiang and Srinivasan, Pratul and Mildenhall, Ben and Sunkavalli, Kalyan and Ha{\v s}an, Milo{\v s} and {Hold-Geoffroy}, Yannick and Kriegman, David and Ramamoorthi, Ravi},
  year = {2020},
  month = aug,
  abstract = {We present neural reflectance fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.},
  archivePrefix = {arXiv},
  eprint = {2008.03824},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Reflectance Fields for Appearance Acquisition-Bi et al-2020.pdf},
  journal = {arXiv:2008.03824 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  language = {en},
  primaryClass = {cs}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pattern recognition and machine learning-Bishop-2006.pdf},
  isbn = {978-0-387-31073-2},
  language = {en},
  lccn = {Q327 .B52 2006},
  series = {Information Science and Statistics}
}

@article{blahaSemanticallyInformedMultiview2017,
  title = {Semantically {{Informed Multiview Surface Refinement}}},
  author = {Blaha, Maros and Rothermel, Mathias and Oswald, Martin R. and Sattler, Torsten and Richard, Audrey and Wegner, Jan D. and Pollefeys, Marc and Schindler, Konrad},
  year = {2017},
  month = jun,
  abstract = {We present a method to jointly refine the geometry and semantic segmentation of 3D surface meshes. Our method alternates between updating the shape and the semantic labels. In the geometry refinement step, the mesh is deformed with variational energy minimization, such that it simultaneously maximizes photo-consistency and the compatibility of the semantic segmentations across a set of calibrated images. Label-specific shape priors account for interactions between the geometry and the semantic labels in 3D. In the semantic segmentation step, the labels on the mesh are updated with MRF inference, such that they are compatible with the semantic segmentations in the input images. Also, this step includes prior assumptions about the surface shape of different semantic classes. The priors induce a tight coupling, where semantic information influences the shape update and vice versa. Specifically, we introduce priors that favor (i) adaptive smoothing, depending on the class label; (ii) straightness of class boundaries; and (iii) semantic labels that are consistent with the surface orientation. The novel mesh-based reconstruction is evaluated in a series of experiments with real and synthetic data. We compare both to state-of-the-art, voxel-based semantic 3D reconstruction, and to purely geometric mesh refinement, and demonstrate that the proposed scheme yields improved 3D geometry as well as an improved semantic segmentation.},
  archivePrefix = {arXiv},
  eprint = {1706.08336},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Semantically Informed Multiview Surface Refinement-Blaha et al-2017.pdf;/Users/sunjiaming/Zotero/storage/BC8MW4CX/1706.html},
  journal = {arXiv:1706.08336 [cs]},
  primaryClass = {cs}
}

@inproceedings{bleyerPatchMatchStereoStereo2011,
  title = {{{PatchMatch Stereo}} - {{Stereo Matching}} with {{Slanted Support Windows}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2011},
  author = {Bleyer, Michael and Rhemann, Christoph and Rother, Carsten},
  year = {2011},
  pages = {14.1-14.11},
  publisher = {{British Machine Vision Association}},
  address = {{Dundee}},
  doi = {10.5244/C.25.14},
  abstract = {Common local stereo methods match support windows at integer-valued disparities. The implicit assumption that pixels within the support region have constant disparity does not hold for slanted surfaces and leads to a bias towards reconstructing frontoparallel surfaces. This work overcomes this bias by estimating an individual 3D plane at each pixel onto which the support region is projected. The major challenge of this approach is to find a pixel's optimal 3D plane among all possible planes whose number is infinite. We show that an ideal algorithm to solve this problem is PatchMatch [1] that we extend to find an approximate nearest neighbor according to a plane. In addition to PatchMatch's spatial propagation scheme, we propose (1) view propagation where planes are propagated among left and right views of the stereo pair and (2) temporal propagation where planes are propagated from preceding and consecutive frames of a video when doing temporal stereo. Adaptive support weights are used in matching cost aggregation to improve results at disparity borders. We also show that our slanted support windows can be used to compute a cost volume for global stereo methods, which allows for explicit treatment of occlusions and can handle large untextured regions. In the results we demonstrate that our method reconstructs highly slanted surfaces and achieves impressive disparity details with sub-pixel precision. In the Middlebury table, our method is currently top-performer among local methods and takes rank 2 among approximately 110 competitors if sub-pixel precision is considered.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PatchMatch Stereo - Stereo Matching with Slanted Support Windows-Bleyer et al-2011.pdf},
  isbn = {978-1-901725-43-8},
  language = {en}
}

@article{bloeschCodeSLAMLearningCompact2018,
  ids = {bloeschCodeSLAMLearningCompact2018a},
  title = {{{CodeSLAM}} - {{Learning}} a {{Compact}}, {{Optimisable Representation}} for {{Dense Visual SLAM}}},
  author = {Bloesch, Michael and Czarnowski, Jan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J.},
  year = {2018},
  month = apr,
  abstract = {The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.},
  archivePrefix = {arXiv},
  eprint = {1804.00874},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM-Bloesch et al-22.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM-Bloesch et al-2018.pdf;/Users/sunjiaming/Zotero/storage/BJTVC4M3/1804.html},
  journal = {arXiv:1804.00874 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{bloeschLearningMeshesDense,
  title = {Learning {{Meshes}} for {{Dense Visual SLAM}}},
  author = {Bloesch, Michael and Laidlow, Tristan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J},
  pages = {10},
  abstract = {Estimating motion and surrounding geometry of a moving camera remains a challenging inference problem. From an information theoretic point of view, estimates should get better as more information is included, such as is done in dense SLAM, but this is strongly dependent on the validity of the underlying models. In the present paper, we use triangular meshes as both compact and dense geometry representation. To allow for simple and fast usage, we propose a view-based formulation for which we predict the in-plane vertex coordinates directly from images and then employ the remaining vertex depth components as free variables. Flexible and continuous integration of information is achieved through the use of a residual based inference technique. This so-called factor graph encodes all information as mapping from free variables to residuals, the squared sum of which is minimised during inference. We propose the use of different types of learnable residuals, which are trained end-to-end to increase their suitability as information bearing models and to enable accurate and reliable estimation. Detailed evaluation of all components is provided on both synthetic and real data which confirms the practicability of the presented approach.},
  file = {/Users/sunjiaming/Zotero/storage/RKBNP2RM/Bloesch et al. - Learning Meshes for Dense Visual SLAM.pdf},
  language = {en}
}

@article{bolyaYOLACTRealtimeInstance2019,
  title = {{{YOLACT}}: {{Real}}-Time {{Instance Segmentation}}},
  shorttitle = {{{YOLACT}}},
  author = {Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae},
  year = {2019},
  month = apr,
  abstract = {We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.},
  archivePrefix = {arXiv},
  eprint = {1904.02689},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/YOLACT-Bolya et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2YKMJQ22/1904.html},
  journal = {arXiv:1904.02689 [cs]},
  primaryClass = {cs}
}

@article{bond-taylorGradientOriginNetworks2020,
  title = {Gradient {{Origin Networks}}},
  author = {{Bond-Taylor}, Sam and Willcocks, Chris G.},
  year = {2020},
  month = jul,
  abstract = {This paper proposes a new type of implicit generative model that is able to quickly learn a latent representation without an explicit encoder. This is achieved with an implicit neural network that takes as inputs points in the coordinate space alongside a latent vector initialised with zeros. The gradients of the data fitting loss with respect to this zero vector are jointly optimised to act as latent points that capture the data manifold. The results show similar characteristics to autoencoders, but with fewer parameters and the advantages of implicit representation networks.},
  archivePrefix = {arXiv},
  eprint = {2007.02798},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Gradient Origin Networks-Bond-Taylor_Willcocks-2020.pdf;/Users/sunjiaming/Zotero/storage/LVFU7CDB/2007.html},
  journal = {arXiv:2007.02798 [cs]},
  keywords = {68T01 (Primary); 68T07 (Secondary),Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.3,I.4.0,I.5.0},
  primaryClass = {cs}
}

@article{bossTwoshotSpatiallyvaryingBRDF2020,
  title = {Two-Shot {{Spatially}}-Varying {{BRDF}} and {{Shape Estimation}}},
  author = {Boss, Mark and Jampani, Varun and Kim, Kihwan and Lensch, Hendrik P. A. and Kautz, Jan},
  year = {2020},
  month = apr,
  abstract = {Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.},
  archivePrefix = {arXiv},
  eprint = {2004.00403},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Two-shot Spatially-varying BRDF and Shape Estimation-Boss et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BV24MM99/2004.html},
  journal = {arXiv:2004.00403 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@book{botschPolygonMeshProcessing2010,
  title = {Polygon Mesh Processing},
  editor = {Botsch, Mario},
  year = {2010},
  publisher = {{A K Peters}},
  address = {{Natick, Mass}},
  annotation = {OCLC: ocn423214772},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Polygon mesh processing-Botsch-2010.pdf},
  isbn = {978-1-56881-426-1},
  language = {en},
  lccn = {QA447 .P62 2010}
}

@article{boudiafMetricLearningCrossentropy2020,
  title = {Metric Learning: Cross-Entropy vs. Pairwise Losses},
  shorttitle = {Metric Learning},
  author = {Boudiaf, Malik and Rony, J{\'e}r{\^o}me and Ziko, Imtiaz Masud and Granger, Eric and Pedersoli, Marco and Piantanida, Pablo and Ayed, Ismail Ben},
  year = {2020},
  month = mar,
  abstract = {Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses and convoluted sample-mining and implementation strategies to ease optimization. The standard cross-entropy loss for classification has been largely overlooked in DML. On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances. However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances. As a result, minimizing the cross-entropy can be seen as an approximate bound-optimization (or Majorize-Minimize) algorithm for minimizing this pairwise loss. Second, we show that, more generally, minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. These findings indicate that the cross-entropy represents a proxy for maximizing the mutual information -- as pairwise losses do -- without the need for complex sample-mining and optimization schemes. Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships. Our experiments over four standard DML benchmarks (CUB200, Cars-196, Stanford Online Product and In-Shop) strongly support our findings. We consistently obtained state-of-the-art results, outperforming many recent and complex DML methods.},
  archivePrefix = {arXiv},
  eprint = {2003.08983},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Metric learning-Boudiaf et al-2020.pdf;/Users/sunjiaming/Zotero/storage/U8NSBFZD/2003.html},
  journal = {arXiv:2003.08983 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{bowmanProbabilisticDataAssociation2017,
  title = {Probabilistic Data Association for Semantic {{SLAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bowman, Sean L. and Atanasov, Nikolay and Daniilidis, Kostas and Pappas, George J.},
  year = {2017},
  month = may,
  pages = {1722--1729},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/ICRA.2017.7989203},
  abstract = {Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Probabilistic data association for semantic SLAM-Bowman et al-2017.pdf},
  isbn = {978-1-5090-4633-1},
  keywords = {semantic slam},
  language = {en}
}

@book{boydIntroductionAppliedLinear2018,
  title = {Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares},
  shorttitle = {Introduction to Applied Linear Algebra},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York, NY}},
  file = {/Users/sunjiaming/Zotero/storage/A7HAJSPH/Boyd and Vandenberghe - 2018 - Introduction to applied linear algebra vectors, m.pdf},
  isbn = {978-1-316-51896-0},
  language = {en},
  lccn = {QA184.2}
}

@article{brachmannDSACDifferentiableRANSAC2018,
  title = {{{DSAC}} - {{Differentiable RANSAC}} for {{Camera Localization}}},
  author = {Brachmann, Eric and Krull, Alexander and Nowozin, Sebastian and Shotton, Jamie and Michel, Frank and Gumhold, Stefan and Rother, Carsten},
  year = {2018},
  month = mar,
  abstract = {RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.},
  archivePrefix = {arXiv},
  eprint = {1611.05705},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DSAC - Differentiable RANSAC for Camera Localization-Brachmann et al-2018.pdf;/Users/sunjiaming/Zotero/storage/AQKCEZRU/1611.html},
  journal = {arXiv:1611.05705 [cs]},
  primaryClass = {cs}
}

@article{brachmannExpertSampleConsensus2019,
  title = {Expert {{Sample Consensus Applied}} to {{Camera Re}}-{{Localization}}},
  author = {Brachmann, Eric and Rother, Carsten},
  year = {2019},
  month = aug,
  abstract = {Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets.},
  archivePrefix = {arXiv},
  eprint = {1908.02484},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Expert Sample Consensus Applied to Camera Re-Localization-Brachmann_Rother-2019.pdf;/Users/sunjiaming/Zotero/storage/385GXIBG/1908.html},
  journal = {arXiv:1908.02484 [cs]},
  primaryClass = {cs}
}

@incollection{brachmannLearning6DObject2014,
  title = {Learning {{6D Object Pose Estimation Using 3D Object Coordinates}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Brachmann, Eric and Krull, Alexander and Michel, Frank and Gumhold, Stefan and Shotton, Jamie and Rother, Carsten},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8690},
  pages = {536--551},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10605-2_35},
  abstract = {This work addresses the problem of estimating the 6D Pose of specific objects from a single RGB-D image. We present a flexible approach that can deal with generic objects, both textured and texture-less. The key new concept is a learned, intermediate representation in form of a dense 3D object coordinate labelling paired with a dense class labelling. We are able to show that for a common dataset with texture-less objects, where template-based techniques are suitable and state of the art, our approach is slightly superior in terms of accuracy. We also demonstrate the benefits of our approach, compared to template-based techniques, in terms of robustness with respect to varying lighting conditions. Towards this end, we contribute a new ground truth dataset with 10k images of 20 objects captured each under three different lighting conditions. We demonstrate that our approach scales well with the number of objects and has capabilities to run fast.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning 6D Object Pose Estimation Using 3D Object Coordinates-Brachmann et al-2014.pdf},
  isbn = {978-3-319-10604-5 978-3-319-10605-2},
  language = {en}
}

@article{brachmannLearningLessMore2018,
  title = {Learning {{Less}} Is {{More}} - {{6D Camera Localization}} via {{3D Surface Regression}}},
  author = {Brachmann, Eric and Rother, Carsten},
  year = {2018},
  month = mar,
  abstract = {Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.},
  archivePrefix = {arXiv},
  eprint = {1711.10228},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Less is More - 6D Camera Localization via 3D Surface Regression-Brachmann_Rother-2018.pdf;/Users/sunjiaming/Zotero/storage/TEZDIHAI/1711.html},
  journal = {arXiv:1711.10228 [cs]},
  primaryClass = {cs}
}

@article{brachmannNeuralGuidedRANSACLearning2019,
  ids = {brachmannNeuralGuidedRANSACLearning2019a},
  title = {Neural-{{Guided RANSAC}}: {{Learning Where}} to {{Sample Model Hypotheses}}},
  shorttitle = {Neural-{{Guided RANSAC}}},
  author = {Brachmann, Eric and Rother, Carsten},
  year = {2019},
  month = may,
  abstract = {We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.},
  archivePrefix = {arXiv},
  eprint = {1905.04132},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural-Guided RANSAC-Brachmann_Rother-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural-Guided RANSAC-Brachmann_Rother-22.pdf;/Users/sunjiaming/Zotero/storage/4X98DY83/1905.html;/Users/sunjiaming/Zotero/storage/S8HDHDU8/1905.html},
  journal = {arXiv:1905.04132 [cs]},
  primaryClass = {cs}
}

@article{bragmanStochasticFilterGroups2019,
  title = {Stochastic {{Filter Groups}} for {{Multi}}-{{Task CNNs}}: {{Learning Specialist}} and {{Generalist Convolution Kernels}}},
  shorttitle = {Stochastic {{Filter Groups}} for {{Multi}}-{{Task CNNs}}},
  author = {Bragman, Felix J. S. and Tanno, Ryutaro and Ourselin, Sebastien and Alexander, Daniel C. and Cardoso, M. Jorge},
  year = {2019},
  month = aug,
  abstract = {The performance of multi-task learning in Convolutional Neural Networks (CNNs) hinges on the design of feature sharing between tasks within the architecture. The number of possible sharing patterns are combinatorial in the depth of the network and the number of tasks, and thus hand-crafting an architecture, purely based on the human intuitions of task relationships can be time-consuming and suboptimal. In this paper, we present a probabilistic approach to learning task-specific and shared representations in CNNs for multi-task learning. Specifically, we propose "stochastic filter groups'' (SFG), a mechanism to assign convolution kernels in each layer to "specialist'' or "generalist'' groups, which are specific to or shared across different tasks, respectively. The SFG modules determine the connectivity between layers and the structures of task-specific and shared representations in the network. We employ variational inference to learn the posterior distribution over the possible grouping of kernels and network parameters. Experiments demonstrate that the proposed method generalises across multiple tasks and shows improved performance over baseline methods.},
  archivePrefix = {arXiv},
  eprint = {1908.09597},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Stochastic Filter Groups for Multi-Task CNNs-Bragman et al-22.pdf;/Users/sunjiaming/Zotero/storage/SR2SIFWX/1908.html},
  journal = {arXiv:1908.09597 [cs]},
  primaryClass = {cs}
}

@article{brazilKinematic3DObject2020,
  title = {Kinematic {{3D Object Detection}} in {{Monocular Video}}},
  author = {Brazil, Garrick and {Pons-Moll}, Gerard and Liu, Xiaoming and Schiele, Bernt},
  year = {2020},
  month = jul,
  abstract = {Perceiving the physical world in 3D is fundamental for self-driving applications. Although temporal motion is an invaluable resource to human vision for detection, tracking, and depth perception, such features have not been thoroughly utilized in modern 3D object detectors. In this work, we propose a novel method for monocular video-based 3D object detection which carefully leverages kinematic motion to improve precision of 3D localization. Specifically, we first propose a novel decomposition of object orientation as well as a self-balancing 3D confidence. We show that both components are critical to enable our kinematic model to work effectively. Collectively, using only a single model, we efficiently leverage 3D kinematics from monocular videos to improve the overall localization precision in 3D object detection while also producing useful by-products of scene dynamics (ego-motion and per-object velocity). We achieve state-of-the-art performance on monocular 3D object detection and the Bird's Eye View tasks within the KITTI self-driving dataset.},
  archivePrefix = {arXiv},
  eprint = {2007.09548},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Kinematic 3D Object Detection in Monocular Video-Brazil et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BEE5E2J5/2007.html},
  journal = {arXiv:2007.09548 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{brazilM3DRPNMonocular3D2019,
  title = {{{M3D}}-{{RPN}}: {{Monocular 3D Region Proposal Network}} for {{Object Detection}}},
  shorttitle = {{{M3D}}-{{RPN}}},
  author = {Brazil, Garrick and Liu, Xiaoming},
  year = {2019},
  month = aug,
  abstract = {Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model.},
  archivePrefix = {arXiv},
  eprint = {1907.06038},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/M3D-RPN-Brazil_Liu-2019.pdf;/Users/sunjiaming/Zotero/storage/Z7JGL8PF/1907.html},
  journal = {arXiv:1907.06038 [cs]},
  primaryClass = {cs}
}

@inproceedings{breglerTrackingPeopleTwists1998,
  title = {Tracking People with Twists and Exponential Maps},
  booktitle = {Proceedings. 1998 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{Cat}}. {{No}}.{{98CB36231}})},
  author = {Bregler, C. and Malik, J.},
  year = {1998},
  pages = {8--15},
  publisher = {{IEEE Comput. Soc}},
  address = {{Santa Barbara, CA, USA}},
  doi = {10.1109/CVPR.1998.698581},
  abstract = {This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-offreedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Tracking people with twists and exponential maps-Bregler_Malik-1998.pdf},
  isbn = {978-0-8186-8497-5},
  language = {en}
}

@article{bubeckConvexOptimizationAlgorithms2014,
  title = {Convex {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  shorttitle = {Convex {{Optimization}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2014},
  month = may,
  abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
  archivePrefix = {arXiv},
  eprint = {1405.4980},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Convex Optimization-Bubeck-2014.pdf},
  journal = {arXiv:1405.4980 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{budvytisLargeScaleJoint2019,
  title = {Large {{Scale Joint Semantic Re}}-{{Localisation}} and {{Scene Understanding}} via {{Globally Unique Instance Coordinate Regression}}},
  author = {Budvytis, Ignas and Teichmann, Marvin and Vojir, Tomas and Cipolla, Roberto},
  year = {2019},
  month = sep,
  abstract = {In this work we present a novel approach to joint semantic localisation and scene understanding. Our work is motivated by the need for localisation algorithms which not only predict 6-DoF camera pose but also simultaneously recognise surrounding objects and estimate 3D geometry. Such capabilities are crucial for computer vision guided systems which interact with the environment: autonomous driving, augmented reality and robotics. In particular, we propose a two step procedure. During the first step we train a convolutional neural network to jointly predict per-pixel globally unique instance labels and corresponding local coordinates for each instance of a static object (e.g. a building). During the second step we obtain scene coordinates by combining object center coordinates and local coordinates and use them to perform 6-DoF camera pose estimation. We evaluate our approach on real world (CamVid-360) and artificial (SceneCity) autonomous driving datasets. We obtain smaller mean distance and angular errors than state-of-the-art 6-DoF pose estimation algorithms based on direct pose regression and pose estimation from scene coordinates on all datasets. Our contributions include: (i) a novel formulation of scene coordinate regression as two separate tasks of object instance recognition and local coordinate regression and a demonstration that our proposed solution allows to predict accurate 3D geometry of static objects and estimate 6-DoF pose of camera on (ii) maps larger by several orders of magnitude than previously attempted by scene coordinate regression methods, as well as on (iii) lightweight, approximate 3D maps built from 3D primitives such as building-aligned cuboids.},
  archivePrefix = {arXiv},
  eprint = {1909.10239},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large Scale Joint Semantic Re-Localisation and Scene Understanding via Globally-Budvytis et al-2019.pdf;/Users/sunjiaming/Zotero/storage/PT8V95AG/1909.html},
  journal = {arXiv:1909.10239 [cs]},
  primaryClass = {cs}
}

@inproceedings{buiNeuralGraphLearning2018,
  title = {Neural {{Graph Learning}}: {{Training Neural Networks Using Graphs}}},
  shorttitle = {Neural {{Graph Learning}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}  - {{WSDM}} '18},
  author = {Bui, Thang D. and Ravi, Sujith and Ramavajjala, Vivek},
  year = {2018},
  pages = {64--71},
  publisher = {{ACM Press}},
  address = {{Marina Del Rey, CA, USA}},
  doi = {10.1145/3159652.3159731},
  abstract = {Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural networks, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training framework with a graph-regularised objective, namely Neural Graph Machines, that can combine the power of neural networks and label propagation. This work generalises previous literature on graph-augmented training of neural networks, enabling it to be applied to multiple neural architectures (Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs, with a runtime that is linear in the number of edges. The proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks (multi-label classification on social graphs, news categorization, document classification and semantic intent classification), with multiple forms of graph inputs (including graphs with and without node-level features) and using different types of neural networks.},
  file = {/Users/sunjiaming/Zotero/storage/A8DADQY3/Bui et al. - 2018 - Neural Graph Learning Training Neural Networks Us.pdf},
  isbn = {978-1-4503-5581-0},
  language = {en}
}

@article{bukschatEfficientPoseEfficientAccurate2020,
  title = {{{EfficientPose}} -- {{An}} Efficient, Accurate and Scalable End-to-End {{6D}} Multi Object Pose Estimation Approach},
  author = {Bukschat, Yannick and Vetter, Marcus},
  year = {2020},
  month = nov,
  abstract = {In this paper we introduce EfficientPose, a new approach for 6D object pose estimation. Our method is highly accurate, efficient and scalable over a wide range of computational resources. Moreover, it can detect the 2D bounding box of multiple objects and instances as well as estimate their full 6D poses in a single shot. This eliminates the significant increase in runtime when dealing with multiple objects other approaches suffer from. These approaches aim to first detect 2D targets, e.g. keypoints, and solve a Perspective-n-Point problem for their 6D pose for each object afterwards. We also propose a novel augmentation method for direct 6D pose estimation approaches to improve performance and generalization, called 6D augmentation. Our approach achieves a new state-of-theart accuracy of 97.35\% in terms of the ADD(-S) metric on the widely-used 6D pose estimation benchmark dataset Linemod using RGB input, while still running end-to-end at over 27 FPS. Through the inherent handling of multiple objects and instances and the fused single shot 2D object detection as well as 6D pose estimation, our approach runs even with multiple objects (eight) end-to-end at over 26 FPS, making it highly attractive to many real world scenarios. Code will be made publicly available at https://github.com/ybkscht/EfficientPose.},
  archivePrefix = {arXiv},
  eprint = {2011.04307},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/EfficientPose -- An efficient, accurate and scalable end-to-end 6D multi object-Bukschat_Vetter-2020.pdf;/Users/sunjiaming/Zotero/storage/SLFIUMR7/2011.html},
  journal = {arXiv:2011.04307 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{burchfielProbabilisticCategoryLevelPose2019,
  title = {Probabilistic {{Category}}-{{Level Pose Estimation}} via {{Segmentation}} and {{Predicted}}-{{Shape Priors}}},
  author = {Burchfiel, Benjamin and Konidaris, George},
  year = {2019},
  month = may,
  abstract = {We introduce a new method for category-level pose estimation which produces a distribution over predicted poses by integrating 3D shape estimates from a generative object model with segmentation information. Given an input depth-image of an object, our variable-time method uses a mixture density network architecture to produce a multi-modal distribution over 3DOF poses; this distribution is then combined with a prior probability encouraging silhouette agreement between the observed input and predicted object pose. Our approach significantly outperforms the current state-of-the-art in category-level 3DOF pose estimation---which outputs a point estimate and does not explicitly incorporate shape and segmentation information---as measured on the Pix3D and ShapeNet datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.12079},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Probabilistic Category-Level Pose Estimation via Segmentation and-Burchfiel_Konidaris-2019.pdf;/Users/sunjiaming/Zotero/storage/EGTA7U8Y/1905.html},
  journal = {arXiv:1905.12079 [cs]},
  primaryClass = {cs}
}

@article{burkovDeepNeuralNetworks,
  title = {Deep {{Neural Networks}} with {{Box Convolutions}}},
  author = {Burkov, Egor and Lempitsky, Victor},
  pages = {11},
  abstract = {Box filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as a part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of network units. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Deep Neural Networks with Box Convolutions-Burkov_Lempitsky-.pdf},
  language = {en}
}

@article{burleyPtexPerFaceTexture2008,
  title = {Ptex: {{Per}}-{{Face Texture Mapping}} for {{Production Rendering}}},
  shorttitle = {Ptex},
  author = {Burley, Brent and Lacewell, Dylan},
  year = {2008},
  month = jun,
  volume = {27},
  pages = {1155--1164},
  issn = {01677055, 14678659},
  doi = {10.1111/j.1467-8659.2008.01253.x},
  abstract = {Explicit parameterization of subdivision surfaces for texture mapping adds significant cost and complexity to film production. Most parameterization methods currently in use require setup effort, and none are completely general. We propose a new texture mapping method for Catmull-Clark subdivision surfaces that requires no explicit parameterization. Our method, Ptex, stores a separate texture per quad face of the subdivision control mesh, along with a novel per-face adjacency map, in a single texture file per surface. Ptex uses the adjacency data to perform seamless anisotropic filtering of multi-resolution textures across surfaces of arbitrary topology. Just as importantly, Ptex requires no manual setup and scales to models of arbitrary mesh complexity and texture detail.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Ptex-Burley_Lacewell-2008.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {4}
}

@article{bustosVisualSLAMWhy2019,
  title = {Visual {{SLAM}}: {{Why Bundle Adjust}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Bustos, Alvaro Parra and Chin, Tat-Jun and Eriksson, Anders and Reid, Ian},
  year = {2019},
  month = feb,
  abstract = {Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion. We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.},
  archivePrefix = {arXiv},
  eprint = {1902.03747},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual SLAM-Bustos et al-2019.pdf;/Users/sunjiaming/Zotero/storage/63PYE9VK/1902.html},
  journal = {arXiv:1902.03747 [cs]},
  keywords = {slam},
  primaryClass = {cs}
}

@article{caesarNuScenesMultimodalDataset2019,
  title = {{{nuScenes}}: {{A}} Multimodal Dataset for Autonomous Driving},
  shorttitle = {{{nuScenes}}},
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  year = {2019},
  month = mar,
  abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven the development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We also define a new metric for 3D detection which consolidates the multiple aspects of the detection task: classification, localization, size, orientation, velocity and attribute estimation. We provide careful dataset analysis as well as baseline performance for lidar and image based detection methods. Data, development kit, and more information are available at www.nuscenes.org.},
  archivePrefix = {arXiv},
  eprint = {1903.11027},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/nuScenes-Caesar et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TNCRIL5T/1903.html},
  journal = {arXiv:1903.11027 [cs, stat]},
  keywords = {dataset},
  primaryClass = {cs, stat}
}

@article{caiReconstructLocallyLocalize,
  title = {Reconstruct {{Locally}}, {{Localize Globally}}: {{A Model Free Method}} for {{Object Pose Estimation}}},
  author = {Cai, Ming and Reid, Ian},
  pages = {11},
  abstract = {Six degree-of-freedom pose estimation of a known object in a single image is a long-standing computer vision objective. It is classically posed as a correspondence problem between a known geometric model, such as a CAD model, and image locations. If a CAD model is not available, it is possible to use multi-view visual reconstruction methods to create a geometric model, and use this in the same manner. Instead, we propose a learning-based method whose input is a collection of images of a target object, and whose output is the pose of the object in a novel view. At inference time, our method maps from the RoI features of the input image to a dense collection of object-centric 3D coordinates, one per pixel. This dense 2D-3D mapping is then used to determine 6dof pose using standard PnP plus RANSAC. The model that maps 2D to object 3D coordinates is established at training time by automatically discovering and matching image landmarks that are consistent across multiple views. We show that this method eliminates the requirement for a 3D CAD model (needed by classical geometry-based methods and state-of-the-art learning based methods alike) but still achieves performance on a par with the prior art.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reconstruct Locally, Localize Globally-Cai_Reid-.pdf},
  language = {en}
}

@article{campbellSolvingBlindPerspectivenPoint2020,
  title = {Solving the {{Blind Perspective}}-n-{{Point Problem End}}-{{To}}-{{End With Robust Differentiable Geometric Optimization}}},
  author = {Campbell, Dylan and Liu, Liu and Gould, Stephen},
  year = {2020},
  month = jul,
  abstract = {Blind Perspective-n-Point (PnP) is the problem of estimating the position and orientation of a camera relative to a scene, given 2D image points and 3D scene points, without prior knowledge of the 2D-3D correspondences. Solving for pose and correspondences simultaneously is extremely challenging since the search space is very large. Fortunately it is a coupled problem: the pose can be found easily given the correspondences and vice versa. Existing approaches assume that noisy correspondences are provided, that a good pose prior is available, or that the problem size is small. We instead propose the first fully end-to-end trainable network for solving the blind PnP problem efficiently and globally, that is, without the need for pose priors. We make use of recent results in differentiating optimization problems to incorporate geometric model fitting into an end-to-end learning framework, including Sinkhorn, RANSAC and PnP algorithms. Our proposed approach significantly outperforms other methods on synthetic and real data.},
  archivePrefix = {arXiv},
  eprint = {2007.14628},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Solving the Blind Perspective-n-Point Problem End-To-End With Robust-Campbell et al-2020.pdf;/Users/sunjiaming/Zotero/storage/M6J59NCS/2007.html},
  journal = {arXiv:2007.14628 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{camposORBSLAM3AccurateOpenSource2020,
  title = {{{ORB}}-{{SLAM3}}: {{An Accurate Open}}-{{Source Library}} for {{Visual}}, {{Visual}}-{{Inertial}} and {{Multi}}-{{Map SLAM}}},
  shorttitle = {{{ORB}}-{{SLAM3}}},
  author = {Campos, Carlos and Elvira, Richard and Rodr{\'i}guez, Juan J. G{\'o}mez and Montiel, Jos{\'e} M. M. and Tard{\'o}s, Juan D.},
  year = {2020},
  month = jul,
  abstract = {This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
  archivePrefix = {arXiv},
  eprint = {2007.11898},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ORB-SLAM3-Campos et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2WIN57GS/2007.html},
  journal = {arXiv:2007.11898 [cs]},
  keywords = {Computer Science - Robotics,neufu_paper},
  primaryClass = {cs}
}

@article{caoLearningIndependentObject2019,
  title = {Learning {{Independent Object Motion}} from {{Unlabelled Stereoscopic Videos}}},
  author = {Cao, Zhe and Kar, Abhishek and Haene, Christian and Malik, Jitendra},
  year = {2019},
  month = jan,
  abstract = {We present a system for learning motion of independently moving objects from stereo videos. The only human annotation used in our system are 2D object bounding boxes which introduce the notion of objects to our system. Unlike prior learning based work which has focused on predicting dense pixel-wise optical flow field and/or a depth map for each image, we propose to predict object instance specific 3D scene flow maps and instance masks from which we are able to derive the motion direction and speed for each object instance. Our network takes the 3D geometry of the problem into account which allows it to correlate the input images. We present experiments evaluating the accuracy of our 3D flow vectors, as well as depth maps and projected 2D optical flow where our jointly learned system outperforms earlier approaches trained for each task independently.},
  archivePrefix = {arXiv},
  eprint = {1901.01971},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Independent Object Motion from Unlabelled Stereoscopic Videos-Cao et al-2019.pdf},
  journal = {arXiv:1901.01971 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{caoPrimeSampleAttention2019,
  title = {Prime {{Sample Attention}} in {{Object Detection}}},
  author = {Cao, Yuhang and Chen, Kai and Loy, Chen Change and Lin, Dahua},
  year = {2019},
  month = apr,
  abstract = {It is a common paradigm in object detection frameworks to treat all samples equally and target at maximizing the performance on average. In this work, we revisit this paradigm through a careful study on how different samples contribute to the overall performance measured in terms of mAP. Our study suggests that the samples in each mini-batch are neither independent nor equally important, and therefore a better classifier on average does not necessarily mean higher mAP. Motivated by this study, we propose the notion of Prime Samples, those that play a key role in driving the detection performance. We further develop a simple yet effective sampling and learning strategy called PrIme Sample Attention (PISA) that directs the focus of the training process towards such samples. Our experiments demonstrate that it is often more effective to focus on prime samples than hard samples when training a detector. Particularly, On the MSCOCO dataset, PISA outperforms the random sampling baseline and hard mining schemes, e.g., OHEM and Focal Loss, consistently by around 2\% on both single-stage and two-stage detectors, even with a strong backbone ResNeXt-101.},
  archivePrefix = {arXiv},
  eprint = {1904.04821},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Prime Sample Attention in Object Detection-Cao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/A2P5P6S9/1904.html},
  journal = {arXiv:1904.04821 [cs]},
  primaryClass = {cs}
}

@article{caoRealtimeHighaccuracy3D,
  title = {Real-Time {{High}}-Accuracy {{3D Reconstruction}} with {{Consumer RGB}}-{{D Cameras}}},
  author = {Cao, Yan-Pei and Kobbelt, Leif and Hu, Shi-Min},
  volume = {1},
  pages = {16},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-time High-accuracy 3D Reconstruction with Consumer RGB-D Cameras-Cao et al-.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {1}
}

@article{carballoLIBREMultiple3D2020,
  title = {{{LIBRE}}: {{The Multiple 3D LiDAR Dataset}}},
  shorttitle = {{{LIBRE}}},
  author = {Carballo, Alexander and Lambert, Jacob and Monrroy, Abraham and Wong, David and Narksri, Patiphon and Kitsukawa, Yuki and Takeuchi, Eijiro and Kato, Shinpei and Takeda, Kazuya},
  year = {2020},
  month = mar,
  abstract = {In this work, we present LIBRE: LiDAR Benchmarking and Reference, a first-of-its-kind dataset featuring 12 different LiDAR sensors, covering a range of manufacturers, models, and laser configurations. Data captured independently from each sensor includes four different environments and configurations: static obstacles placed at known distances and measured from a fixed position within a controlled environment; static obstacles measured from a moving vehicle, captured in a weather chamber where LiDARs were exposed to different conditions (fog, rain, strong light); dynamic objects actively measured from a fixed position by multiple LiDARs mounted side-by-side simultaneously, creating indirect interference conditions; and dynamic traffic objects captured from a vehicle driven on public urban roads multiple times at different times of the day, including data from supporting sensors such as cameras, infrared imaging, and odometry devices. LIBRE will contribute the research community to (1) provide a means for a fair comparison of currently available LiDARs, and (2) facilitate the improvement of existing self-driving vehicles and robotics-related software, in terms of development and tuning of LiDAR-based perception algorithms.},
  archivePrefix = {arXiv},
  eprint = {2003.06129},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LIBRE-Carballo et al-2020.pdf;/Users/sunjiaming/Zotero/storage/AX5P9UB2/2003.html},
  journal = {arXiv:2003.06129 [cs]},
  primaryClass = {cs}
}

@article{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  year = {2020},
  month = may,
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archivePrefix = {arXiv},
  eprint = {2005.12872},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/End-to-End Object Detection with Transformers-Carion et al-2020.pdf;/Users/sunjiaming/Zotero/storage/YRWWECIG/2005.html},
  journal = {arXiv:2005.12872 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{caronDeepClusteringUnsupervised2018,
  title = {Deep {{Clustering}} for {{Unsupervised Learning}} of {{Visual Features}}},
  author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  year = {2018},
  month = jul,
  abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1807.05520},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Clustering for Unsupervised Learning of Visual Features-Caron et al-2018.pdf;/Users/sunjiaming/Zotero/storage/QB8WFJ9S/1807.html},
  journal = {arXiv:1807.05520 [cs]},
  primaryClass = {cs}
}

@article{carreiraHumanPoseEstimation2016,
  title = {Human {{Pose Estimation}} with {{Iterative Error Feedback}}},
  author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
  year = {2016},
  month = jun,
  abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved strong performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). We show that IEF improves over the state-of-the-art on the task of articulated human pose estimation on the challenging MPII dataset.},
  archivePrefix = {arXiv},
  eprint = {1507.06550},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/B9NWS54Y/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf},
  journal = {arXiv:1507.06550 [cs]},
  language = {en},
  primaryClass = {cs}
}

@book{carterVisualGroupTheory2009,
  title = {Visual Group Theory},
  author = {Carter, Nathan C.},
  year = {2009},
  publisher = {{Mathematical Association of America}},
  address = {{Washington, D.C.}},
  annotation = {OCLC: ocn308195514},
  file = {/Users/sunjiaming/Zotero/storage/TNP2H6BK/Carter - 2009 - Visual group theory.pdf},
  isbn = {978-0-88385-757-1},
  language = {en},
  lccn = {QA172.4 .C37 2009},
  series = {Classroom Resource Materials}
}

@article{casasIntentNetLearningPredict,
  title = {{{IntentNet}}: {{Learning}} to {{Predict Intention}} from {{Raw Sensor Data}}},
  author = {Casas, Sergio and Luo, Wenjie and Urtasun, Raquel},
  pages = {10},
  abstract = {In order to plan a safe maneuver, self-driving vehicles need to understand the intent of other traffic participants. We define intent as a combination of discrete high level behaviors as well as continuous trajectories describing future motion. In this paper we develop a one-stage detector and forecaster that exploits both 3D point clouds produced by a LiDAR sensor as well as dynamic maps of the environment. Our multi-task model achieves better accuracy than the respective separate modules while saving computation, which is critical to reduce reaction time in self-driving applications.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/IntentNet-Casas et al-.pdf},
  language = {en}
}

@article{castroAccurate6DObject2019,
  title = {Accurate {{6D Object Pose Estimation}} by {{Pose Conditioned Mesh Reconstruction}}},
  author = {Castro, Pedro and Armagan, Anil and Kim, Tae-Kyun},
  year = {2019},
  month = oct,
  abstract = {Current 6D object pose methods consist of deep CNN models fully optimized for a single object but with its architecture standardized among objects with different shapes. In contrast to previous works, we explicitly exploit each object's distinct topological information i.e. 3D dense meshes in the pose estimation model, with an automated process and prior to any post-processing refinement stage. In order to achieve this, we propose a learning framework in which a Graph Convolutional Neural Network reconstructs a pose conditioned 3D mesh of the object. A robust estimation of the allocentric orientation is recovered by computing, in a differentiable manner, the Procrustes' alignment between the canonical and reconstructed dense 3D meshes. 6D egocentric pose is then lifted using additional mask and 2D centroid projection estimations. Our method is capable of self validating its pose estimation by measuring the quality of the reconstructed mesh, which is invaluable in real life applications. In our experiments on the LINEMOD, OCCLUSION and YCB-Video benchmarks, the proposed method outperforms state-of-the-arts.},
  archivePrefix = {arXiv},
  eprint = {1910.10653},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Accurate 6D Object Pose Estimation by Pose Conditioned Mesh Reconstruction-Castro et al-2019.pdf;/Users/sunjiaming/Zotero/storage/P2YZE8EW/1910.html},
  journal = {arXiv:1910.10653 [cs]},
  primaryClass = {cs}
}

@article{cavalliAdaLAMRevisitingHandcrafted2020,
  title = {{{AdaLAM}}: {{Revisiting Handcrafted Outlier Detection}}},
  shorttitle = {{{AdaLAM}}},
  author = {Cavalli, Luca and Larsson, Viktor and Oswald, Martin Ralf and Sattler, Torsten and Pollefeys, Marc},
  year = {2020},
  month = jun,
  abstract = {Local feature matching is a critical component of many computer vision pipelines, including among others Structure-from-Motion, SLAM, and Visual Localization. However, due to limitations in the descriptors, raw matches are often contaminated by a majority of outliers. As a result, outlier detection is a fundamental problem in computer vision, and a wide range of approaches have been proposed over the last decades. In this paper we revisit handcrafted approaches to outlier filtering. Based on best practices, we propose a hierarchical pipeline for effective outlier detection as well as integrate novel ideas which in sum lead to AdaLAM, an efficient and competitive approach to outlier rejection. AdaLAM is designed to effectively exploit modern parallel hardware, resulting in a very fast, yet very accurate, outlier filter. We validate AdaLAM on multiple large and diverse datasets, and we submit to the Image Matching Challenge (CVPR2020), obtaining competitive results with simple baseline descriptors. We show that AdaLAM is more than competitive to current state of the art, both in terms of efficiency and effectiveness.},
  archivePrefix = {arXiv},
  eprint = {2006.04250},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/AdaLAM-Cavalli et al-2020.pdf;/Users/sunjiaming/Zotero/storage/STLNNJAA/2006.html},
  journal = {arXiv:2006.04250 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{chabotDeepMANTACoarsetofine2017,
  title = {Deep {{MANTA}}: {{A Coarse}}-to-Fine {{Many}}-{{Task Network}} for Joint {{2D}} and {{3D}} Vehicle Analysis from Monocular Image},
  shorttitle = {Deep {{MANTA}}},
  author = {Chabot, Florian and Chaouch, Mohamed and Rabarisoa, Jaonary and Teuli{\`e}re, C{\'e}line and Chateau, Thierry},
  year = {2017},
  month = mar,
  abstract = {In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.},
  archivePrefix = {arXiv},
  eprint = {1703.07570},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep MANTA-Chabot et al-2017.pdf;/Users/sunjiaming/Zotero/storage/S39FBY9I/1703.html},
  journal = {arXiv:1703.07570 [cs]},
  primaryClass = {cs}
}

@article{chabraDeepLocalShapes2020,
  title = {Deep {{Local Shapes}}: {{Learning Local SDF Priors}} for {{Detailed 3D Reconstruction}}},
  shorttitle = {Deep {{Local Shapes}}},
  author = {Chabra, Rohan and Lenssen, Jan Eric and Ilg, Eddy and Schmidt, Tanner and Straub, Julian and Lovegrove, Steven and Newcombe, Richard},
  year = {2020},
  month = mar,
  abstract = {Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.},
  archivePrefix = {arXiv},
  eprint = {2003.10983},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Local Shapes-Chabra et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GMBS8G7C/2003.html},
  journal = {arXiv:2003.10983 [cs]},
  primaryClass = {cs}
}

@article{chabraStereoDRNetDilatedResidual2019,
  title = {{{StereoDRNet}}: {{Dilated Residual Stereo Net}}},
  shorttitle = {{{StereoDRNet}}},
  author = {Chabra, Rohan and Straub, Julian and Sweeny, Chris and Newcombe, Richard and Fuchs, Henry},
  year = {2019},
  month = apr,
  abstract = {We propose a system that uses a convolution neural network (CNN) to estimate depth from a stereo pair followed by volumetric fusion of the predicted depth maps to produce a 3D reconstruction of a scene. Our proposed depth refinement architecture, predicts view-consistent disparity and occlusion maps that helps the fusion system to produce geometrically consistent reconstructions. We utilize 3D dilated convolutions in our proposed cost filtering network that yields better filtering while almost halving the computational cost in comparison to state of the art cost filtering architectures.For feature extraction we use the Vortex Pooling architecture. The proposed method achieves state of the art results in KITTI 2012, KITTI 2015 and ETH 3D stereo benchmarks. Finally, we demonstrate that our system is able to produce high fidelity 3D scene reconstructions that outperforms the state of the art stereo system.},
  archivePrefix = {arXiv},
  eprint = {1904.02251},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/StereoDRNet-Chabra et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XKSGB79S/1904.html},
  journal = {arXiv:1904.02251 [cs]},
  keywords = {disparity},
  primaryClass = {cs}
}

@article{changDeepOpticsMonocular2019,
  title = {Deep {{Optics}} for {{Monocular Depth Estimation}} and {{3D Object Detection}}},
  author = {Chang, Julie and Wetzstein, Gordon},
  year = {2019},
  month = apr,
  abstract = {Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.},
  archivePrefix = {arXiv},
  eprint = {1904.08601},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Optics for Monocular Depth Estimation and 3D Object Detection-Chang_Wetzstein-2019.pdf;/Users/sunjiaming/Zotero/storage/EGBNLAI6/1904.html},
  journal = {arXiv:1904.08601 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{changPyramidStereoMatching2018,
  title = {Pyramid {{Stereo Matching Network}}},
  author = {Chang, Jia-Ren and Chen, Yong-Sheng},
  year = {2018},
  month = mar,
  abstract = {Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in illposed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.},
  archivePrefix = {arXiv},
  eprint = {1803.08669},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pyramid Stereo Matching Network-Chang_Chen-2018.pdf;/Users/sunjiaming/Zotero/storage/R657FXVK/1803.html},
  journal = {arXiv:1803.08669 [cs]},
  primaryClass = {cs}
}

@article{changShapeNetInformationRich3D2015,
  ids = {changShapeNetInformationRich3D2015a},
  title = {{{ShapeNet}}: {{An Information}}-{{Rich 3D Model Repository}}},
  shorttitle = {{{ShapeNet}}},
  author = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
  year = {2015},
  month = dec,
  abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
  archivePrefix = {arXiv},
  eprint = {1512.03012},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Chang et al_2015_ShapeNet.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ShapeNet-Chang et al-2015.pdf;/Users/sunjiaming/Zotero/storage/HTBET6FJ/1512.html;/Users/sunjiaming/Zotero/storage/TYQVQ3WY/1512.html},
  journal = {arXiv:1512.03012 [cs]},
  primaryClass = {cs}
}

@article{chen3DNeighborhoodConvolution2019,
  title = {{{3D Neighborhood Convolution}}: {{Learning Depth}}-{{Aware Features}} for {{RGB}}-{{D}} and {{RGB Semantic Segmentation}}},
  shorttitle = {{{3D Neighborhood Convolution}}},
  author = {Chen, Yunlu and Mensink, Thomas and Gavves, Efstratios},
  year = {2019},
  month = oct,
  abstract = {A key challenge for RGB-D segmentation is how to effectively incorporate 3D geometric information from the depth channel into 2D appearance features. We propose to model the effective receptive field of 2D convolution based on the scale and locality from the 3D neighborhood. Standard convolutions are local in the image space (\$u, v\$), often with a fixed receptive field of 3x3 pixels. We propose to define convolutions local with respect to the corresponding point in the 3D real-world space (\$x, y, z\$), where the depth channel is used to adapt the receptive field of the convolution, which yields the resulting filters invariant to scale and focusing on the certain range of depth. We introduce 3D Neighborhood Convolution (3DN-Conv), a convolutional operator around 3D neighborhoods. Further, we can use estimated depth to use our RGB-D based semantic segmentation model from RGB input. Experimental results validate that our proposed 3DN-Conv operator improves semantic segmentation, using either ground-truth depth (RGB-D) or estimated depth (RGB).},
  archivePrefix = {arXiv},
  eprint = {1910.01460},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Neighborhood Convolution-Chen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QILPJVRZ/1910.html},
  journal = {arXiv:1910.01460 [cs]},
  primaryClass = {cs}
}

@article{chen3DObjectProposals2015,
  title = {{{3D Object Proposals}} for {{Accurate Object Class Detection}}},
  author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  year = {2015},
  pages = {9},
  abstract = {The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D Object Proposals for Accurate Object Class Detection-Chen et al-.pdf},
  keywords = {3d detection},
  language = {en}
}

@article{chenBSPNetGeneratingCompact2019,
  title = {{{BSP}}-{{Net}}: {{Generating Compact Meshes}} via {{Binary Space Partitioning}}},
  shorttitle = {{{BSP}}-{{Net}}},
  author = {Chen, Zhiqin and Tagliasacchi, Andrea and Zhang, Hao},
  year = {2019},
  month = nov,
  abstract = {Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-ofthe-art methods while using much fewer primitives.},
  archivePrefix = {arXiv},
  eprint = {1911.06971},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/BSP-Net-Chen et al-2019.pdf},
  journal = {arXiv:1911.06971 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{chenDropOctaveReducing2019,
  title = {Drop an {{Octave}}: {{Reducing Spatial Redundancy}} in {{Convolutional Neural Networks}} with {{Octave Convolution}}},
  shorttitle = {Drop an {{Octave}}},
  author = {Chen, Yunpeng and Fang, Haoqi and Xu, Bing and Yan, Zhicheng and Kalantidis, Yannis and Rohrbach, Marcus and Yan, Shuicheng and Feng, Jiashi},
  year = {2019},
  month = apr,
  abstract = {In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies and design a novel Octave Convolution (OctConv) operation to store and process feature maps that vary spatially "slower" at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale meth-ods, OctConv is formulated as a single, generic, plug-and-play convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing con-volutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9\% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs.},
  archivePrefix = {arXiv},
  eprint = {1904.05049},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Drop an Octave-Chen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RW8U8B85/1904.html},
  journal = {arXiv:1904.05049 [cs]},
  primaryClass = {cs}
}

@article{chenDSGNDeepStereo2020,
  title = {{{DSGN}}: {{Deep Stereo Geometry Network}} for {{3D Object Detection}}},
  shorttitle = {{{DSGN}}},
  author = {Chen, Yilun and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},
  year = {2020},
  month = jan,
  abstract = {Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors and there remains a large gap in terms of performance between image-based and LiDAR-based methods, caused by inappropriate representation for the prediction in 3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN), reduces this gap significantly by detecting 3D objects on a differentiable volumetric representation -- 3D geometric volume, which effectively encodes 3D geometric structure for 3D regular space. With this representation, we learn depth information and semantic cues simultaneously. For the first time, we provide a simple and effective one-stage stereo-based 3D detection pipeline that jointly estimates the depth and detects 3D objects in an end-to-end learning manner. Our approach outperforms previous stereo-based 3D detectors (about 10 higher in terms of AP) and even achieves comparable performance with a few LiDAR-based methods on the KITTI 3D object detection leaderboard. Code will be made publicly available.},
  archivePrefix = {arXiv},
  eprint = {2001.03398},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DSGN-Chen et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BSPRU7AL/2001.html},
  journal = {arXiv:2001.03398 [cs]},
  primaryClass = {cs}
}

@article{chenEndtoEndLearnableGeometric2019,
  title = {End-to-{{End Learnable Geometric Vision}} by {{Backpropagating PnP Optimization}}},
  author = {Chen, Bo and Chin, Tat-Jun and Parra, Alvaro and Cao, Jiewei and Li, Nan},
  year = {2019},
  month = nov,
  abstract = {Deep networks excel in learning patterns from large amounts of data. On the other hand, many geometric vision tasks are specified as optimization problems. To seamlessly combine deep learning and geometric vision, it is vital to perform learning and geometric optimization end-to-end. Towards this aim, we present BPnP, a novel network layer that backpropagates gradients through a Perspective-n-Points (PnP) solver to guide parameter updates of a neural network. Based on implicit differentiation, we show that the gradients of a ``self-contained" PnP solver can be derived exactly and efficiently, as if the optimizer block were a differentiable layer. We validate BPnP by incorporating it in a deep model that can learn camera intrinsics, camera extrinsics (poses) and 3D structure from large datasets. Further, we develop an end-to-end trainable pipeline for object pose estimation, which achieves greater accuracy by combining feature-based heatmap losses with 2D-3D reprojection errors. Since our approach can be extended to other optimization problems, our work helps to pave the way to perform learnable geometric vision in a principled manner. Our code is available on http://github.com/BoChenYS/BPnP.},
  archivePrefix = {arXiv},
  eprint = {1909.06043},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization-Chen et al-22.pdf;/Users/sunjiaming/Zotero/storage/E2JHKSNW/1909.html},
  journal = {arXiv:1909.06043 [cs]},
  primaryClass = {cs}
}

@article{chenGAPNetGraphAttention2019,
  title = {{{GAPNet}}: {{Graph Attention}} Based {{Point Neural Network}} for {{Exploiting Local Feature}} of {{Point Cloud}}},
  shorttitle = {{{GAPNet}}},
  author = {Chen, Can and Fragonara, Luca Zanotti and Tsourdos, Antonios},
  year = {2019},
  month = may,
  abstract = {Exploiting fine-grained semantic features on point cloud is still challenging due to its irregular and sparse structure in a non-Euclidean space. Among existing studies, PointNet provides an efficient and promising approach to learn shape features directly on unordered 3D point cloud and has achieved competitive performance. However, local feature that is helpful towards better contextual learning is not considered. Meanwhile, attention mechanism shows efficiency in capturing node representation on graph-based data by attending over neighboring nodes. In this paper, we propose a novel neural network for point cloud, dubbed GAPNet, to learn local geometric representations by embedding graph attention mechanism within stacked Multi-Layer-Perceptron (MLP) layers. Firstly, we introduce a GAPLayer to learn attention features for each point by highlighting different attention weights on neighborhood. Secondly, in order to exploit sufficient features, a multi-head mechanism is employed to allow GAPLayer to aggregate different features from independent heads. Thirdly, we propose an attention pooling layer over neighbors to capture local signature aimed at enhancing network robustness. Finally, GAPNet applies stacked MLP layers to attention features and local signature to fully extract local geometric structures. The proposed GAPNet architecture is tested on the ModelNet40 and ShapeNet part datasets, and achieves state-of-the-art performance in both shape classification and part segmentation tasks.},
  archivePrefix = {arXiv},
  eprint = {1905.08705},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GAPNet-Chen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/L3VK7WEY/1905.html},
  journal = {arXiv:1905.08705 [cs]},
  primaryClass = {cs}
}

@article{chengBottomupHigherResolutionNetworks2019,
  title = {Bottom-up {{Higher}}-{{Resolution Networks}} for {{Multi}}-{{Person Pose Estimation}}},
  author = {Cheng, Bowen and Xiao, Bin and Wang, Jingdong and Shi, Honghui and Huang, Thomas S. and Zhang, Lei},
  year = {2019},
  month = nov,
  abstract = {Bottom-up multi-person pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present a Higher-Resolution Network (HigherHRNet) to learn high-resolution feature pyramids. Equipped with mutli-resolution supervision for training and multiresolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize the keypoints, especially for small person, more precisely. The feature pyramid in HigherHRNet consists of the feature map output from HRNet and the upsampled higherresolution one through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5\% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves a state-of-the-art result of 70.5\% AP on COCO test-dev without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.},
  archivePrefix = {arXiv},
  eprint = {1908.10357},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Bottom-up Higher-Resolution Networks for Multi-Person Pose Estimation-Cheng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/W4ZRIYEP/1908.html},
  journal = {arXiv:1908.10357 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{chengDeepStereoUsing2019,
  title = {Deep {{Stereo}} Using {{Adaptive Thin Volume Representation}} with {{Uncertainty Awareness}}},
  author = {Cheng, Shuo and Xu, Zexiang and Zhu, Shilin and Li, Zhuwen and Li, Li Erran and Ramamoorthi, Ravi and Su, Hao},
  year = {2019},
  month = nov,
  abstract = {We present Uncertainty-aware Cascaded Stereo Network (UCS-Net) for 3D reconstruction from multiple RGB images. Multi-view stereo (MVS) aims to reconstruct fine-grained scene geometry from multi-view images. Previous learning-based MVS methods estimate per-view depth using plane sweep volumes with a fixed depth hypothesis at each plane; this generally requires densely sampled planes for desired accuracy, and it is very hard to achieve high-resolution depth. In contrast, we propose adaptive thin volumes (ATVs); in an ATV, the depth hypothesis of each plane is spatially varying, which adapts to the uncertainties of previous per-pixel depth predictions. Our UCS-Net has three stages: the first stage processes a small standard plane sweep volume to predict low-resolution depth; two ATVs are then used in the following stages to refine the depth with higher resolution and higher accuracy. Our ATV consists of only a small number of planes; yet, it efficiently partitions local depth ranges within learned small intervals. In particular, we propose to use variance-based uncertainty estimates to adaptively construct ATVs; this differentiable process introduces reasonable and fine-grained spatial partitioning. Our multi-stage framework progressively subdivides the vast scene space with increasing depth resolution and precision, which enables scene reconstruction with high completeness and accuracy in a coarse-to-fine fashion. We demonstrate that our method achieves superior performance compared with state-of-the-art benchmarks on various challenging datasets.},
  archivePrefix = {arXiv},
  eprint = {1911.12012},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Stereo using Adaptive Thin Volume Representation with Uncertainty Awareness-Cheng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AIFJIVGX/1911.html},
  journal = {arXiv:1911.12012 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{chengHierarchicalNeuralArchitecture2020,
  title = {Hierarchical {{Neural Architecture Search}} for {{Deep Stereo Matching}}},
  author = {Cheng, Xuelian and Zhong, Yiran and Harandi, Mehrtash and Dai, Yuchao and Chang, Xiaojun and Drummond, Tom and Li, Hongdong and Ge, Zongyuan},
  year = {2020},
  month = oct,
  abstract = {To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at https://github.com/XuelianCheng/LEAStereo.},
  archivePrefix = {arXiv},
  eprint = {2010.13501},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Hierarchical Neural Architecture Search for Deep Stereo Matching-Cheng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/PGTIEQB7/2010.html},
  journal = {arXiv:2010.13501 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{chengPanopticDeepLab2019,
  title = {Panoptic-{{DeepLab}}},
  author = {Cheng, Bowen and Collins, Maxwell D. and Zhu, Yukun and Liu, Ting and Huang, Thomas S. and Adam, Hartwig and Chen, Liang-Chieh},
  year = {2019},
  month = oct,
  abstract = {We present Panoptic-DeepLab, a bottom-up and single-shot approach for panoptic segmentation. Our Panoptic-DeepLab is conceptually simple and delivers state-of-the-art results. In particular, we adopt the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. Our single Panoptic-DeepLab sets the new state-of-art at all three Cityscapes benchmarks, reaching 84.2\% mIoU, 38.2\% AP, and 65.5\% PQ on test set, and advances results on the other challenging Mapillary Vista.},
  archivePrefix = {arXiv},
  eprint = {1910.04751},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Panoptic-DeepLab-Cheng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WJFR3YB5/1910.html},
  journal = {arXiv:1910.04751 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{chenLearningImplicitFields2018,
  title = {Learning {{Implicit Fields}} for {{Generative Shape Modeling}}},
  author = {Chen, Zhiqin and Zhang, Hao},
  year = {2018},
  month = dec,
  abstract = {We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Implicit Fields for Generative Shape Modeling-Chen_Zhang-2018.pdf},
  language = {en}
}

@article{chenLearningJoint2D3D,
  title = {Learning {{Joint 2D}}-{{3D Representations}} for {{Depth Completion}}},
  author = {Chen, Yun and Yang, Bin and Liang, Ming and Urtasun, Raquel},
  pages = {10},
  abstract = {In this paper, we tackle the problem of depth completion from RGBD data. Towards this goal, we design a simple yet effective neural network block that learns to extract joint 2D and 3D features. Specifically, the block consists of two domain-specific sub-networks that apply 2D convolution on image pixels and continuous convolution on 3D points, with their output features fused in image space. We build the depth completion network simply by stacking the proposed block, which has the advantage of learning hierarchical representations that are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the effectiveness of our approach on the challenging KITTI depth completion benchmark and show that our approach outperforms the state-of-the-art.},
  file = {/Users/sunjiaming/Zotero/storage/Z3WGBC97/Chen et al. - Learning Joint 2D-3D Representations for Depth Com.pdf},
  language = {en}
}

@article{chenLearningPredict3D2019,
  title = {Learning to {{Predict 3D Objects}} with an {{Interpolation}}-Based {{Differentiable Renderer}}},
  author = {Chen, Wenzheng and Gao, Jun and Ling, Huan and Smith, Edward J. and Lehtinen, Jaakko and Jacobson, Alec and Fidler, Sanja},
  year = {2019},
  month = aug,
  abstract = {Many machine learning models operate on images, but ignore the fact that images are 2D projections formed by 3D geometry interacting with light, in a process called rendering. Enabling ML models to understand image formation might be key for generalization. However, due to an essential rasterization step involving discrete assignment operations, rendering pipelines are non-differentiable and thus largely inaccessible to gradient-based ML techniques. In this paper, we present DIB-R, a differentiable rendering framework which allows gradients to be analytically computed for all pixels in an image. Key to our approach is to view foreground rasterization as a weighted interpolation of local properties and background rasterization as an distance-based aggregation of global geometry. Our approach allows for accurate optimization over vertex positions, colors, normals, light directions and texture coordinates through a variety of lighting models. We showcase our approach in two ML applications: single-image 3D object prediction, and 3D textured object generation, both trained using exclusively using 2D supervision. Our project website is: https://nv-tlabs.github.io/DIB-R/},
  archivePrefix = {arXiv},
  eprint = {1908.01210},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Predict 3D Objects with an Interpolation-based Differentiable-Chen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/EBYJXNHG/1908.html},
  journal = {arXiv:1908.01210 [cs]},
  primaryClass = {cs}
}

@article{chenMotionTransformerTransferringNeural2019,
  title = {{{MotionTransformer}}: {{Transferring Neural Inertial Tracking}} between {{Domains}}},
  shorttitle = {{{MotionTransformer}}},
  author = {Chen, Changhao and Miao, Yishu and Lu, Chris Xiaoxuan and Xie, Linhai and Blunsom, Phil and Markham, Andrew and Trigoni, Niki},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {8009--8016},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33018009},
  abstract = {Inertial information processing plays a pivotal role in egomotion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose MotionTransformer - a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.},
  file = {/Users/sunjiaming/Zotero/storage/QJRBB9D8/Chen et al. - 2019 - MotionTransformer Transferring Neural Inertial Tr.pdf},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  language = {en}
}

@inproceedings{chenMultiview3DObject2017,
  title = {Multi-View {{3D Object Detection Network}} for {{Autonomous Driving}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  year = {2017},
  month = jul,
  pages = {6526--6534},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.691},
  abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Multi-view 3D Object Detection Network for Autonomous Driving-Chen et al-2017.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{chenOptimizingVideoObject2018,
  title = {Optimizing {{Video Object Detection}} via a {{Scale}}-{{Time Lattice}}},
  author = {Chen, Kai and Wang, Jiaqi and Yang, Shuo and Zhang, Xingcheng and Xiong, Yuanjun and Loy, Chen Change and Lin, Dahua},
  year = {2018},
  month = apr,
  abstract = {High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that require detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6\% at 20 fps, or 79.0\% at 62 fps as a performance/speed tradeoff.},
  archivePrefix = {arXiv},
  eprint = {1804.05472},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Optimizing Video Object Detection via a Scale-Time Lattice-Chen et al-2018.pdf},
  journal = {arXiv:1804.05472 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{chenPointBasedMultiViewStereo2019,
  title = {Point-{{Based Multi}}-{{View Stereo Network}}},
  author = {Chen, Rui and Han, Songfang and Xu, Jing and Su, Hao},
  year = {2019},
  month = aug,
  abstract = {We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet .},
  archivePrefix = {arXiv},
  eprint = {1908.04422},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Point-Based Multi-View Stereo Network-Chen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/R4AANNLP/1908.html},
  journal = {arXiv:1908.04422 [cs]},
  primaryClass = {cs}
}

@article{chenSimpleFrameworkContrastive,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  pages = {17},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-ofthe-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
  file = {/Users/sunjiaming/Zotero/storage/H4VW3DAR/Chen et al. - A Simple Framework for Contrastive Learning of Vis.pdf},
  language = {en}
}

@article{chenTensorMaskFoundationDense2019,
  title = {{{TensorMask}}: {{A Foundation}} for {{Dense Object Segmentation}}},
  shorttitle = {{{TensorMask}}},
  author = {Chen, Xinlei and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2019},
  month = mar,
  abstract = {Sliding-window object detectors that generate boundingbox object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense slidingwindow instance segmentation, which is surprisingly underexplored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.},
  archivePrefix = {arXiv},
  eprint = {1903.12174},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TensorMask-Chen et al-2019.pdf},
  journal = {arXiv:1903.12174 [cs]},
  language = {en},
  primaryClass = {cs}
}

@incollection{cherabierLearningPriorsSemantic2018,
  title = {Learning {{Priors}} for {{Semantic 3D Reconstruction}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Cherabier, Ian and Sch{\"o}nberger, Johannes L. and Oswald, Martin R. and Pollefeys, Marc and Geiger, Andreas},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11216},
  pages = {325--341},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01258-8_20},
  abstract = {We present a novel semantic 3D reconstruction framework which embeds variational regularization into a neural network. Our network performs a fixed number of unrolled multi-scale optimization iterations with shared interaction weights. In contrast to existing variational methods for semantic 3D reconstruction, our model is end-to-end trainable and captures more complex dependencies between the semantic labels and the 3D geometry. Compared to previous learning-based approaches to 3D reconstruction, we integrate powerful long-range dependencies using variational coarse-to-fine optimization. As a result, our network architecture requires only a moderate number of parameters while keeping a high level of expressiveness which enables learning from very little data. Experiments on real and synthetic datasets demonstrate that our network achieves higher accuracy compared to a purely variational approach while at the same time requiring two orders of magnitude less iterations to converge. Moreover, our approach handles ten times more semantic class labels using the same computational resources.},
  file = {/Users/sunjiaming/Zotero/storage/6JSBM586/Cherabier et al. - 2018 - Learning Priors for Semantic 3D Reconstruction.pdf},
  isbn = {978-3-030-01257-1 978-3-030-01258-8},
  language = {en}
}

@article{chibaneImplicitFunctionsFeature2020,
  title = {Implicit {{Functions}} in {{Feature Space}} for {{3D Shape Reconstruction}} and {{Completion}}},
  author = {Chibane, Julian and Alldieck, Thiemo and {Pons-Moll}, Gerard},
  year = {2020},
  month = mar,
  abstract = {While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.},
  archivePrefix = {arXiv},
  eprint = {2003.01456},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion-Chibane et al-2020.pdf;/Users/sunjiaming/Zotero/storage/VVLNFBXF/2003.html},
  journal = {arXiv:2003.01456 [cs]},
  primaryClass = {cs}
}

@article{chibaneNeuralUnsignedDistance,
  title = {Neural {{Unsigned Distance Fields}} for {{Implicit Function Learning}}},
  author = {Chibane, Julian and Mir, Aymen and {Pons-Moll}, Gerard},
  pages = {15},
  abstract = {In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet [13] show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at [1].},
  file = {/Users/sunjiaming/Zotero/storage/7UU7SJHQ/Chibane et al. - Neural Unsigned Distance Fields for Implicit Funct.pdf},
  language = {en}
}

@article{chiuProbabilistic3DMultiObject2020,
  ids = {chiuProbabilistic3DMultiObject2020a},
  title = {Probabilistic {{3D Multi}}-{{Object Tracking}} for {{Autonomous Driving}}},
  author = {Chiu, Hsu-kuang and Prioletti, Antonio and Li, Jie and Bohg, Jeannette},
  year = {2020},
  month = jan,
  abstract = {3D multi-object tracking is a key module in autonomous driving applications that provides a reliable dynamic representation of the world to the planning module. In this paper, we present our on-line tracking method, which made the first place in the NuScenes Tracking Challenge, held at the AI Driving Olympics Workshop at NeurIPS 2019. Our method estimates the object states by adopting a Kalman Filter. We initialize the state covariance as well as the process and observation noise covariance with statistics from the training set. We also use the stochastic information from the Kalman Filter in the data association step by measuring the Mahalanobis distance between the predicted object states and current object detections. Our experimental results on the NuScenes validation and test set show that our method outperforms the AB3DMOT baseline method by a large margin in the Average Multi-Object Tracking Accuracy (AMOTA) metric.},
  archivePrefix = {arXiv},
  eprint = {2001.05673},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Chiu et al_2020_Probabilistic 3D Multi-Object Tracking for Autonomous Driving.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Probabilistic 3D Multi-Object Tracking for Autonomous Driving-Chiu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BLHSENVV/2001.html;/Users/sunjiaming/Zotero/storage/YDDBF486/2001.html},
  journal = {arXiv:2001.05673 [cs]},
  primaryClass = {cs}
}

@article{choiLookingRelationsFuture2019,
  title = {Looking to {{Relations}} for {{Future Trajectory Forecast}}},
  author = {Choi, Chiho and Dariush, Behzad},
  year = {2019},
  month = may,
  abstract = {Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on a public benchmark dataset demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {1905.08855},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Looking to Relations for Future Trajectory Forecast-Choi_Dariush-2019.pdf;/Users/sunjiaming/Zotero/storage/MVI3UFFI/1905.html},
  journal = {arXiv:1905.08855 [cs]},
  primaryClass = {cs}
}

@article{choiRobustReconstructionIndoor,
  title = {Robust {{Reconstruction}} of {{Indoor Scenes}}},
  author = {Choi, Sungjoon and Zhou, Qian-Yi and Koltun, Vladlen},
  pages = {10},
  abstract = {We present an approach to indoor scene reconstruction from RGB-D video. The key idea is to combine geometric registration of scene fragments with robust global optimization based on line processes. Geometric registration is error-prone due to sensor noise, which leads to aliasing of geometric detail and inability to disambiguate different surfaces in the scene. The presented optimization approach disables erroneous geometric alignments even when they significantly outnumber correct ones. Experimental results demonstrate that the presented approach substantially increases the accuracy of reconstructed scene models.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust Reconstruction of Indoor Scenes-Choi et al-.pdf},
  keywords = {neufu_paper},
  language = {en}
}

@article{choromanskiRethinkingAttentionPerformers2020,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2020},
  month = sep,
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archivePrefix = {arXiv},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Rethinking Attention with Performers-Choromanski et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ADL77D22/2009.html},
  journal = {arXiv:2009.14794 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{choy3DR2N2UnifiedApproach2016,
  title = {{{3D}}-{{R2N2}}: {{A Unified Approach}} for {{Single}} and {{Multi}}-View {{3D Object Reconstruction}}},
  shorttitle = {{{3D}}-{{R2N2}}},
  author = {Choy, Christopher B. and Xu, Danfei and Gwak, JunYoung and Chen, Kevin and Savarese, Silvio},
  year = {2016},
  month = apr,
  abstract = {Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).},
  archivePrefix = {arXiv},
  eprint = {1604.00449},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-R2N2-Choy et al-2016.pdf;/Users/sunjiaming/Zotero/storage/3BQS3V5H/1604.html},
  journal = {arXiv:1604.00449 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{choy4DSpatioTemporalConvNets2019,
  title = {{{4D Spatio}}-{{Temporal ConvNets}}: {{Minkowski Convolutional Neural Networks}}},
  shorttitle = {{{4D Spatio}}-{{Temporal ConvNets}}},
  author = {Choy, Christopher and Gwak, JunYoung and Savarese, Silvio},
  year = {2019},
  month = apr,
  abstract = {In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, those 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors and propose the generalized sparse convolution that encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for high-dimensional convolutional neural networks. We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases.},
  archivePrefix = {arXiv},
  eprint = {1904.08755},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/4D Spatio-Temporal ConvNets-Choy et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XKJCBL98/1904.html},
  journal = {arXiv:1904.08755 [cs]},
  primaryClass = {cs}
}

@article{choyFullyConvolutionalGeometric,
  title = {Fully {{Convolutional Geometric Features}}},
  author = {Choy, Christopher and Park, Jaesik and Koltun, Vladlen},
  pages = {9},
  abstract = {Extracting geometric features from 3D scans or point clouds is the first step in applications such as registration, reconstruction, and tracking. State-of-the-art methods require computing low-level features as input or extracting patch-based features with limited receptive field. In this work, we present fully-convolutional geometric features, computed in a single pass by a 3D fully-convolutional network. We also present new metric learning losses that dramatically improve performance. Fully-convolutional geometric features are compact, capture broad spatial context, and scale to large scenes. We experimentally validate our approach on both indoor and outdoor datasets. Fullyconvolutional geometric features achieve state-of-the-art accuracy without requiring prepossessing, are compact (32 dimensions), and are 290 times faster than the most accurate prior method.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fully Convolutional Geometric Features-Choy et al-.pdf},
  language = {en}
}

@article{choyUniversalCorrespondenceNetwork2016,
  title = {Universal {{Correspondence Network}}},
  author = {Choy, Christopher B. and Gwak, JunYoung and Savarese, Silvio and Chandraker, Manmohan},
  year = {2016},
  month = jun,
  abstract = {We present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations. In contrast to previous CNN-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity. Our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with \$O(n)\$ feed forward passes for \$n\$ keypoints, instead of \$O(n\^2)\$ for typical patch similarity methods. We propose a convolutional spatial transformer to mimic patch normalization in traditional features like SIFT, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations. Extensive experiments on KITTI, PASCAL, and CUB-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features.},
  archivePrefix = {arXiv},
  eprint = {1606.03558},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Universal Correspondence Network-Choy et al-2016.pdf;/Users/sunjiaming/Zotero/storage/RKHC6TCY/1606.html},
  journal = {arXiv:1606.03558 [cs]},
  primaryClass = {cs}
}

@article{christiansenUnsuperPointEndtoendUnsupervised2019,
  title = {{{UnsuperPoint}}: {{End}}-to-End {{Unsupervised Interest Point Detector}} and {{Descriptor}}},
  shorttitle = {{{UnsuperPoint}}},
  author = {Christiansen, Peter Hviid and Kragh, Mikkel Fly and Brodskiy, Yury and Karstoft, Henrik},
  year = {2019},
  month = jul,
  abstract = {It is hard to create consistent ground truth data for interest points in natural images, since interest points are hard to define clearly and consistently for a human annotator. This makes interest point detectors non-trivial to build. In this work, we introduce an unsupervised deep learning-based interest point detector and descriptor. Using a self-supervised approach, we utilize a siamese network and a novel loss function that enables interest point scores and positions to be learned automatically. The resulting interest point detector and descriptor is UnsuperPoint. We use regression of point positions to 1) make UnsuperPoint end-to-end trainable and 2) to incorporate non-maximum suppression in the model. Unlike most trainable detectors, it requires no generation of pseudo ground truth points, no structure-from-motion-generated representations and the model is learned from only one round of training. Furthermore, we introduce a novel loss function to regularize network predictions to be uniformly distributed. UnsuperPoint runs in real-time with 323 frames per second (fps) at a resolution of \$224\textbackslash times320\$ and 90 fps at \$480\textbackslash times640\$. It is comparable or better than state-of-the-art performance when measured for speed, repeatability, localization, matching score and homography estimation on the HPatch dataset.},
  archivePrefix = {arXiv},
  eprint = {1907.04011},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/UnsuperPoint-Christiansen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/N4S7R6VK/1907.html},
  journal = {arXiv:1907.04011 [cs]},
  primaryClass = {cs}
}

@article{chuExpressiveTelepresenceModular,
  title = {Expressive {{Telepresence}} via {{Modular Codec Avatars}}},
  author = {Chu, Hang and Ma, Shugao},
  pages = {16},
  abstract = {VR telepresence consists of interacting with another human in a virtual space represented by an avatar. Today most avatars are cartoon-like, but soon the technology will allow video-realistic ones. This paper aims in this direction, and presents Modular Codec Avatars (MCA), a method to generate hyper-realistic faces driven by the cameras in the VR headset. MCA extends traditional Codec Avatars (CA) by replacing the holistic models with a learned modular representation. It is important to note that traditional person-specific CAs are learned from few training samples, and typically lack robustness as well as limited expressiveness when transferring facial expressions. MCAs solve these issues by learning a modulated adaptive blending of different facial components as well as an exemplar-based latent alignment. We demonstrate that MCA achieves improved expressiveness and robustness w.r.t to CA in a variety of real-world datasets and practical scenarios. Finally, we showcase new applications in VR telepresence enabled by the proposed model.},
  file = {/Users/sunjiaming/Zotero/storage/YB4LBQ9U/Chu and Ma - Expressive Telepresence via Modular Codec Avatars.pdf},
  language = {en}
}

@book{ChumWernerMatas,
  title = {O. {{Chum}}, {{T}}. {{Werner}}, and {{J}}. {{Matas}}: {{Two}}-View {{Geometry Estimation Unaffected}} by a {{Dominant Plane}}; {{CVPR}} 2005 {{Two}}-View {{Geometry Estimation Unaffected}} by a {{Dominant Plane}}},
  shorttitle = {O. {{Chum}}, {{T}}. {{Werner}}, and {{J}}. {{Matas}}},
  abstract = {A RANSAC-based algorithm for robust estimation of epipo-lar geometry from point correspondences in the possible presence of a dominant scene plane is presented. The al-gorithm handles scenes with (i) all points in a single plane, (ii) majority of points in a single plane and the rest off the plane, (iii) no dominant plane. It is not required to know a priori which of the cases (i)  \textendash{} (iii) occurs. The algorithm exploits a theorem we proved, that if five or more of seven correspondences are related by a homog-raphy then there is an epipolar geometry consistent with the seven-tuple as well as with all correspondences related by the homography. This means that a seven point sample consisting of two outliers and five inliers lying in a domi-nant plane produces an epipolar geometry which is wrong and yet consistent with a high number of correspondences. The theorem explains why RANSAC often fails to estimate epipolar geometry in the presence of a dominant plane. Rather surprisingly, the theorem also implies that RANSAC-based homography estimation is faster when drawing non-minimal samples of seven correspondences than minimal samples of four correspondences. 1.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/O-.pdf;/Users/sunjiaming/Zotero/storage/9V4GT9RV/summary.html}
}

@article{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archivePrefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling-Chung et al-2014.pdf;/Users/sunjiaming/Zotero/storage/J9BHDD4A/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf},
  journal = {arXiv:1412.3555 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{cieslewskiMatchingFeaturesDescriptors2018,
  title = {Matching {{Features}} without {{Descriptors}}: {{Implicitly Matched Interest Points}} ({{IMIPs}})},
  shorttitle = {Matching {{Features}} without {{Descriptors}}},
  author = {Cieslewski, Titus and Bloesch, Michael and Scaramuzza, Davide},
  year = {2018},
  month = nov,
  abstract = {The extraction and matching of interest points is a prerequisite for visual pose estimation and related problems. Traditionally, matching has been achieved by assigning descriptors to interest points and matching points that have similar descriptors. In this paper, we propose a method by which interest points are instead already implicitly matched at detection time. Thanks to this, descriptors do not need to be calculated, stored, communicated, or matched any more. This is achieved by a convolutional neural network with multiple output channels. The i-th interest point is the location of the maximum of the i-th channel, and the i-th interest point in one image is implicitly matched with the i-th interest point in another image. This paper describes how to design and train such a network in a way that results in successful relative pose estimation performance with as little as 128 output channels. While the overall matching score is slightly lower than with traditional methods, the network also outputs the confidence for a specific interest point resulting in a valid match. Most importantly, the approach completely gets rid of descriptors and thus enables localization systems with a significantly smaller memory footprint and multi-agent localization systems that require significantly less bandwidth. We evaluate performance relative to state-of-the-art alternatives.},
  archivePrefix = {arXiv},
  eprint = {1811.10681},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Matching Features without Descriptors-Cieslewski et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Matching Features without Descriptors-Cieslewski et al-22.pdf},
  journal = {arXiv:1811.10681 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{clarkLearningSolveNonlinear,
  title = {Learning to {{Solve Nonlinear Least Squares}} for {{Dense Tracking}} and {{Mapping}}},
  author = {Clark, Ronald and Bloesch, Michael and Czarnowski, Jan and Leutenegger, Stefan and Davison, Andrew J},
  pages = {16},
  abstract = {Sum-of-squares objective functions are very popular in computer vision algorithms. However, these objective functions are not always easy to optimize. The underlying assumptions made by solvers are often not satisfied and many problems are inherently ill-posed. In this paper, we propose a neural nonlinear least squares optimization algorithm which learns to effectively optimize these cost functions even in the presence of adversities. Unlike traditional approaches, the proposed solver requires no hand-crafted regularizers or priors as these are implicitly learned from the data. We apply our method to the problem of motion stereo ie. jointly estimating the motion and scene geometry from pairs of images of a monocular sequence. We show that our learned optimizer is able to efficiently and effectively solve this challenging optimization problem.},
  file = {/Users/sunjiaming/Zotero/storage/5UG8P4N5/Clark et al. - Learning to Solve Nonlinear Least Squares for Dens.pdf},
  language = {en}
}

@book{cohen-orSamplerUsefulComputational2015,
  title = {A {{Sampler}} of {{Useful Computational Tools}} for {{Applied Geometry}}, {{Computer Graphics}}, and {{Image Processing}}},
  editor = {{Cohen-Or}, Daniel and Greif, Chen and Ju, Tao and Mitra, Niloy J. and Shamir, Ariel and {Sorkine-Hornung}, Olga and Zhang, Hao (Richard)},
  year = {2015},
  month = may,
  edition = {Zeroth},
  publisher = {{A K Peters/CRC Press}},
  doi = {10.1201/b18472},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Sampler of Useful Computational Tools for Applied Geometry, Computer-Cohen-Or et al-2015.pdf},
  isbn = {978-0-429-16275-6},
  language = {en}
}

@article{cohen-steinerVariationalShapeApproximation,
  title = {Variational {{Shape Approximation}}},
  author = {{Cohen-Steiner}, David and Alliez, Pierre and Desbrun, Mathieu},
  pages = {10},
  abstract = {A method for concise, faithful approximation of complex 3D datasets is key to reducing the computational cost of graphics applications. Despite numerous applications ranging from geometry compression to reverse engineering, efficiently capturing the geometry of a surface remains a tedious task. In this paper, we present both theoretical and practical contributions that result in a novel and versatile framework for geometric approximation of surfaces. We depart from the usual strategy by casting shape approximation as a variational geometric partitioning problem. Using the concept of geometric proxies, we drive the distortion error down through repeated clustering of faces into best-fitting regions. Our approach is entirely discrete and error-driven, and does not require parameterization or local estimations of differential quantities. We also introduce a new metric based on normal deviation, and demonstrate its superior behavior at capturing anisotropy.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Variational Shape Approximation-Cohen-Steiner et al-.pdf},
  language = {en}
}

@article{cohenGroupEquivariantConvolutional2016,
  title = {Group {{Equivariant Convolutional Networks}}},
  author = {Cohen, Taco S. and Welling, Max},
  year = {2016},
  month = feb,
  abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  archivePrefix = {arXiv},
  eprint = {1602.07576},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Group Equivariant Convolutional Networks-Cohen_Welling-2016.pdf;/Users/sunjiaming/Zotero/storage/USJNL8W2/1602.html},
  journal = {arXiv:1602.07576 [cs, stat]},
  keywords = {feature learning},
  primaryClass = {cs, stat}
}

@article{cohenSphericalCNNs2018,
  title = {Spherical {{CNNs}}},
  author = {Cohen, Taco S. and Geiger, Mario and Koehler, Jonas and Welling, Max},
  year = {2018},
  month = jan,
  abstract = {Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.},
  archivePrefix = {arXiv},
  eprint = {1801.10130},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Spherical CNNs-Cohen et al-2018.pdf;/Users/sunjiaming/Zotero/storage/UMDIJHGS/1801.html},
  journal = {arXiv:1801.10130 [cs, stat]},
  keywords = {feature learning},
  primaryClass = {cs, stat}
}

@article{cordonnierMultiHeadAttentionCollaborate2020,
  title = {Multi-{{Head Attention}}: {{Collaborate Instead}} of {{Concatenate}}},
  shorttitle = {Multi-{{Head Attention}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  year = {2020},
  month = jun,
  abstract = {Attention layers are widely used in natural language processing (NLP) and are beginning to influence computer vision architectures. However, they suffer from over-parameterization. For instance, it was shown that the majority of attention heads could be pruned without impacting accuracy. This work aims to enhance current understanding on how multiple heads interact. Motivated by the observation that trained attention heads share common key/query projections, we propose a collaborative multi-head attention layer that enables heads to learn shared projections. Our scheme improves the computational cost and number of parameters in an attention layer and can be used as a drop-in replacement in any transformer architecture. For instance, by allowing heads to collaborate on a neural machine translation task, we can reduce the key dimension by a factor of eight without any loss in performance. We also show that it is possible to re-parametrize a pre-trained multi-head attention layer into our collaborative attention layer. Even without retraining, collaborative multi-head attention manages to reduce the size of the key and query projections by half without sacrificing accuracy. Our code is public.},
  archivePrefix = {arXiv},
  eprint = {2006.16362},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-Head Attention-Cordonnier et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9SM9FN3R/2006.html},
  journal = {arXiv:2006.16362 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{crawfordLearning3DObjectOriented,
  title = {Learning {{3D Object}}-{{Oriented World Models}} from {{Unlabeled Videos}}},
  author = {Crawford, Eric and Pineau, Joelle},
  pages = {8},
  abstract = {The physical world can be decomposed into discrete 3D objects. Reasoning about the world in terms of these objects may provide a number of advantages to learning agents. For example, objects interact compositionally, and this can support a strong form of generalization. Knowing properties of individual objects and rules for how those properties interact, one can predict the effects that objects will have on one another even if one has never witnessed an interaction between the types of objects in question. The promise of object-level reasoning has fueled a recent surge of interest in systems capable of learning to extract object-oriented representations from perceptual input without supervision. However, the vast majority of such systems treat objects as 2-dimensional entities, effectively ignoring their 3-dimensional nature. In the current work, we propose a probabilistic, object-oriented model equipped with the inductive bias that the world is made up of 3D objects moving through a 3D world, and make a number of structural adaptations which take advantage of that bias. In a series of experiments we show that this system is capable not only of segmenting objects from the perceptual stream, but also of extracting 3D information about objects (e.g. depth) and of tracking them through 3D space.},
  file = {/Users/sunjiaming/Zotero/storage/WSR6VL7Z/Crawford and Pineau - Learning 3D Object-Oriented World Models from Unla.pdf},
  language = {en}
}

@misc{CS107Syllabus,
  title = {{{CS107 Syllabus}}},
  file = {/Users/sunjiaming/Zotero/storage/UMM2PHY5/syllabus.html},
  howpublished = {https://web.stanford.edu/class/cs107/syllabus.html}
}

@inproceedings{curlessVolumetricMethodBuilding1996,
  title = {A Volumetric Method for Building Complex Models from Range Images},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '96},
  author = {Curless, Brian and Levoy, Marc},
  year = {1996},
  pages = {303--312},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/237170.237269},
  abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A volumetric method for building complex models from range images-Curless_Levoy-1996.pdf},
  isbn = {978-0-89791-746-9},
  language = {en}
}

@article{czarnowskiDeepFactorsRealTimeProbabilistic2020,
  title = {{{DeepFactors}}: {{Real}}-{{Time Probabilistic Dense Monocular SLAM}}},
  shorttitle = {{{DeepFactors}}},
  author = {Czarnowski, Jan and Laidlow, Tristan and Clark, Ronald and Davison, Andrew J.},
  year = {2020},
  pages = {1--1},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2020.2965415},
  abstract = {The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepFactors-Czarnowski et al-2020.pdf},
  journal = {IEEE Robotics and Automation Letters},
  language = {en}
}

@article{dahnertJointEmbedding3D2019,
  title = {Joint {{Embedding}} of {{3D Scan}} and {{CAD Objects}}},
  author = {Dahnert, Manuel and Dai, Angela and Guibas, Leonidas and Nie{\ss}ner, Matthias},
  year = {2019},
  month = aug,
  abstract = {3D scan geometry and CAD models often contain complementary information towards understanding environments, which could be leveraged through establishing a mapping between the two domains. However, this is a challenging task due to strong, lower-level differences between scan and CAD geometry. We propose a novel approach to learn a joint embedding space between scan and CAD geometry, where semantically similar objects from both domains lie close together. To achieve this, we introduce a new 3D CNN-based approach to learn a joint embedding space representing object similarities across these domains. To learn a shared space where scan objects and CAD models can interlace, we propose a stacked hourglass approach to separate foreground and background from a scan object, and transform it to a complete, CAD-like representation to produce a shared embedding space. This embedding space can then be used for CAD model retrieval; to further enable this task, we introduce a new dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. Our learned joint embedding outperforms current state of the art for CAD model retrieval by 12\% in instance retrieval accuracy.},
  archivePrefix = {arXiv},
  eprint = {1908.06989},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Embedding of 3D Scan and CAD Objects-Dahnert et al-2019.pdf;/Users/sunjiaming/Zotero/storage/6VR942SG/1908.html},
  journal = {arXiv:1908.06989 [cs]},
  primaryClass = {cs}
}

@article{dai3DMVJoint3DMultiView2018,
  title = {{{3DMV}}: {{Joint 3D}}-{{Multi}}-{{View Prediction}} for {{3D Semantic Scene Segmentation}}},
  shorttitle = {{{3DMV}}},
  author = {Dai, Angela and Nie{\ss}ner, Matthias},
  year = {2018},
  month = mar,
  abstract = {We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D scans in indoor environments using a joint 3D-multi-view prediction network. In contrast to existing methods that either use geometry or RGB data as input for this task, we combine both data modalities in a joint, end-to-end network architecture. Rather than simply projecting color data into a volumetric grid and operating solely in 3D -- which would result in insufficient detail -- we first extract feature maps from associated RGB images. These features are then mapped into the volumetric feature grid of a 3D network using a differentiable backprojection layer. Since our target is 3D scanning scenarios with possibly many frames, we use a multi-view pooling approach in order to handle a varying number of RGB input views. This learned combination of RGB and geometric features with our joint 2D-3D architecture achieves significantly better results than existing baselines. For instance, our final result on the ScanNet 3D segmentation benchmark increases from 52.8\textbackslash\% to 75\textbackslash\% accuracy compared to existing volumetric architectures.},
  archivePrefix = {arXiv},
  eprint = {1803.10409},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3DMV-Dai_Nießner-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3DMV-Dai_Nießner-22.pdf;/Users/sunjiaming/Zotero/storage/R24LGM5P/1803.html},
  journal = {arXiv:1803.10409 [cs]},
  keywords = {3d segmentation},
  primaryClass = {cs}
}

@article{daiBundleFusionRealtimeGlobally2016,
  title = {{{BundleFusion}}: {{Real}}-Time {{Globally Consistent 3D Reconstruction}} Using {{On}}-the-Fly {{Surface Re}}-Integration},
  shorttitle = {{{BundleFusion}}},
  author = {Dai, Angela and Nie{\ss}ner, Matthias and Zollh{\"o}fer, Michael and Izadi, Shahram and Theobalt, Christian},
  year = {2016},
  month = apr,
  abstract = {Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results, but suffer from: (1) needing minutes to perform online correction preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking, and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real-time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real-time to ensure global consistency; all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.},
  archivePrefix = {arXiv},
  eprint = {1604.01093},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/BundleFusion-Dai et al-2016.pdf;/Users/sunjiaming/Zotero/storage/YH62XNPW/1604.html},
  journal = {arXiv:1604.01093 [cs]},
  keywords = {neufu_paper,reconstruction},
  primaryClass = {cs}
}

@article{daiScanNetRichlyannotated3D2017,
  title = {{{ScanNet}}: {{Richly}}-Annotated {{3D Reconstructions}} of {{Indoor Scenes}}},
  shorttitle = {{{ScanNet}}},
  author = {Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  year = {2017},
  month = feb,
  abstract = {A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at http://www.scan-net.org.},
  archivePrefix = {arXiv},
  eprint = {1702.04405},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ScanNet-Dai et al-2017.pdf;/Users/sunjiaming/Zotero/storage/D4FZUWAA/1702.html},
  journal = {arXiv:1702.04405 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{daiSelfsupervisedObjectMotion2019,
  title = {Self-Supervised {{Object Motion}} and {{Depth Estimation}} from {{Video}}},
  author = {Dai, Qi and Patil, Vaishakh and Hecker, Simon and Dai, Dengxin and Van Gool, Luc and Schindler, Konrad},
  year = {2019},
  month = dec,
  abstract = {We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict pixel-wise optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Furthermore, our system eliminates the scale ambiguity of predictions, through employing the pre-computed camera ego-motion and the left-right photometric consistency. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation, and contribute to the depth prediction in dynamic area. Our system outperforms earlier self-supervised approaches in terms of 3D scene flow prediction, and produces comparable results on optical flow estimation.},
  archivePrefix = {arXiv},
  eprint = {1912.04250},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-supervised Object Motion and Depth Estimation from Video-Dai et al-2019.pdf;/Users/sunjiaming/Zotero/storage/X3DYAHED/1912.html},
  journal = {arXiv:1912.04250 [cs]},
  primaryClass = {cs}
}

@article{daiSGNNSparseGenerative2019,
  title = {{{SG}}-{{NN}}: {{Sparse Generative Neural Networks}} for {{Self}}-{{Supervised Scene Completion}} of {{RGB}}-{{D Scans}}},
  shorttitle = {{{SG}}-{{NN}}},
  author = {Dai, Angela and Diller, Christian and Nie{\ss}ner, Matthias},
  year = {2019},
  month = nov,
  abstract = {We present a novel approach that converts partial and noisy RGB-D scans into high-quality 3D scene reconstructions by inferring unobserved scene geometry. Our approach is fully self-supervised and can hence be trained solely on real-world, incomplete scans. To achieve self-supervision, we remove frames from a given (incomplete) 3D scan in order to make it even more incomplete; self-supervision is then formulated by correlating the two levels of partialness of the same scan while masking out regions that have never been observed. Through generalization across a large training set, we can then predict 3D scene completion without ever seeing any 3D scan of entirely complete geometry. Combined with a new 3D sparse generative neural network architecture, our method is able to predict highly-detailed surfaces in a coarse-to-fine hierarchical fashion, generating 3D scenes at 2cm resolution, more than twice the resolution of existing state-of-the-art methods as well as outperforming them by a significant margin in reconstruction quality.},
  archivePrefix = {arXiv},
  eprint = {1912.00036},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SG-NN-Dai et al-2019.pdf;/Users/sunjiaming/Zotero/storage/L5YKH6U4/1912.html},
  journal = {arXiv:1912.00036 [cs]},
  primaryClass = {cs}
}

@article{daiSPSGSelfSupervisedPhotometric2020,
  title = {{{SPSG}}: {{Self}}-{{Supervised Photometric Scene Generation}} from {{RGB}}-{{D Scans}}},
  shorttitle = {{{SPSG}}},
  author = {Dai, Angela and Siddiqui, Yawar and Thies, Justus and Valentin, Julien and Nie{\ss}ner, Matthias},
  year = {2020},
  month = jun,
  abstract = {We present SPSG, a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.},
  archivePrefix = {arXiv},
  eprint = {2006.14660},
  eprinttype = {arxiv},
  journal = {arXiv:2006.14660 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archivePrefix = {arXiv},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/3XFHV8RJ/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/Users/sunjiaming/Zotero/storage/ZR3Z926A/1901.html},
  journal = {arXiv:1901.02860 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{daviesOverfitNeuralNetworks2020,
  title = {Overfit {{Neural Networks}} as a {{Compact Shape Representation}}},
  author = {Davies, Thomas and Nowrouzezahrai, Derek and Jacobson, Alec},
  year = {2020},
  month = sep,
  abstract = {Neural networks have proven to be effective approximators of signed distance fields (SDFs) for solid 3D objects. While prior work has focused on the generalization power of such approximations, we instead explore their suitability as a compact - if purposefully overfit - SDF representation of individual shapes. Specifically, we ask whether neural networks can serve as first-class implicit shape representations in computer graphics. We call such overfit networks Neural Implicits. Similar to SDFs stored on a regular grid, Neural Implicits have fixed storage profiles and memory layout, but afford far greater accuracy. At equal storage cost, Neural Implicits consistently match or exceed the accuracy of irregularly-sampled triangle meshes. We achieve this with a combination of a novel loss function, sampling strategy and supervision protocol designed to facilitate robust shape overfitting. We demonstrate the flexibility of our representation on a variety of standard rendering and modelling tasks.},
  archivePrefix = {arXiv},
  eprint = {2009.09808},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/R2EVWFZE/Davies et al. - 2020 - Overfit Neural Networks as a Compact Shape Represe.pdf;/Users/sunjiaming/Zotero/storage/8NR9FLZE/2009.html},
  journal = {arXiv:2009.09808 [cs]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{davisonFutureMappingComputationalStructure2018,
  ids = {davisonFutureMappingComputationalStructure2018a},
  title = {{{FutureMapping}}: {{The Computational Structure}} of {{Spatial AI Systems}}},
  shorttitle = {{{FutureMapping}}},
  author = {Davison, Andrew J.},
  year = {2018},
  month = mar,
  abstract = {We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic `Spatial AI' perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or comsumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.},
  archivePrefix = {arXiv},
  eprint = {1803.11288},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FutureMapping-Davison-2018.pdf;/Users/sunjiaming/Zotero/storage/7I6FIRCQ/1803.html;/Users/sunjiaming/Zotero/storage/DS4DMQF7/1803.html},
  journal = {arXiv:1803.11288 [cs]},
  keywords = {discussion,slam},
  primaryClass = {cs}
}

@article{davisonFutureMappingGaussianBelief2019,
  title = {{{FutureMapping}} 2: {{Gaussian Belief Propagation}} for {{Spatial AI}}},
  shorttitle = {{{FutureMapping}} 2},
  author = {Davison, Andrew J. and Ortiz, Joseph},
  year = {2019},
  month = oct,
  abstract = {We argue the case for Gaussian Belief Propagation (GBP) as a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation we need in Spatial AI as we aim at high performance smart robots and devices which operate within the constraints of real products. Processor hardware is changing rapidly, and GBP has the right character to take advantage of highly distributed processing and storage while estimating global quantities, as well as great flexibility. We present a detailed tutorial on GBP, relating to the standard factor graph formulation used in robotics and computer vision, and give several simulation examples with code which demonstrate its properties.},
  archivePrefix = {arXiv},
  eprint = {1910.14139},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FutureMapping 2-Davison_Ortiz-2019.pdf;/Users/sunjiaming/Zotero/storage/9JZFSUS8/1910.html},
  journal = {arXiv:1910.14139 [cs]},
  primaryClass = {cs}
}

@incollection{degolImprovedStructureMotion2018,
  title = {Improved {{Structure}} from {{Motion Using Fiducial Marker Matching}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {DeGol, Joseph and Bretl, Timothy and Hoiem, Derek},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11207},
  pages = {281--296},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01219-9_17},
  abstract = {In this paper, we present an incremental structure from motion (SfM) algorithm that significantly outperforms existing algorithms when fiducial markers are present in the scene, and that matches the performance of existing algorithms when no markers are present. Our algorithm uses markers to limit potential incorrect image matches, change the order in which images are added to the reconstruction, and enforce new bundle adjustment constraints. To validate our algorithm, we introduce a new dataset with 16 image collections of large indoor scenes with challenging characteristics (e.g., blank hallways, glass facades, brick walls) and with markers placed throughout. We show that our algorithm produces complete, accurate reconstructions on all 16 image collections, most of which cause other algorithms to fail. Further, by selectively masking fiducial markers, we show that the presence of even a small number of markers can improve the results of our algorithm.},
  file = {/Users/sunjiaming/Zotero/storage/9CLKE8TK/DeGol et al. - 2018 - Improved Structure from Motion Using Fiducial Mark.pdf},
  isbn = {978-3-030-01218-2 978-3-030-01219-9},
  language = {en}
}

@article{deitkeRoboTHOROpenSimulationtoReal2020,
  title = {{{RoboTHOR}}: {{An Open Simulation}}-to-{{Real Embodied AI Platform}}},
  shorttitle = {{{RoboTHOR}}},
  author = {Deitke, Matt and Han, Winson and Herrasti, Alvaro and Kembhavi, Aniruddha and Kolve, Eric and Mottaghi, Roozbeh and Salvador, Jordi and Schwenk, Dustin and VanderBilt, Eli and Wallingford, Matthew and Weihs, Luca and Yatskar, Mark and Farhadi, Ali},
  year = {2020},
  month = apr,
  abstract = {Visual recognition ecosystems (e.g. ImageNet, Pascal, COCO) have undeniably played a prevailing role in the evolution of modern computer vision. We argue that interactive and embodied visual AI has reached a stage of development similar to visual recognition prior to the advent of these ecosystems. Recently, various synthetic environments have been introduced to facilitate research in embodied AI. Notwithstanding this progress, the crucial question of how well models trained in simulation generalize to reality has remained largely unanswered. The creation of a comparable ecosystem for simulation-to-real embodied AI presents many challenges: (1) the inherently interactive nature of the problem, (2) the need for tight alignments between real and simulated worlds, (3) the difficulty of replicating physical conditions for repeatable experiments, (4) and the associated cost. In this paper, we introduce RoboTHOR to democratize research in interactive and embodied visual AI. RoboTHOR offers a framework of simulated environments paired with physical counterparts to systematically explore and overcome the challenges of simulation-to-real transfer, and a platform where researchers across the globe can remotely test their embodied models in the physical world. As a first benchmark, our experiments show there exists a significant gap between the performance of models trained in simulation when they are tested in both simulations and their carefully constructed physical analogs. We hope that RoboTHOR will spur the next stage of evolution in embodied computer vision. RoboTHOR can be accessed at the following link: https://ai2thor.allenai.org/robothor},
  archivePrefix = {arXiv},
  eprint = {2004.06799},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RoboTHOR-Deitke et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EJAJVS8P/2004.html},
  journal = {arXiv:2004.06799 [cs]},
  primaryClass = {cs}
}

@article{dengCerberusMultiheadedDerenderer2019,
  title = {Cerberus: {{A Multi}}-Headed {{Derenderer}}},
  shorttitle = {Cerberus},
  author = {Deng, Boyang and Kornblith, Simon and Hinton, Geoffrey},
  year = {2019},
  month = may,
  abstract = {To generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images, provided the 3D graphics representations are available as labels. When only the unlabeled images are available, however, learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training, the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3D parts from single images without part annotations, and it does quite well at extracting natural parts of human figures.},
  archivePrefix = {arXiv},
  eprint = {1905.11940},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Cerberus-Deng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9LKAF7M3/1905.html},
  journal = {arXiv:1905.11940 [cs]},
  primaryClass = {cs}
}

@article{dengCvxNetsLearnableConvex2019,
  title = {{{CvxNets}}: {{Learnable Convex Decomposition}}},
  shorttitle = {{{CvxNets}}},
  author = {Deng, Boyang and Genova, Kyle and Yazdani, Soroosh and Bouaziz, Sofien and Hinton, Geoffrey and Tagliasacchi, Andrea},
  year = {2019},
  month = sep,
  abstract = {Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental to real-time physics simulation in computer graphics, where it creates a unifying representation of dynamic geometry for collision detection. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an autoencoding process. We investigate the applications of the network including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval.},
  archivePrefix = {arXiv},
  eprint = {1909.05736},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CvxNets-Deng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RWR3JQJ7/1909.html},
  journal = {arXiv:1909.05736 [cs]},
  primaryClass = {cs}
}

@article{dengNASANeuralArticulated2020,
  title = {{{NASA}}: {{Neural Articulated Shape Approximation}}},
  shorttitle = {{{NASA}}},
  author = {Deng, Boyang and Lewis, J. P. and Jeruzalski, Timothy and {Pons-Moll}, Gerard and Hinton, Geoffrey and Norouzi, Mohammad and Tagliasacchi, Andrea},
  year = {2020},
  month = jul,
  abstract = {Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.},
  archivePrefix = {arXiv},
  eprint = {1912.03207},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/NASA-Deng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/K672LUGD/1912.html},
  journal = {arXiv:1912.03207 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{dengPoseRBPFRaoBlackwellizedParticle2019,
  title = {{{PoseRBPF}}: {{A Rao}}-{{Blackwellized Particle Filter}} for {{6D Object Pose Tracking}}},
  shorttitle = {{{PoseRBPF}}},
  author = {Deng, Xinke and Mousavian, Arsalan and Xiang, Yu and Xia, Fei and Bretl, Timothy and Fox, Dieter},
  year = {2019},
  month = may,
  abstract = {Tracking 6D poses of objects from videos provides rich information to a robot in performing different tasks such as manipulation and navigation. In this work, we formulate the 6D object pose tracking problem in the Rao-Blackwellized particle filtering framework, where the 3D rotation and the 3D translation of an object are decoupled. This factorization allows our approach, called PoseRBPF, to efficiently estimate the 3D translation of an object along with the full distribution over the 3D rotation. This is achieved by discretizing the rotation space in a fine-grained manner, and training an auto-encoder network to construct a codebook of feature embeddings for the discretized rotations. As a result, PoseRBPF can track objects with arbitrary symmetries while still maintaining adequate posterior distributions. Our approach achieves state-of-the-art results on two 6D pose estimation benchmarks. A video showing the experiments can be found at https://youtu.be/lE5gjzRKWuA},
  archivePrefix = {arXiv},
  eprint = {1905.09304},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PoseRBPF-Deng et al-2019.pdf;/Users/sunjiaming/Zotero/storage/53RBZN6G/1905.html},
  journal = {arXiv:1905.09304 [cs]},
  primaryClass = {cs}
}

@article{deoConvolutionalSocialPooling2018,
  title = {Convolutional {{Social Pooling}} for {{Vehicle Trajectory Prediction}}},
  author = {Deo, Nachiket and Trivedi, Mohan M.},
  year = {2018},
  month = may,
  abstract = {Forecasting the motion of surrounding vehicles is a critical ability for an autonomous vehicle deployed in complex traffic. Motion of all vehicles in a scene is governed by the traffic context, i.e., the motion and relative spatial configuration of neighboring vehicles. In this paper we propose an LSTM encoder-decoder model that uses convolutional social pooling as an improvement to social pooling layers for robustly learning interdependencies in vehicle motion. Additionally, our model outputs a multi-modal predictive distribution over future trajectories based on maneuver classes. We evaluate our model using the publicly available NGSIM US-101 and I-80 datasets. Our results show improvement over the state of the art in terms of RMS values of prediction error and negative log-likelihoods of true future trajectories under the model's predictive distribution. We also present a qualitative analysis of the model's predicted distributions for various traffic scenarios.},
  archivePrefix = {arXiv},
  eprint = {1805.06771},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Convolutional Social Pooling for Vehicle Trajectory Prediction-Deo_Trivedi-2018.pdf;/Users/sunjiaming/Zotero/storage/KKZXE5RY/1805.html},
  journal = {arXiv:1805.06771 [cs]},
  keywords = {motion prediction},
  primaryClass = {cs}
}

@article{detoneSelfImprovingVisualOdometry2018,
  title = {Self-{{Improving Visual Odometry}}},
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2018},
  month = dec,
  abstract = {We propose a self-supervised learning framework that uses unlabeled monocular video sequences to generate large-scale supervision for training a Visual Odometry (VO) frontend, a network which computes pointwise data associations across images. Our self-improving method enables a VO frontend to learn over time, unlike other VO and SLAM systems which require time-consuming hand-tuning or expensive data collection to adapt to new environments. Our proposed frontend operates on monocular images and consists of a single multi-task convolutional neural network which outputs 2D keypoints locations, keypoint descriptors, and a novel point stability score. We use the output of VO to create a self-supervised dataset of point correspondences to retrain the frontend. When trained using VO at scale on 2.5 million monocular images from ScanNet, the stability classifier automatically discovers a ranking for keypoints that are not likely to help in VO, such as t-junctions across depth discontinuities, features on shadows and highlights, and dynamic objects like people. The resulting frontend outperforms both traditional methods (SIFT, ORB, AKAZE) and deep learning methods (SuperPoint and LF-Net) in a 3D-to-2D pose estimation task on ScanNet.},
  archivePrefix = {arXiv},
  eprint = {1812.03245},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Self-Improving Visual Odometry-DeTone et al-2018.pdf;/Users/sunjiaming/Zotero/storage/ZJYFNRE9/1812.html},
  journal = {arXiv:1812.03245 [cs]},
  keywords = {slam},
  language = {en},
  primaryClass = {cs}
}

@article{detoneSuperPointSelfSupervisedInterest2018,
  title = {{{SuperPoint}}: {{Self}}-{{Supervised Interest Point Detection}} and {{Description}}},
  shorttitle = {{{SuperPoint}}},
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2018},
  month = apr,
  abstract = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.},
  archivePrefix = {arXiv},
  eprint = {1712.07629},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SuperPoint-DeTone et al-22.pdf;/Users/sunjiaming/Zotero/storage/D7WHCMZP/1712.html},
  journal = {arXiv:1712.07629 [cs]},
  primaryClass = {cs}
}

@article{devitoOptDomainSpecific2016,
  title = {Opt: {{A Domain Specific Language}} for {{Non}}-Linear {{Least Squares Optimization}} in {{Graphics}} and {{Imaging}}},
  shorttitle = {Opt},
  author = {DeVito, Zachary and Mara, Michael and Zollh{\"o}fer, Michael and Bernstein, Gilbert and {Ragan-Kelley}, Jonathan and Theobalt, Christian and Hanrahan, Pat and Fisher, Matthew and Nie{\ss}ner, Matthias},
  year = {2016},
  month = apr,
  abstract = {Many graphics and vision problems can be expressed as non-linear least squares optimizations of objective functions over visual data, such as images and meshes. The mathematical descriptions of these functions are extremely concise, but their implementation in real code is tedious, especially when optimized for real-time performance on modern GPUs in interactive applications. In this work, we propose a new language, Opt (available under http://optlang.org), for writing these objective functions over image- or graph-structured unknowns concisely and at a high level. Our compiler automatically transforms these specifications into state-of-the-art GPU solvers based on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate different variations of the solver, so users can easily explore tradeoffs in numerical precision, matrix-free methods, and solver approaches. In our results, we implement a variety of real-world graphics and vision applications. Their energy functions are expressible in tens of lines of code, and produce highly-optimized GPU solver implementations. These solver have performance competitive with the best published hand-tuned, application-specific GPU solvers, and orders of magnitude beyond a general-purpose auto-generated solver.},
  archivePrefix = {arXiv},
  eprint = {1604.06525},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Opt-DeVito et al-2016.pdf;/Users/sunjiaming/Zotero/storage/G5RN28K2/1604.html},
  journal = {arXiv:1604.06525 [cs]},
  keywords = {graphics,optimization},
  primaryClass = {cs}
}

@article{dingDeepMappingUnsupervisedMap2018,
  title = {{{DeepMapping}}: {{Unsupervised Map Estimation From Multiple Point Clouds}}},
  shorttitle = {{{DeepMapping}}},
  author = {Ding, Li and Feng, Chen},
  year = {2018},
  month = nov,
  abstract = {We propose DeepMapping, a novel registration framework using deep neural networks (DNNs) as auxiliary functions to align multiple point clouds from scratch to a globally consistent frame. We use DNNs to model the highly non-convex mapping process that traditionally involves hand-crafted data association, sensor pose initialization, and global refinement. Our key novelty is that properly defining unsupervised losses to ``train'' these DNNs through back-propagation is equivalent to solving the underlying registration problem, yet enables fewer dependencies on good initialization as required by ICP. Our framework contains two DNNs: a localization network that estimates the poses for input point clouds, and a map network that models the scene structure by estimating the occupancy status of global coordinates. This allows us to convert the registration problem to a binary occupancy classification, which can be solved efficiently using gradientbased optimization. We further show that DeepMapping can be readily extended to address the problem of Lidar SLAM by imposing geometric constraints between consecutive point clouds. Experiments are conducted on both simulated and real datasets. Qualitative and quantitative comparisons demonstrate that DeepMapping often enables more robust and accurate global registration of multiple point clouds than existing techniques. Our code is available at http://ai4ce.github.io/DeepMapping/.},
  archivePrefix = {arXiv},
  eprint = {1811.11397},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepMapping-Ding_Feng-22.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/DeepMapping-Ding_Feng-2018.pdf;/Users/sunjiaming/Zotero/storage/ILBYV789/1811.html},
  journal = {arXiv:1811.11397 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{dingLearningDepthGuidedConvolutions2019,
  title = {Learning {{Depth}}-{{Guided Convolutions}} for {{Monocular 3D Object Detection}}},
  author = {Ding, Mingyu and Huo, Yuqi and Yi, Hongwei and Wang, Zhe and Shi, Jianping and Lu, Zhiwu and Luo, Ping},
  year = {2019},
  month = dec,
  abstract = {3D object detection from a single image without LiDAR is a challenging task due to the lack of accurate depth information. Conventional 2D convolutions are unsuitable for this task because they fail to capture local object and its scale information, which are vital for 3D object detection. To better represent 3D structure, prior arts typically transform depth maps estimated from 2D images into a pseudo-LiDAR representation, and then apply existing 3D point-cloud based object detectors. However, their results depend heavily on the accuracy of the estimated depth maps, resulting in suboptimal performance. In this work, instead of using pseudo-LiDAR representation, we improve the fundamental 2D fully convolutions by proposing a new local convolutional network (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D\$\^4\$LCN), where the filters and their receptive fields can be automatically learned from image-based depth maps, making different pixels of different images have different filters. D\$\^4\$LCN overcomes the limitation of conventional 2D convolutions and narrows the gap between image representation and 3D point cloud representation. Extensive experiments show that D\$\^4\$LCN outperforms existing works by large margins. For example, the relative improvement of D\$\^4\$LCN against the state-of-the-art on KITTI is 9.1\textbackslash\% in the moderate setting. The code is available at https://github.com/dingmyu/D4LCN.},
  archivePrefix = {arXiv},
  eprint = {1912.04799},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Depth-Guided Convolutions for Monocular 3D Object Detection-Ding et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VD35R6Y7/1912.html},
  journal = {arXiv:1912.04799 [cs]},
  primaryClass = {cs}
}

@article{doerschTutorialVariationalAutoencoders2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = jun,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archivePrefix = {arXiv},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tutorial on Variational Autoencoders-Doersch-2016.pdf;/Users/sunjiaming/Zotero/storage/3MT7PZ8D/1606.html},
  journal = {arXiv:1606.05908 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{dongVisualInertialSemanticSceneRepresentation2017,
  title = {Visual-{{Inertial}}-{{Semantic Scene Representation}} for {{3D Object Detection}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dong, Jingming and Fei, Xiaohan and Soatto, Stefano},
  year = {2017},
  month = jul,
  pages = {3567--3577},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.380},
  abstract = {We describe a system to detect objects in threedimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network. The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.},
  file = {/Users/sunjiaming/Zotero/storage/VWPATUPT/Dong et al. - 2017 - Visual-Inertial-Semantic Scene Representation for .pdf},
  isbn = {978-1-5386-0457-1},
  keywords = {3d detection,filtering},
  language = {en}
}

@article{donneLearningNonvolumetricDepth,
  title = {Learning {{Non}}-Volumetric {{Depth Fusion}} Using {{Successive Reprojections}}},
  author = {Donne, Simon and Geiger, Andreas},
  pages = {10},
  abstract = {Given a set of input views, multi-view stereopsis techniques estimate depth maps to represent the 3D reconstruction of the scene; these are fused into a single, consistent, reconstruction \textendash{} most often a point cloud. In this work we propose to learn an auto-regressive depth refinement directly from data. While deep learning has improved the accuracy and speed of depth estimation significantly, learned MVS techniques remain limited to the planesweeping paradigm. We refine a set of input depth maps by successively reprojecting information from neighbouring views to leverage multiview constraints. Compared to learning-based volumetric fusion techniques, an image-based representation allows significantly more detailed reconstructions; compared to traditional point-based techniques, our method learns noise suppression and surface completion in a data-driven fashion. Due to the limited availability of high-quality reconstruction datasets with ground truth, we introduce two novel synthetic datasets to (pre-)train our network. Our approach is able to improve both the output depth maps and the reconstructed point cloud, for both learned and traditional depth estimation front-ends, on both synthetic and real data.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Non-volumetric Depth Fusion using Successive Reprojections-Donne_Geiger-.pdf},
  language = {en}
}

@article{douFusion4DRealtimePerformance2016,
  title = {{{Fusion4D}}: Real-Time Performance Capture of Challenging Scenes},
  shorttitle = {{{Fusion4D}}},
  author = {Dou, Mingsong and Taylor, Jonathan and Kohli, Pushmeet and Tankovich, Vladimir and Izadi, Shahram and Khamis, Sameh and Degtyarev, Yury and Davidson, Philip and Fanello, Sean Ryan and Kowdle, Adarsh and Escolano, Sergio Orts and Rhemann, Christoph and Kim, David},
  year = {2016},
  month = jul,
  volume = {35},
  pages = {1--13},
  issn = {07300301},
  doi = {10.1145/2897824.2925969},
  abstract = {We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fusion4D-Dou et al-2016.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {4}
}

@article{duanCurriculumDeepSDF2020,
  title = {Curriculum {{DeepSDF}}},
  author = {Duan, Yueqi and Zhu, Haidong and Wang, He and Yi, Li and Nevatia, Ram and Guibas, Leonidas J.},
  year = {2020},
  month = mar,
  abstract = {When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a "shape curriculum" for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.},
  archivePrefix = {arXiv},
  eprint = {2003.08593},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Curriculum DeepSDF-Duan et al-2020.pdf;/Users/sunjiaming/Zotero/storage/T3MKC888/2003.html},
  journal = {arXiv:2003.08593 [cs]},
  primaryClass = {cs}
}

@inproceedings{dubeSegMap3DSegment2018,
  title = {{{SegMap}}: {{3D Segment Mapping}} Using {{Data}}-{{Driven Descriptors}}},
  shorttitle = {{{SegMap}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIV}}},
  author = {Dub{\'e}, Renaud and Cramariuc, Andrei and Dugas, Daniel and Nieto, Juan and Siegwart, Roland and Cadena, Cesar},
  year = {2018},
  month = jun,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2018.XIV.003},
  abstract = {When performing localization and mapping, working at the level of structure can be advantageous in terms of robustness to environmental changes and differences in illumination. This paper presents SegMap: a map representation solution to the localization and mapping problem based on the extraction of segments in 3D point clouds. In addition to facilitating the computationally intensive task of processing 3D point clouds, working at the level of segments addresses the data compression requirements of real-time single- and multi-robot systems. While current methods extract descriptors for the single task of localization, SegMap leverages a data-driven descriptor in order to extract meaningful features that can also be used for reconstructing a dense 3D map of the environment and for extracting semantic information. This is particularly interesting for navigation tasks and for providing visual feedback to endusers such as robot operators, for example in search and rescue scenarios. These capabilities are demonstrated in multiple urban driving and search and rescue experiments. Our method leads to an increase of area under the ROC curve of 28.3\% over current state of the art using eigenvalue based features. We also obtain very similar reconstruction capabilities to a model specifically trained for this task. The SegMap implementation is available open-source along with easy to run demonstrations at www.github.com/ethz-asl/segmap.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SegMap-Dubé et al-2018.pdf},
  isbn = {978-0-9923747-4-7},
  language = {en}
}

@article{duggalDeepPrunerLearningEfficient2019,
  title = {{{DeepPruner}}: {{Learning Efficient Stereo Matching}} via {{Differentiable PatchMatch}}},
  shorttitle = {{{DeepPruner}}},
  author = {Duggal, Shivam and Wang, Shenlong and Ma, Wei-Chiu and Hu, Rui and Urtasun, Raquel},
  year = {2019},
  month = sep,
  abstract = {Our goal is to significantly speed up the runtime of current state-of-the-art stereo algorithms to enable real-time inference. Towards this goal, we developed a differentiable PatchMatch module that allows us to discard most disparities without requiring full cost volume evaluation. We then exploit this representation to learn which range to prune for each pixel. By progressively reducing the search space and effectively propagating such information, we are able to efficiently compute the cost volume for high likelihood hypotheses and achieve savings in both memory and computation. Finally, an image guided refinement module is exploited to further improve the performance. Since all our components are differentiable, the full network can be trained end-to-end. Our experiments show that our method achieves competitive results on KITTI and SceneFlow datasets while running in real-time at 62ms.},
  archivePrefix = {arXiv},
  eprint = {1909.05845},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepPruner-Duggal et al-2019.pdf;/Users/sunjiaming/Zotero/storage/6WVDK6B7/1909.html},
  journal = {arXiv:1909.05845 [cs]},
  primaryClass = {cs}
}

@article{dusmanuD2NetTrainableCNN2019,
  ids = {dusmanuD2NetTrainableCNN2019a},
  title = {D2-{{Net}}: {{A Trainable CNN}} for {{Joint Detection}} and {{Description}} of {{Local Features}}},
  shorttitle = {D2-{{Net}}},
  author = {Dusmanu, Mihai and Rocco, Ignacio and Pajdla, Tomas and Pollefeys, Marc and Sivic, Josef and Torii, Akihiko and Sattler, Torsten},
  year = {2019},
  month = may,
  abstract = {In this work we address the problem of finding reliable pixel-level correspondences under difficult imaging conditions. We propose an approach where a single convolutional neural network plays a dual role: It is simultaneously a dense feature descriptor and a feature detector. By postponing the detection to a later stage, the obtained keypoints are more stable than their traditional counterparts based on early detection of low-level structures. We show that this model can be trained using pixel correspondences extracted from readily available large-scale SfM reconstructions, without any further annotations. The proposed method obtains state-of-the-art performance on both the difficult Aachen Day-Night localization dataset and the InLoc indoor localization benchmark, as well as competitive performance on other benchmarks for image matching and 3D reconstruction.},
  archivePrefix = {arXiv},
  eprint = {1905.03561},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/D2-Net-Dusmanu et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/D2-Net-Dusmanu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9G4FVUE8/1905.html;/Users/sunjiaming/Zotero/storage/FYR86PVG/1905.html},
  journal = {arXiv:1905.03561 [cs]},
  primaryClass = {cs}
}

@article{dusmanuMultiViewOptimizationLocal2020,
  title = {Multi-{{View Optimization}} of {{Local Feature Geometry}}},
  author = {Dusmanu, Mihai and Sch{\"o}nberger, Johannes L. and Pollefeys, Marc},
  year = {2020},
  month = mar,
  abstract = {In this work, we address the problem of refining the geometry of local image features from multiple views without known scene or camera geometry. Current approaches to local feature detection are inherently limited in their keypoint localization accuracy because they only operate on a single view. This limitation has a negative impact on downstream tasks such as Structure-from-Motion, where inaccurate keypoints lead to large errors in triangulation and camera localization. Our proposed method naturally complements the traditional feature extraction and matching paradigm. We first estimate local geometric transformations between tentative matches and then optimize the keypoint locations over multiple views jointly according to a non-linear least squares formulation. Throughout a variety of experiments, we show that our method consistently improves the triangulation and camera localization performance for both hand-crafted and learned local features.},
  archivePrefix = {arXiv},
  eprint = {2003.08348},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-View Optimization of Local Feature Geometry-Dusmanu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/AWS3A8TE/2003.html},
  journal = {arXiv:2003.08348 [cs]},
  primaryClass = {cs}
}

@article{eigenDepthMapPrediction2014,
  title = {Depth {{Map Prediction}} from a {{Single Image}} Using a {{Multi}}-{{Scale Deep Network}}},
  author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
  year = {2014},
  month = jun,
  abstract = {Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.},
  archivePrefix = {arXiv},
  eprint = {1406.2283},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Depth Map Prediction from a Single Image using a Multi-Scale Deep Network-Eigen et al-2014.pdf;/Users/sunjiaming/Zotero/storage/Z3K5VC5M/1406.html},
  journal = {arXiv:1406.2283 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{eldesokeyUncertaintyAwareCNNsDepth2020,
  title = {Uncertainty-{{Aware CNNs}} for {{Depth Completion}}: {{Uncertainty}} from {{Beginning}} to {{End}}},
  shorttitle = {Uncertainty-{{Aware CNNs}} for {{Depth Completion}}},
  author = {Eldesokey, Abdelrahman and Felsberg, Michael and Holmquist, Karl and Persson, Mikael},
  year = {2020},
  month = jun,
  abstract = {The focus in deep learning research has been mostly to push the limits of prediction accuracy. However, this was often achieved at the cost of increased complexity, raising concerns about the interpretability and the reliability of deep networks. Recently, an increasing attention has been given to untangling the complexity of deep networks and quantifying their uncertainty for different computer vision tasks. Differently, the task of depth completion has not received enough attention despite the inherent noisy nature of depth sensors. In this work, we thus focus on modeling the uncertainty of depth data in depth completion starting from the sparse noisy input all the way to the final prediction. We propose a novel approach to identify disturbed measurements in the input by learning an input confidence estimator in a self-supervised manner based on the normalized convolutional neural networks (NCNNs). Further, we propose a probabilistic version of NCNNs that produces a statistically meaningful uncertainty measure for the final prediction. When we evaluate our approach on the KITTI dataset for depth completion, we outperform all the existing Bayesian Deep Learning approaches in terms of prediction accuracy, quality of the uncertainty measure, and the computational efficiency. Moreover, our small network with 670k parameters performs on-par with conventional approaches with millions of parameters. These results give strong evidence that separating the network into parallel uncertainty and prediction streams leads to state-of-the-art performance with accurate uncertainty estimates.},
  archivePrefix = {arXiv},
  eprint = {2006.03349},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/NTF4Q77U/2006.html},
  journal = {arXiv:2006.03349 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{engelDirectSparseOdometry2016,
  title = {Direct {{Sparse Odometry}}},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  year = {2016},
  month = oct,
  abstract = {We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  archivePrefix = {arXiv},
  eprint = {1607.02565},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Direct Sparse Odometry-Engel et al-2016.pdf;/Users/sunjiaming/Zotero/storage/MW2PGC2J/1607.html},
  journal = {arXiv:1607.02565 [cs]},
  primaryClass = {cs}
}

@article{engelmann3DMPAMultiProposal2020,
  title = {{{3D}}-{{MPA}}: {{Multi Proposal Aggregation}} for {{3D Semantic Instance Segmentation}}},
  shorttitle = {{{3D}}-{{MPA}}},
  author = {Engelmann, Francis and Bokeloh, Martin and Fathi, Alireza and Leibe, Bastian and Nie{\ss}ner, Matthias},
  year = {2020},
  month = mar,
  abstract = {We present 3D-MPA, a method for instance segmentation on 3D point clouds. Given an input point cloud, we propose an object-centric approach where each point votes for its object center. We sample object proposals from the predicted object centers. Then, we learn proposal features from grouped point features that voted for the same object center. A graph convolutional network introduces inter-proposal relations, providing higher-level feature learning in addition to the lower-level point features. Each proposal comprises a semantic label, a set of associated points over which we define a foreground-background mask, an objectness score and aggregation features. Previous works usually perform non-maximum-suppression (NMS) over proposals to obtain the final object detections or semantic instances. However, NMS can discard potentially correct predictions. Instead, our approach keeps all proposals and groups them together based on the learned aggregation features. We show that grouping proposals improves over NMS and outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset.},
  archivePrefix = {arXiv},
  eprint = {2003.13867},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-MPA-Engelmann et al-2020.pdf;/Users/sunjiaming/Zotero/storage/HJMPQT8L/2003.html},
  journal = {arXiv:2003.13867 [cs]},
  primaryClass = {cs}
}

@incollection{engelmannJointObjectPose2016,
  title = {Joint {{Object Pose Estimation}} and {{Shape Reconstruction}} in {{Urban Street Scenes Using 3D Shape Priors}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Engelmann, Francis and St{\"u}ckler, J{\"o}rg and Leibe, Bastian},
  editor = {Rosenhahn, Bodo and Andres, Bjoern},
  year = {2016},
  volume = {9796},
  pages = {219--230},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-45886-1_18},
  abstract = {Estimating the pose and 3D shape of a large variety of instances within an object class from stereo images is a challenging problem, especially in realistic conditions such as urban street scenes. We propose a novel approach for using compact shape manifolds of the shape within an object class for object segmentation, pose and shape estimation. Our method first detects objects and estimates their pose coarsely in the stereo images using a state-of-the-art 3D object detection method. An energy minimization method then aligns shape and pose concurrently with the stereo reconstruction of the object. In experiments, we evaluate our approach for detection, pose and shape estimation of cars in real stereo images of urban street scenes. We demonstrate that our shape manifold alignment method yields improved results over the initial stereo reconstruction and object detection method in depth and pose accuracy.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Joint Object Pose Estimation and Shape Reconstruction in Urban Street Scenes-Engelmann et al-2016.pdf},
  isbn = {978-3-319-45885-4 978-3-319-45886-1},
  language = {en}
}

@inproceedings{engelmannSAMPShapeMotion2017,
  title = {{{SAMP}}: {{Shape}} and {{Motion Priors}} for {{4D Vehicle Reconstruction}}},
  shorttitle = {{{SAMP}}},
  booktitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Engelmann, Francis and Stuckler, Jorg and Leibe, Bastian},
  year = {2017},
  month = mar,
  pages = {400--408},
  publisher = {{IEEE}},
  address = {{Santa Rosa, CA, USA}},
  doi = {10.1109/WACV.2017.51},
  abstract = {Inferring the pose and shape of vehicles in 3D from a movable platform still remains a challenging task due to the projective sensing principle of cameras, difficult surface properties e.g. reflections or transparency, and illumination changes between images. In this paper, we propose to use 3D shape and motion priors to regularize the estimation of the trajectory and the shape of vehicles in sequences of stereo images. We represent shapes by 3D signed distance functions and embed them in a low-dimensional manifold. Our optimization method allows for imposing a common shape across all image observations along an object track. We employ a motion model to regularize the trajectory to plausible object motions. We evaluate our method on the KITTI dataset and show state-of-the-art results in terms of shape reconstruction and pose estimation accuracy.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SAMP-Engelmann et al-2017.pdf},
  isbn = {978-1-5090-4822-9},
  language = {en}
}

@inproceedings{engelSemidenseVisualOdometry2013,
  title = {Semi-Dense {{Visual Odometry}} for a {{Monocular Camera}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
  year = {2013},
  month = dec,
  pages = {1449--1456},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.183},
  abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to benefit from the simplicity and accuracy of dense tracking \textendash which does not depend on visual features \textendash{} while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications.},
  file = {/Users/sunjiaming/Zotero/storage/E9ZXVQ39/Engel et al. - 2013 - Semi-dense Visual Odometry for a Monocular Camera.pdf},
  isbn = {978-1-4799-2840-8},
  language = {en}
}

@article{erlerPoints2SurfLearningImplicit2020,
  title = {{{Points2Surf}}: {{Learning Implicit Surfaces}} from {{Point Cloud Patches}}},
  shorttitle = {{{Points2Surf}}},
  author = {Erler, Philipp and Guerrero, Paul and Ohrhallinger, Stefan and Wimmer, Michael and Mitra, Niloy J.},
  year = {2020},
  month = jul,
  abstract = {A key step in any scanning-based asset creation workflow is to convert unordered point clouds to a surface. Classical methods (e.g., Poisson reconstruction) start to degrade in the presence of noisy and partial scans. Hence, deep learning based methods have recently been proposed to produce complete surfaces, even from partial scans. However, such data-driven methods struggle to generalize to new shapes with large geometric and topological variations. We present Points2Surf, a novel patch-based learning framework that produces accurate surfaces directly from raw scans without normals. Learning a prior over a combination of detailed local patches and coarse global information improves generalization performance and reconstruction accuracy. Our extensive comparison on both synthetic and real data demonstrates a clear advantage of our method over state-of-the-art alternatives on previously unseen classes (on average, Points2Surf brings down reconstruction error by 30\textbackslash\% over SPR and by 270\textbackslash\%+ over deep learning based SotA methods) at the cost of longer computation times and a slight increase in small-scale topological noise in some cases. Our source code, pre-trained model, and dataset are available on: https://github.com/ErlerPhilipp/points2surf},
  archivePrefix = {arXiv},
  eprint = {2007.10453},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Points2Surf-Erler et al-2020.pdf;/Users/sunjiaming/Zotero/storage/8X9QVBPB/2007.html},
  journal = {arXiv:2007.10453 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.5},
  primaryClass = {cs}
}

@article{eslamiNeuralSceneRepresentation2018,
  title = {Neural Scene Representation and Rendering},
  author = {Eslami, S. M. Ali and Jimenez Rezende, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2018},
  month = jun,
  volume = {360},
  pages = {1204--1210},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar6170},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural scene representation and rendering-Eslami et al-2018.pdf},
  journal = {Science},
  language = {en},
  number = {6394}
}

@article{estevesLearningEquivariantRepresentations,
  title = {Learning {{SO}}(3) {{Equivariant Representations}} with {{Spherical CNNs}}},
  author = {Esteves, Carlos and Daniilidis, Kostas and Makadia, Ameesh and {Allec-Blanchette}, Christine},
  pages = {17},
  abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multivalued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning SO(3) Equivariant Representations with Spherical CNNs-Esteves et al-.pdf},
  keywords = {feature learning},
  language = {en}
}

@article{estevesPOLARTRANSFORMERNETWORKS2018,
  title = {{{POLAR TRANSFORMER NETWORKS}}},
  author = {Esteves, Carlos and {Allen-Blanchette}, Christine and Zhou, Xiaowei and Daniilidis, Kostas},
  year = {2018},
  pages = {14},
  abstract = {Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves stateof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/POLAR TRANSFORMER NETWORKS-Esteves et al-2018.pdf},
  keywords = {feature learning},
  language = {en}
}

@article{fabbriLearningDetectTrack2018,
  title = {Learning to {{Detect}} and {{Track Visible}} and {{Occluded Body Joints}} in a {{Virtual World}}},
  author = {Fabbri, Matteo and Lanzi, Fabio and Calderara, Simone and Palazzi, Andrea and Vezzani, Roberto and Cucchiara, Rita},
  year = {2018},
  month = mar,
  abstract = {Multi-People Tracking in an open-world setting requires a special effort in precise detection. Moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. For the purpose, we propose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. Our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. We propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part affinity fields and temporal affinity fields) fed by a time linker feature extractor. To overcome the lack of surveillance data with tracking, body part and occlusion annotations we created the vastest Computer Graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. Our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules.},
  archivePrefix = {arXiv},
  eprint = {1803.08319},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World-Fabbri et al-2018.pdf;/Users/sunjiaming/Zotero/storage/GFHRUMAA/1803.html},
  journal = {arXiv:1803.08319 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{facilCAMConvsCameraAwareMultiScale2019,
  title = {{{CAM}}-{{Convs}}: {{Camera}}-{{Aware Multi}}-{{Scale Convolutions}} for {{Single}}-{{View Depth}}},
  shorttitle = {{{CAM}}-{{Convs}}},
  author = {Facil, Jose M. and Ummenhofer, Benjamin and Zhou, Huizhong and Montesano, Luis and Brox, Thomas and Civera, Javier},
  year = {2019},
  month = apr,
  abstract = {Single-view depth estimation suffers from the problem that a network trained on images from one camera does not generalize to images taken with a different camera model. Thus, changing the camera model requires collecting an entirely new training dataset. In this work, we propose a new type of convolution that can take the camera parameters into account, thus allowing neural networks to learn calibration-aware patterns. Experiments confirm that this improves the generalization capabilities of depth prediction networks considerably, and clearly outperforms the state of the art when the train and test images are acquired with different cameras.},
  archivePrefix = {arXiv},
  eprint = {1904.02028},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CAM-Convs-Facil et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Y37MF5P2/1904.html},
  journal = {arXiv:1904.02028 [cs]},
  primaryClass = {cs}
}

@inproceedings{fanelloHyperDepthLearningDepth2016,
  title = {{{HyperDepth}}: {{Learning Depth}} from {{Structured Light}} without {{Matching}}},
  shorttitle = {{{HyperDepth}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Fanello, Sean Ryan and Rhemann, Christoph and Tankovich, Vladimir and Kowdle, Adarsh and Escolano, Sergio Orts and Kim, David and Izadi, Shahram},
  year = {2016},
  month = jun,
  pages = {5441--5450},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.587},
  abstract = {Structured light sensors are popular due to their robustness to untextured scenes and multipath. These systems triangulate depth by solving a correspondence problem between each camera and projector pixel. This is often framed as a local stereo matching task, correlating patches of pixels in the observed and reference image. However, this is computationally intensive, leading to reduced depth accuracy and framerate. We contribute an algorithm for solving this correspondence problem efficiently, without compromising depth accuracy. For the first time, this problem is cast as a classification-regression task, which we solve extremely efficiently using an ensemble of cascaded random forests. Our algorithm scales in number of disparities, and each pixel can be processed independently, and in parallel. No matching or even access to the corresponding reference pattern is required at runtime, and regressed labels are directly mapped to depth. Our GPU-based algorithm runs at a 1KHz for 1.3MP input/output images, with disparity error of 0.1 subpixels. We show a prototype high framerate depth camera running at 375Hz, useful for solving tracking-related problems. We demonstrate our algorithmic performance, creating high resolution real-time depth maps that surpass the quality of current state of the art depth technologies, highlighting quantization-free results with reduced holes, edge fattening and other stereo-based depth artifacts.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/HyperDepth-Fanello et al-2016.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}

@article{fanPointSetGeneration2016,
  title = {A {{Point Set Generation Network}} for {{3D Object Reconstruction}} from a {{Single Image}}},
  author = {Fan, Haoqiang and Su, Hao and Guibas, Leonidas},
  year = {2016},
  month = dec,
  abstract = {Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.},
  archivePrefix = {arXiv},
  eprint = {1612.00603},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Point Set Generation Network for 3D Object Reconstruction from a Single Image-Fan et al-2016.pdf;/Users/sunjiaming/Zotero/storage/KCCIYTW9/1612.html},
  journal = {arXiv:1612.00603 [cs]},
  primaryClass = {cs}
}

@article{faragherUnderstandingBasisKalman2012,
  title = {Understanding the {{Basis}} of the {{Kalman Filter Via}} a {{Simple}} and {{Intuitive Derivation}} [{{Lecture Notes}}]},
  author = {Faragher, Ramsey},
  year = {2012},
  month = sep,
  volume = {29},
  pages = {128--132},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2203621},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Understanding the Basis of the Kalman Filter Via a Simple and Intuitive-Faragher-2012.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {5}
}

@article{fathyHierarchicalMetricLearning2018,
  title = {Hierarchical {{Metric Learning}} and {{Matching}} for {{2D}} and {{3D Geometric Correspondences}}},
  author = {Fathy, Mohammed E. and Tran, Quoc-Huy and Zia, M. Zeeshan and Vernaza, Paul and Chandraker, Manmohan},
  year = {2018},
  month = mar,
  abstract = {Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.},
  archivePrefix = {arXiv},
  eprint = {1803.07231},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Hierarchical Metric Learning and Matching for 2D and 3D Geometric-Fathy et al-2018.pdf},
  journal = {arXiv:1803.07231 [cs]},
  language = {en},
  primaryClass = {cs}
}

@incollection{fathyHierarchicalMetricLearning2018a,
  title = {Hierarchical {{Metric Learning}} and {{Matching}} for {{2D}} and {{3D Geometric Correspondences}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Fathy, Mohammed E. and Tran, Quoc-Huy and Zia, M. Zeeshan and Vernaza, Paul and Chandraker, Manmohan},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11219},
  pages = {832--850},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01267-0_49},
  abstract = {Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform handcrafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Hierarchical Metric Learning and Matching for 2D and 3D Geometric-Fathy et al-3.pdf},
  isbn = {978-3-030-01266-3 978-3-030-01267-0},
  language = {en}
}

@incollection{fathyHierarchicalMetricLearning2018b,
  title = {Hierarchical {{Metric Learning}} and {{Matching}} for {{2D}} and {{3D Geometric Correspondences}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Fathy, Mohammed E. and Tran, Quoc-Huy and Zia, M. Zeeshan and Vernaza, Paul and Chandraker, Manmohan},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11219},
  pages = {832--850},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01267-0_49},
  abstract = {Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform handcrafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Hierarchical Metric Learning and Matching for 2D and 3D Geometric-Fathy et al-22.pdf},
  isbn = {978-3-030-01266-3 978-3-030-01267-0},
  language = {en}
}

@article{feichtenhoferDetectTrackTrack2017,
  title = {Detect to {{Track}} and {{Track}} to {{Detect}}},
  author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  year = {2017},
  month = oct,
  abstract = {Recent approaches for high accuracy detection and tracking of object categories in video consist of complex multistage solutions that become more cumbersome each year. In this paper we propose a ConvNet architecture that jointly performs detection and tracking, solving the task in a simple and effective way. Our contributions are threefold: (i) we set up a ConvNet architecture for simultaneous detection and tracking, using a multi-task objective for frame-based object detection and across-frame track regression; (ii) we introduce correlation features that represent object co-occurrences across time to aid the ConvNet during tracking; and (iii) we link the frame level detections based on our across-frame tracklets to produce high accuracy detections at the video level. Our ConvNet architecture for spatiotemporal object detection is evaluated on the large-scale ImageNet VID dataset where it achieves state-of-the-art results. Our approach provides better single model performance than the winning method of the last ImageNet challenge while being conceptually much simpler. Finally, we show that by increasing the temporal stride we can dramatically increase the tracker speed.},
  archivePrefix = {arXiv},
  eprint = {1710.03958},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Detect to Track and Track to Detect-Feichtenhofer et al-2017.pdf;/Users/sunjiaming/Zotero/storage/CT4FFANI/1710.html},
  journal = {arXiv:1710.03958 [cs]},
  primaryClass = {cs}
}

@article{feiVisualInertialObjectDetection2018,
  title = {Visual-{{Inertial Object Detection}} and {{Mapping}}},
  author = {Fei, Xiaohan and Soatto, Stefano},
  year = {2018},
  month = jun,
  abstract = {We present a method to populate an unknown environment with models of previously seen objects, placed in a Euclidean reference frame that is inferred causally and on-line using monocular video along with inertial sensors. The system we implement returns a sparse point cloud for the regions of the scene that are visible but not recognized as a previously seen object, and a detailed object model and its pose in the Euclidean frame otherwise. The system includes bottom-up and topdown components, whereby deep networks trained for detection provide likelihood scores for object hypotheses provided by a nonlinear filter, whose state serves as memory. Additional networks provide likelihood scores for edges, which complements detection networks trained to be invariant to small deformations. We test our algorithm on existing datasets, and also introduce the VISMA dataset, that provides ground truth pose, point-cloud map, and object models, along with time-stamped inertial measurements.},
  archivePrefix = {arXiv},
  eprint = {1806.08498},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Visual-Inertial Object Detection and Mapping-Fei_Soatto-2018.pdf},
  journal = {arXiv:1806.08498 [cs]},
  language = {en},
  primaryClass = {cs}
}

@incollection{feiVisualInertialObjectDetection2018a,
  title = {Visual-{{Inertial Object Detection}} and {{Mapping}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Fei, Xiaohan and Soatto, Stefano},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11215},
  pages = {318--334},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01252-6_19},
  abstract = {We present a method to populate an unknown environment with models of previously seen objects, placed in a Euclidean reference frame that is inferred causally and on-line using monocular video along with inertial sensors. The system we implement returns a sparse point cloud for the regions of the scene that are visible but not recognized as a previously seen object, and a detailed object model and its pose in the Euclidean frame otherwise. The system includes bottom-up and topdown components, whereby deep networks trained for detection provide likelihood scores for object hypotheses provided by a nonlinear filter, whose state serves as memory. Additional networks provide likelihood scores for edges, which complements detection networks trained to be invariant to small deformations. We test our algorithm on existing datasets, and also introduce the VISMA dataset, that provides ground truth pose, point-cloud map, and object models, along with time-stamped inertial measurements.},
  file = {/Users/sunjiaming/Zotero/storage/HAJWGNHV/Fei and Soatto - 2018 - Visual-Inertial Object Detection and Mapping.pdf},
  isbn = {978-3-030-01251-9 978-3-030-01252-6},
  keywords = {3d detection,filtering},
  language = {en}
}

@article{felzenszwalbEfficientGraphBasedImage2004,
  title = {Efficient {{Graph}}-{{Based Image Segmentation}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  year = {2004},
  month = sep,
  volume = {59},
  pages = {167--181},
  issn = {0920-5691},
  doi = {10.1023/B:VISI.0000022288.19776.77},
  abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Efficient Graph-Based Image Segmentation-Felzenszwalb_Huttenlocher-2004.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@article{fernandez-labradorUnsupervisedLearningCategorySpecific2020,
  title = {Unsupervised {{Learning}} of {{Category}}-{{Specific Symmetric 3D Keypoints}} from {{Point Sets}}},
  author = {{Fernandez-Labrador}, Clara and Chhatkuli, Ajad and Paudel, Danda Pani and Guerrero, Jose J. and Demonceaux, C{\'e}dric and Van Gool, Luc},
  year = {2020},
  month = mar,
  abstract = {Automatic discovery of category-specific 3D keypoints from a collection of objects of some category is a challenging problem. One reason is that not all objects in a category necessarily have the same semantic parts. The level of difficulty adds up further when objects are represented by 3D point clouds, with variations in shape and unknown coordinate frames. We define keypoints to be category-specific, if they meaningfully represent objects' shape and their correspondences can be simply established order-wise across all objects. This paper aims at learning category-specific 3D keypoints, in an unsupervised manner, using a collection of misaligned 3D point clouds of objects from an unknown category. In order to do so, we model shapes defined by the keypoints, within a category, using the symmetric linear basis shapes without assuming the plane of symmetry to be known. The usage of symmetry prior leads us to learn stable keypoints suitable for higher misalignments. To the best of our knowledge, this is the first work on learning such keypoints directly from 3D point clouds. Using categories from four benchmark datasets, we demonstrate the quality of our learned keypoints by quantitative and qualitative evaluations. Our experiments also show that the keypoints discovered by our method are geometrically and semantically consistent.},
  archivePrefix = {arXiv},
  eprint = {2003.07619},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point-Fernandez-Labrador et al-2020.pdf;/Users/sunjiaming/Zotero/storage/8WTYJR6X/2003.html},
  journal = {arXiv:2003.07619 [cs]},
  primaryClass = {cs}
}

@inproceedings{finmanLifelongObjectSegmentation2013,
  title = {Toward Lifelong Object Segmentation from Change Detection in Dense {{RGB}}-{{D}} Maps},
  booktitle = {2013 {{European Conference}} on {{Mobile Robots}}},
  author = {Finman, Ross and Whelan, Thomas and Kaess, Michael and Leonard, John J.},
  year = {2013},
  month = sep,
  pages = {178--185},
  publisher = {{IEEE}},
  address = {{Barcelona, Catalonia, Spain}},
  doi = {10.1109/ECMR.2013.6698839},
  abstract = {In this paper, we present a system for automatically learning segmentations of objects given changes in dense RGB-D maps over the lifetime of a robot. Using recent advances in RGBD mapping to construct multiple dense maps, we detect changes between mapped regions from multiple traverses by performing a 3-D difference of the scenes. Our method takes advantage of the free space seen in each map to account for variability in how the maps were created. The resulting changes from the 3D difference are our discovered objects, which are then used to train multiple segmentation algorithms in the original map. The final objects can then be matched in other maps given their corresponding features and learned segmentation method. If the same object is discovered multiple times in different contexts, the features and segmentation method are refined, incorporating all instances to better learn objects over time. We verify our approach with multiple objects in numerous and varying maps.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Toward lifelong object segmentation from change detection in dense RGB-D maps-Finman et al-2013.pdf},
  isbn = {978-1-4799-0263-7},
  language = {en}
}

@inproceedings{fioraioJointDetectionTracking2013,
  title = {Joint {{Detection}}, {{Tracking}} and {{Mapping}} by {{Semantic Bundle Adjustment}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Fioraio, Nicola and Di Stefano, Luigi},
  year = {2013},
  month = jun,
  pages = {1538--1545},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2013.202},
  abstract = {In this paper we propose a novel Semantic Bundle Adjustment framework whereby known rigid stationary objects are detected while tracking the camera and mapping the environment. The system builds on established tracking and mapping techniques to exploit incremental 3D reconstruction in order to validate hypotheses on the presence and pose of sought objects. Then, detected objects are explicitly taken into account for a global semantic optimization of both camera and object poses. Thus, unlike all systems proposed so far, our approach allows for solving jointly the detection and SLAM problems, so as to achieve object detection together with improved SLAM accuracy.}
}

@article{florenceDenseObjectNets2018,
  title = {Dense {{Object Nets}}: {{Learning Dense Visual Object Descriptors By}} and {{For Robotic Manipulation}}},
  shorttitle = {Dense {{Object Nets}}},
  author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
  year = {2018},
  month = jun,
  abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
  archivePrefix = {arXiv},
  eprint = {1806.08756},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dense Object Nets-Florence et al-2018.pdf;/Users/sunjiaming/Zotero/storage/GC2YUTUW/1806.html},
  journal = {arXiv:1806.08756 [cs]},
  primaryClass = {cs}
}

@inproceedings{flynnDeepViewViewSynthesis2019a,
  title = {{{DeepView}}: {{View Synthesis With Learned Gradient Descent}}},
  shorttitle = {{{DeepView}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Flynn, John and Broxton, Michael and Debevec, Paul and DuVall, Matthew and Fyffe, Graham and Overbeck, Ryan and Snavely, Noah and Tucker, Richard},
  year = {2019},
  month = jun,
  pages = {2362--2371},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00247},
  abstract = {We present a novel approach to view synthesis using multiplane images (MPIs). Building on recent advances in learned gradient descent, our algorithm generates an MPI from a set of sparse camera viewpoints. The resulting method incorporates occlusion reasoning, improving performance on challenging scene features such as object boundaries, lighting reflections, thin structures, and scenes with high depth complexity. We show that our method achieves high-quality, state-of-the-art results on two datasets: the Kalantari light field dataset, and a new camera array dataset, Spaces, which we make publicly available.},
  file = {/Users/sunjiaming/Zotero/storage/TRA685UG/Flynn et al. - 2019 - DeepView View Synthesis With Learned Gradient Des.pdf},
  isbn = {978-1-72813-293-8},
  language = {en}
}

@article{fontanInformationDrivenDirectRGBD,
  title = {Information-{{Driven Direct RGB}}-{{D Odometry}}},
  author = {Fontan, Alejandro and Civera, Javier and Triebel, Rudolph},
  pages = {9},
  abstract = {This paper presents an information-theoretic approach to point selection for direct RGB-D odometry. The aim is to select only the most informative measurements, in order to reduce the optimization problem with a minimal impact in the accuracy. It is usual practice in visual odometry/SLAM to track several hundreds of points, achieving real-time performance in high-end desktop PCs. Reducing their computational footprint will facilitate the implementation of odometry and SLAM in low-end platforms such as small robots and AR/VR glasses. Our experimental results show that our novel information-based selection criteria allows us to reduce the number of tracked points an order of magnitude (down to only 24 of them), achieving an accuracy similar to the state of the art (sometimes outperforming it) while reducing 10\texttimes{} the computational demand.},
  file = {/Users/sunjiaming/Zotero/storage/A953KUP3/Fontan et al. - Information-Driven Direct RGB-D Odometry.pdf},
  language = {en}
}

@article{fooladgarSurveyIndoorRGBD2019,
  title = {A Survey on Indoor {{RGB}}-{{D}} Semantic Segmentation: From Hand-Crafted Features to Deep Convolutional Neural Networks},
  shorttitle = {A Survey on Indoor {{RGB}}-{{D}} Semantic Segmentation},
  author = {Fooladgar, Fahimeh and Kasaei, Shohreh},
  year = {2019},
  month = may,
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-019-7684-3},
  abstract = {Semantic segmentation is one of the most important tasks in the field of computer vision. It is the main step towards scene understanding. With the advent of RGB-Depth sensors, such as Microsoft Kinect, nowadays RGB-Depth images are easily available. This has changed the landscape of some tasks such as semantic segmentation. As the depth images are independent of illumination, the combination of depth and RGB images can improve the quality of semantic labeling. The related research has been divided into two main categories, based on the usage of hand-crafted features and deep learning. Although the state-of-the-art results are mainly achieved by deep learning methods, traditional methods have also been at the center of attention for some years and lots of valuable work have been done in that category. As the field of semantic segmentation is very broad, in this survey, a comprehensive analysis has been carried out on RGB-Depth semantic segmentation methods, their challenges and contributions, available RGB-Depth datasets, metrics of evaluation, state-of-the-art results, and promising directions of the field.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A survey on indoor RGB-D semantic segmentation-Fooladgar_Kasaei-2019.pdf},
  journal = {Multimedia Tools and Applications},
  language = {en}
}

@article{forresterCOUNTERINTUITIVEBEHAVIORSOCIAL,
  title = {{{COUNTERINTUITIVE BEHAVIOR}}  {{OF SOCIAL SYSTEMS}}},
  author = {Forrester, Jay W},
  pages = {28},
  abstract = {This paper addresses several social concerns: population trends; quality of urban life; policies for urban growth; and the unexpected, ineffective, or detrimental results often generated by government programs. Society becomes frustrated as repeated attacks on deficiencies in social systems lead only to worse symptoms. Legislation is debated and passed with great hope, but many programs prove to be ineffective. Results are often far short of expectations. Because dynamic behavior of social systems is not understood, government programs often cause exactly the reverse of desired results. The field of system dynamics now can explain how such contrary results happen. Fundamental reasons cause people to misjudge behavior of social systems. Orderly processes in creating human judgment and intuition lead people to wrong decisions when faced with complex and highly interacting systems. Until we reach a much better public understanding of social systems, attempts to develop corrective programs for social troubles will continue to be disappointing. This paper cautions against continuing to depend on the same past approaches that have led to present feelings of frustration. New methods developed over the last 30 years will lead to a better understanding of social systems and thereby to more effective policies for guiding the future.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/COUNTERINTUITIVE BEHAVIOR OF SOCIAL SYSTEMS-Forrester-.pdf},
  language = {en}
}

@article{fremontScenicLanguageBasedScene2018,
  title = {Scenic: {{Language}}-{{Based Scene Generation}}},
  shorttitle = {Scenic},
  author = {Fremont, Daniel J. and Yue, Xiangyu and Dreossi, Tommaso and Ghosh, Shromona and {Sangiovanni-Vincentelli}, Alberto L. and Seshia, Sanjit A.},
  year = {2018},
  month = sep,
  abstract = {Synthetic data has proved increasingly useful in both training and testing machine learning models such as neural networks. The major problem in synthetic data generation is producing meaningful data that is not simply random but reflects properties of real-world data or covers particular cases of interest. In this paper, we show how a probabilistic programming language can be used to guide data synthesis by encoding domain knowledge about what data is useful. Specifically, we focus on data sets arising from "scenes", configurations of physical objects; for example, images of cars on a road. We design a domain-specific language, Scenic, for describing "scenarios" that are distributions over scenes. The syntax of Scenic makes it easy to specify complex relationships between the positions and orientations of objects. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. A Scenic scenario thereby implicitly defines a distribution over scenes, and we formulate the problem of sampling from this distribution as "scene improvisation". We implement an improviser for Scenic scenarios and apply it in a case study generating synthetic data sets for a convolutional neural network designed to detect cars in road images. Our experiments demonstrate the usefulness of our approach by using Scenic to analyze and improve the performance of the network in various scenarios.},
  archivePrefix = {arXiv},
  eprint = {1809.09310},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scenic-Fremont et al-2018.pdf;/Users/sunjiaming/Zotero/storage/BG4HP5FY/1809.html},
  journal = {arXiv:1809.09310 [cs]},
  primaryClass = {cs}
}

@book{friedmanEssentialsProgrammingLanguages2008,
  title = {Essentials of Programming Languages},
  author = {Friedman, Daniel P. and Wand, Mitchell},
  year = {2008},
  edition = {3rd ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Essentials of programming languages-Friedman_Wand-2008.pdf},
  isbn = {978-0-262-06279-4},
  language = {en},
  lccn = {QA76.7 .F73 2008}
}

@article{frossardEndtoendLearningMultisensor2018,
  title = {End-to-End {{Learning}} of {{Multi}}-Sensor {{3D Tracking}} by {{Detection}}},
  author = {Frossard, Davi and Urtasun, Raquel},
  year = {2018},
  month = jun,
  abstract = {In this paper we propose a novel approach to tracking by detection that can exploit both cameras as well as LIDAR data to produce very accurate 3D trajectories. Towards this goal, we formulate the problem as a linear program that can be solved exactly, and learn convolutional networks for detection as well as matching in an end-to-end manner. We evaluate our model in the challenging KITTI dataset and show very competitive results.},
  archivePrefix = {arXiv},
  eprint = {1806.11534},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/End-to-end Learning of Multi-sensor 3D Tracking by Detection-Frossard_Urtasun-2018.pdf},
  journal = {arXiv:1806.11534 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{fruendIntroductionSemidefiniteProgramming,
  title = {Introduction to {{Semidefinite Programming}}},
  author = {Fruend, Robert},
  pages = {51},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Introduction to Semidefinite Programming-Fruend-.pdf},
  language = {en}
}

@article{fu3DFUTURE3DFurniture2020,
  title = {{{3D}}-{{FUTURE}}: {{3D Furniture}} Shape with {{TextURE}}},
  shorttitle = {{{3D}}-{{FUTURE}}},
  author = {Fu, Huan and Jia, Rongfei and Gao, Lin and Gong, Mingming and Zhao, Binqiang and Maybank, Steve and Tao, Dacheng},
  year = {2020},
  month = sep,
  abstract = {The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 20,240 clean and realistic synthetic images of 5,000 different rooms. There are 9,992 unique detailed 3D instances of furniture with high-resolution textures. Experienced designers developed the room scenes, and the 3D CAD shapes in the scene are used for industrial production. Given the well-organized 3D-FUTURE, we provide baseline experiments on several widely studied tasks, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, and texture recovery for 3D shapes, to facilitate related future researches on our database.},
  archivePrefix = {arXiv},
  eprint = {2009.09633},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-FUTURE-Fu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/CJITTHR2/2009.html},
  journal = {arXiv:2009.09633 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{fuDeepOrdinalRegression2018,
  title = {Deep {{Ordinal Regression Network}} for {{Monocular Depth Estimation}}},
  author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  year = {2018},
  month = jun,
  abstract = {Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi-layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and \textbackslash dd\{faster convergence in synch\}. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The method described in this paper achieves state-of-the-art results on four challenging benchmarks, i.e., KITTI [17], ScanNet [9], Make3D [50], and NYU Depth v2 [42], and win the 1st prize in Robust Vision Challenge 2018. Code has been made available at: https://github.com/hufu6371/DORN.},
  archivePrefix = {arXiv},
  eprint = {1806.02446},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Deep Ordinal Regression Network for Monocular Depth Estimation-Fu et al-2018.pdf},
  journal = {arXiv:1806.02446 [cs]},
  keywords = {depth},
  language = {en},
  primaryClass = {cs}
}

@article{fuRealTimeLargeScaleDense2018,
  title = {Real-{{Time Large}}-{{Scale Dense Mapping}} with {{Surfels}}},
  author = {Fu, Xingyin and Zhu, Feng and Wu, Qingxiao and Sun, Yunlei and Lu, Rongrong and Yang, Ruigang},
  year = {2018},
  month = may,
  volume = {18},
  pages = {1493},
  issn = {1424-8220},
  doi = {10.3390/s18051493},
  abstract = {Real-time dense mapping systems have been developed since the birth of consumer RGB-D cameras. Currently, there are two commonly used models in dense mapping systems: truncated signed distance function (TSDF) and surfel. The state-of-the-art dense mapping systems usually work fine with small-sized regions. The generated dense surface may be unsatisfactory around the loop closures when the system tracking drift grows large. In addition, the efficiency of the system with surfel model slows down when the number of the model points in the map becomes large. In this paper, we propose to use two maps in the dense mapping system. The RGB-D images are integrated into a local surfel map. The old surfels that reconstructed in former times and far away from the camera frustum are moved from the local map to the global map. The updated surfels in the local map when every frame arrives are kept bounded. Therefore, in our system, the scene that can be reconstructed is very large, and the frame rate of our system remains high. We detect loop closures and optimize the pose graph to distribute system tracking drift. The positions and normals of the surfels in the map are also corrected using an embedded deformation graph so that they are consistent with the updated poses. In order to deal with large surface deformations, we propose a new method for constructing constraints with system trajectories and loop closure keyframes. The proposed new method stabilizes large-scale surface deformation. Experimental results show that our novel system behaves better than the prior state-of-the-art dense mapping systems.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-Time Large-Scale Dense Mapping with Surfels-Fu et al-2018.pdf},
  journal = {Sensors},
  keywords = {reconstruction},
  language = {en},
  number = {5}
}

@article{fuTextureMapping3D,
  title = {Texture {{Mapping}} for {{3D Reconstruction With RGB}}-{{D Sensor}}},
  author = {Fu, Yanping and Yan, Qingan and Yang, Long and Liao, Jie and Xiao, Chunxia},
  pages = {9},
  abstract = {Acquiring realistic texture details for 3D models is important in 3D reconstruction. However, the existence of geometric errors, caused by noisy RGB-D sensor data, always makes the color images cannot be accurately aligned onto reconstructed 3D models. In this paper, we propose a global-to-local correction strategy to obtain more desired texture mapping results. Our algorithm first adaptively selects an optimal image for each face of the 3D model, which can effectively remove blurring and ghost artifacts produced by multiple image blending. We then adopt a nonrigid global-to-local correction step to reduce the seaming effect between textures. This can effectively compensate for the texture and the geometric misalignment caused by camera pose drift and geometric errors. We evaluate the proposed algorithm in a range of complex scenes and demonstrate its effective performance in generating seamless high fidelity textures for 3D models.},
  file = {/Users/sunjiaming/Zotero/storage/3MMG57GI/Fu et al. - Texture Mapping for 3D Reconstruction With RGB-D S.pdf},
  language = {en}
}

@article{fyffeCosineLobeBased,
  title = {Cosine {{Lobe Based Relighting}} from {{Gradient Illumination Photographs}}},
  author = {Fyffe, Graham and Wilson, Cyrus A and Debevec, Paul},
  pages = {9},
  abstract = {We present an image-based method for relighting a scene by analytically fitting a cosine lobe to the reflectance function at each pixel, based on gradient illumination photographs. Realistic relighting results for many materials are obtained using a single per-pixel cosine lobe obtained from just two color photographs: one under uniform white illumination and the other under colored gradient illumination. For materials with wavelength-dependent scattering, a better fit can be obtained using independent cosine lobes for the red, green, and blue channels, obtained from three monochromatic gradient illumination conditions instead of the colored gradient condition. We explore two cosine lobe reflectance functions, both of which allow an analytic fit to the gradient conditions. One is non-zero over half the sphere of lighting directions, which works well for diffuse and specular materials, but fails for materials with broader scattering such as fur. The other is non-zero everywhere, which works well for broadly scattering materials and still produces visually plausible results for diffuse and specular materials. Additionally, we estimate scene geometry from the photometric normals to produce hard shadows cast by the geometry, while still reconstructing the input photographs exactly.},
  file = {/Users/sunjiaming/Zotero/storage/PYLTTPEW/Fyffe et al. - Cosine Lobe Based Relighting from Gradient Illumin.pdf},
  language = {en}
}

@article{gaidonVirtualWorldsProxy2016,
  title = {Virtual {{Worlds}} as {{Proxy}} for {{Multi}}-{{Object Tracking Analysis}}},
  author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
  year = {2016},
  month = may,
  abstract = {Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.},
  archivePrefix = {arXiv},
  eprint = {1605.06457},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Virtual Worlds as Proxy for Multi-Object Tracking Analysis-Gaidon et al-2016.pdf;/Users/sunjiaming/Zotero/storage/IT84M4K7/1605.html},
  journal = {arXiv:1605.06457 [cs, stat]},
  keywords = {dataset},
  primaryClass = {cs, stat}
}

@article{galvez-lopezBagsBinaryWords2012,
  title = {Bags of {{Binary Words}} for {{Fast Place Recognition}} in {{Image Sequences}}},
  author = {{Galvez-L{\'o}pez}, D. and Tardos, J. D.},
  year = {2012},
  month = oct,
  volume = {28},
  pages = {1188--1197},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2012.2197158},
  abstract = {We propose a novel method for visual place recognition using bag of words obtained from FAST+BRIEF features. For the first time, we build a vocabulary tree that discretizes a binary descriptor space, and use the tree to speed up correspondences for geometrical verification. We present competitive results with no false positives in very different datasets, using exactly the same vocabulary and settings. The whole technique, including feature extraction, requires 22ms per frame in a sequence with 26300 images, being one order of magnitude faster than previous approaches.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Bags of Binary Words for Fast Place Recognition in Image Sequences-Galvez-López_Tardos-2012.pdf},
  journal = {IEEE Transactions on Robotics},
  language = {en},
  number = {5}
}

@article{gaoDeformableKernelsAdapting2019,
  title = {Deformable {{Kernels}}: {{Adapting Effective Receptive Fields}} for {{Object Deformation}}},
  shorttitle = {Deformable {{Kernels}}},
  author = {Gao, Hang and Zhu, Xizhou and Lin, Steve and Dai, Jifeng},
  year = {2019},
  month = oct,
  abstract = {Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the "effective" receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.},
  archivePrefix = {arXiv},
  eprint = {1910.02940},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deformable Kernels-Gao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/ZR3PT2F6/1910.html},
  journal = {arXiv:1910.02940 [cs]},
  primaryClass = {cs}
}

@article{gaoLearningDeformableTetrahedral2020,
  title = {Learning {{Deformable Tetrahedral Meshes}} for {{3D Reconstruction}}},
  author = {Gao, Jun and Chen, Wenzheng and Xiang, Tommy and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
  year = {2020},
  month = nov,
  abstract = {3D shape representations that accommodate learning-based 3D reconstruction are an open problem in machine learning and computer graphics. Previous work on neural 3D reconstruction demonstrated benefits, but also limitations, of point cloud, voxel, surface mesh, and implicit function representations. We introduce Deformable Tetrahedral Meshes (DefTet) as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem. Unlike existing volumetric approaches, DefTet optimizes for both vertex placement and occupancy, and is differentiable with respect to standard 3D reconstruction loss functions. It is thus simultaneously high-precision, volumetric, and amenable to learning-based neural architectures. We show that it can represent arbitrary, complex topology, is both memory and computationally efficient, and can produce high-fidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches. The predicted surfaces are also inherently defined as tetrahedral meshes, thus do not require post-processing. We demonstrate that DefTet matches or exceeds both the quality of the previous best approaches and the performance of the fastest ones. Our approach obtains high-quality tetrahedral meshes computed directly from noisy point clouds, and is the first to showcase high-quality 3D tet-mesh results using only a single image as input.},
  archivePrefix = {arXiv},
  eprint = {2011.01437},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Deformable Tetrahedral Meshes for 3D Reconstruction-Gao et al-2020.pdf;/Users/sunjiaming/Zotero/storage/P88H2W84/2011.html},
  journal = {arXiv:2011.01437 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{gaoRes2NetNewMultiscale2019,
  title = {{{Res2Net}}: {{A New Multi}}-Scale {{Backbone Architecture}}},
  shorttitle = {{{Res2Net}}},
  author = {Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},
  year = {2019},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2019.2938758},
  abstract = {Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.},
  archivePrefix = {arXiv},
  eprint = {1904.01169},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Res2Net-Gao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/6KG4Q5UZ/1904.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@article{gaoSDMNETDeepGenerative2019,
  title = {{{SDM}}-{{NET}}: {{Deep Generative Network}} for {{Structured Deformable Mesh}}},
  shorttitle = {{{SDM}}-{{NET}}},
  author = {Gao, Lin and Yang, Jie and Wu, Tong and Yuan, Yu-Jie and Fu, Hongbo and Lai, Yu-Kun and Zhang, Hao},
  year = {2019},
  month = aug,
  abstract = {We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details. Through extensive experiments and comparisons with the state-of-the-art deep generative models of shapes, we demonstrate the superiority of SDM-NET in generating meshes with visual quality, flexible topology, and meaningful structures, which benefit shape interpolation and other subsequently modeling tasks.},
  archivePrefix = {arXiv},
  eprint = {1908.04520},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SDM-NET-Gao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/UQDTW3BI/1908.html},
  journal = {arXiv:1908.04520 [cs]},
  primaryClass = {cs}
}

@article{gaoVectorNetEncodingHD2020,
  title = {{{VectorNet}}: {{Encoding HD Maps}} and {{Agent Dynamics}} from {{Vectorized Representation}}},
  shorttitle = {{{VectorNet}}},
  author = {Gao, Jiyang and Sun, Chen and Zhao, Hang and Shen, Yi and Anguelov, Dragomir and Li, Congcong and Schmid, Cordelia},
  year = {2020},
  month = may,
  abstract = {Behavior prediction in dynamic, multi-agent systems is an important problem in the context of self-driving cars, due to the complex representations and interactions of road components, including moving agents (e.g. pedestrians and vehicles) and road context information (e.g. lanes, traffic lights). This paper introduces VectorNet, a hierarchical graph neural network that first exploits the spatial locality of individual road components represented by vectors and then models the high-order interactions among all components. In contrast to most recent approaches, which render trajectories of moving agents and road context information as bird-eye images and encode them with convolutional neural networks (ConvNets), our approach operates on a vector representation. By operating on the vectorized high definition (HD) maps and agent trajectories, we avoid lossy rendering and computationally intensive ConvNet encoding steps. To further boost VectorNet's capability in learning context features, we propose a novel auxiliary task to recover the randomly masked out map entities and agent trajectories based on their context. We evaluate VectorNet on our in-house behavior prediction benchmark and the recently released Argoverse forecasting dataset. Our method achieves on par or better performance than the competitive rendering approach on both benchmarks while saving over 70\% of the model parameters with an order of magnitude reduction in FLOPs. It also outperforms the state of the art on the Argoverse dataset.},
  archivePrefix = {arXiv},
  eprint = {2005.04259},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/VectorNet-Gao et al-2020.pdf;/Users/sunjiaming/Zotero/storage/57TIBB2S/2005.html},
  journal = {arXiv:2005.04259 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{garlandSurfaceSimplificationUsing1997,
  title = {Surface Simplification Using Quadric Error Metrics},
  booktitle = {Proceedings of the 24th Annual Conference on {{Computer}} Graphics and Interactive Techniques  - {{SIGGRAPH}} '97},
  author = {Garland, Michael and Heckbert, Paul S.},
  year = {1997},
  pages = {209--216},
  publisher = {{ACM Press}},
  address = {{Not Known}},
  doi = {10.1145/258734.258849},
  abstract = {Many applications in computer graphics require complex, highly detailed models. However, the level of detail actually necessary may vary considerably. To control processing time, it is often desirable to use approximations in place of excessively detailed models. We have developed a surface simplification algorithm which can rapidly produce high quality approximations of polygonal models. The algorithm uses iterative contractions of vertex pairs to simplify models and maintains surface error approximations using quadric matrices. By contracting arbitrary vertex pairs (not just edges), our algorithm is able to join unconnected regions of models. This can facilitate much better approximations, both visually and with respect to geometric error. In order to allow topological joining, our system also supports non-manifold surface models.},
  file = {/Users/sunjiaming/Zotero/storage/B8FUE2RF/Garland and Heckbert - 1997 - Surface simplification using quadric error metrics.pdf},
  isbn = {978-0-89791-896-1},
  language = {en}
}

@article{garnett3DLaneNetEndtoend3D2018,
  title = {{{3D}}-{{LaneNet}}: End-to-End {{3D}} Multiple Lane Detection},
  shorttitle = {{{3D}}-{{LaneNet}}},
  author = {Garnett, Noa and Cohen, Rafi and Pe'er, Tomer and Lahav, Roee and Levi, Dan},
  year = {2018},
  month = nov,
  abstract = {We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with onboard sensing instead of relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intranetwork IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our endto-end approach replacing common heuristics such as clustering and outlier rejection. In addition, our approach explicitly handles complex situations such as lane merges and splits. Promising results are shown on a new 3D lane synthetic dataset. For comparison with existing methods, we verify our approach on the image-only tuSimple lane detection benchmark and reach competitive performance.},
  archivePrefix = {arXiv},
  eprint = {1811.10203},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D-LaneNet-Garnett et al-2018.pdf},
  journal = {arXiv:1811.10203 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{garonFrameworkEvaluating6DOF2018,
  title = {A {{Framework}} for {{Evaluating}} 6-{{DOF Object Trackers}}},
  author = {Garon, Mathieu and Laurendeau, Denis and Lalonde, Jean-Fran{\c c}ois},
  year = {2018},
  month = mar,
  abstract = {We present a challenging and realistic novel dataset for evaluating 6-DOF object tracking algorithms. Existing datasets show serious limitations---notably, unrealistic synthetic data, or real data with large fiducial markers---preventing the community from obtaining an accurate picture of the state-of-the-art. Using a data acquisition pipeline based on a commercial motion capture system for acquiring accurate ground truth poses of real objects with respect to a Kinect V2 camera, we build a dataset which contains a total of 297 calibrated sequences. They are acquired in three different scenarios to evaluate the performance of trackers: stability, robustness to occlusion and accuracy during challenging interactions between a person and the object. We conduct an extensive study of a deep 6-DOF tracking architecture and determine a set of optimal parameters. We enhance the architecture and the training methodology to train a 6-DOF tracker that can robustly generalize to objects never seen during training, and demonstrate favorable performance compared to previous approaches trained specifically on the objects to track.},
  archivePrefix = {arXiv},
  eprint = {1803.10075},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Framework for Evaluating 6-DOF Object Trackers-Garon et al-2018.pdf;/Users/sunjiaming/Zotero/storage/5YY6PB6R/1803.html},
  journal = {arXiv:1803.10075 [cs]},
  primaryClass = {cs}
}

@incollection{garonFrameworkEvaluating6DOF2018a,
  title = {A {{Framework}} for {{Evaluating}} 6-{{DOF Object Trackers}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Garon, Mathieu and Laurendeau, Denis and Lalonde, Jean-Fran{\c c}ois},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11215},
  pages = {608--623},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01252-6_36},
  abstract = {We present a challenging and realistic novel dataset for evaluating 6-DOF object tracking algorithms. Existing datasets show serious limitations\textemdash notably, unrealistic synthetic data, or real data with large fiducial markers\textemdash preventing the community from obtaining an accurate picture of the state-of-the-art. Using a data acquisition pipeline based on a commercial motion capture system for acquiring accurate ground truth poses of real objects with respect to a Kinect V2 camera, we build a dataset which contains a total of 297 calibrated sequences. They are acquired in three different scenarios to evaluate the performance of trackers: stability, robustness to occlusion and accuracy during challenging interactions between a person and the object. We conduct an extensive study of a deep 6-DOF tracking architecture and determine a set of optimal parameters. We enhance the architecture and the training methodology to train a 6-DOF tracker that can robustly generalize to objects never seen during training, and demonstrate favorable performance compared to previous approaches trained specifically on the objects to track.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Framework for Evaluating 6-DOF Object Trackers-Garon et al-22.pdf},
  isbn = {978-3-030-01251-9 978-3-030-01252-6},
  language = {en}
}

@inproceedings{gaurWeaklySupervisedManifold2017,
  title = {Weakly {{Supervised Manifold Learning}} for {{Dense Semantic Object Correspondence}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Gaur, Utkarsh and Manjunath, B. S.},
  year = {2017},
  month = oct,
  pages = {1744--1752},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.192},
  abstract = {The goal of the semantic object correspondence problem is to compute dense association maps for a pair of images such that the same object parts get matched for very different appearing object instances. Our method builds on the recent findings that deep convolutional neural networks (DCNNs) implicitly learn a latent model of object parts even when trained for classification. We also leverage a key correspondence problem insight that the geometric structure between object parts is consistent across multiple object instances. These two concepts are then combined in the form of a novel optimization scheme. This optimization learns a feature embedding by rewarding for projecting features closer on the manifold if they have low feature-space distance. Simultaneously, the optimization penalizes feature clusters whose geometric structure is inconsistent with the observed geometric structure of object parts. In this manner, by accounting for feature space similarities and feature neighborhood context together, a manifold is learned where features belonging to semantically similar object parts cluster together. We also describe transferring these embedded features to the sister tasks of semantic keypoint classification and localization task via a Siamese DCNN. We provide qualitative results on the Pascal VOC 2012 images and quantitative results on the Pascal Berkeley dataset where we improve on the state of the art by over 5\% on classification and over 9\% on localization tasks.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Weakly Supervised Manifold Learning for Dense Semantic Object Correspondence-Gaur_Manjunath-2017.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@inproceedings{geigerAreWeReady2012,
  title = {Are We Ready for Autonomous Driving? {{The KITTI}} Vision Benchmark Suite},
  shorttitle = {Are We Ready for Autonomous Driving?},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Geiger, A. and Lenz, P. and Urtasun, R.},
  year = {2012},
  month = jun,
  pages = {3354--3361},
  publisher = {{IEEE}},
  address = {{Providence, RI}},
  doi = {10.1109/CVPR.2012.6248074},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Are we ready for autonomous driving-Geiger et al-2012.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Are we ready for autonomous driving-Geiger et al-22.pdf},
  isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
  keywords = {dataset},
  language = {en}
}

@article{geigerVisionMeetsRobotics2013,
  title = {Vision Meets Robotics: {{The KITTI}} Dataset},
  shorttitle = {Vision Meets Robotics},
  author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
  year = {2013},
  month = sep,
  volume = {32},
  pages = {1231--1237},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364913491297},
  abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations and range from freeways over rural areas to innercity scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Vision meets robotics-Geiger et al-2013.pdf;/Users/sunjiaming/Zotero/storage/2AHSRYUG/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf},
  journal = {The International Journal of Robotics Research},
  keywords = {dataset},
  language = {en},
  number = {11}
}

@article{genovaDeepStructuredImplicit2019,
  title = {Deep {{Structured Implicit Functions}}},
  author = {Genova, Kyle and Cole, Forrester and Sud, Avneesh and Sarna, Aaron and Funkhouser, Thomas},
  year = {2019},
  month = dec,
  abstract = {The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Deep Structured Implicit Functions (DSIF), a 3D shape representation that decomposes space into a structured set of local deep implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections. Please see our video at https://youtu.be/HCBtG0-EZ2s},
  archivePrefix = {arXiv},
  eprint = {1912.06126},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Structured Implicit Functions-Genova et al-2019.pdf;/Users/sunjiaming/Zotero/storage/S8ZFIKK6/1912.html},
  journal = {arXiv:1912.06126 [cs]},
  primaryClass = {cs}
}

@article{genovaLearningShapeTemplates2019,
  ids = {genovaLearningShapeTemplates2019a,genovaLearningShapeTemplates2019b},
  title = {Learning {{Shape Templates}} with {{Structured Implicit Functions}}},
  author = {Genova, Kyle and Cole, Forrester and Vlasic, Daniel and Sarna, Aaron and Freeman, William T. and Funkhouser, Thomas},
  year = {2019},
  month = apr,
  abstract = {Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image.},
  archivePrefix = {arXiv},
  eprint = {1904.06447},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Shape Templates with Structured Implicit Functions-Genova et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Shape Templates with Structured Implicit Functions-Genova et al-22.pdf;/Users/sunjiaming/Zotero/storage/8GBBD7FV/Genova et al. - 2019 - Learning Shape Templates with Structured Implicit .pdf;/Users/sunjiaming/Zotero/storage/BD6D4X9M/1904.html;/Users/sunjiaming/Zotero/storage/JPKUM6IL/1904.html;/Users/sunjiaming/Zotero/storage/UEPJZKRD/1904.html},
  journal = {arXiv:1904.06447 [cs]},
  primaryClass = {cs}
}

@article{geOptimizedProductQuantization2014,
  title = {Optimized {{Product Quantization}}},
  author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  year = {2014},
  month = apr,
  volume = {36},
  pages = {744--755},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.240},
  abstract = {Product quantization (PQ) is an effective vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition is important for the PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing quantization distortions w.r.t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Optimized Product Quantization-Ge et al-2014.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {4}
}

@article{germainS2DNetLearningAccurate2020,
  title = {{{S2DNet}}: {{Learning Accurate Correspondences}} for {{Sparse}}-to-{{Dense Feature Matching}}},
  shorttitle = {{{S2DNet}}},
  author = {Germain, Hugo and Bourmaud, Guillaume and Lepetit, Vincent},
  year = {2020},
  month = apr,
  abstract = {Establishing robust and accurate correspondences is a fundamental backbone to many computer vision algorithms. While recent learning-based feature matching methods have shown promising results in providing robust correspondences under challenging conditions, they are often limited in terms of precision. In this paper, we introduce S2DNet, a novel feature matching pipeline, designed and trained to efficiently establish both robust and accurate correspondences. By leveraging a sparse-to-dense matching paradigm, we cast the correspondence learning problem as a supervised classification task to learn to output highly peaked correspondence maps. We show that S2DNet achieves state-of-the-art results on the HPatches benchmark, as well as on several long-term visual localization datasets.},
  archivePrefix = {arXiv},
  eprint = {2004.01673},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/S2DNet-Germain et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NZDEEAE9/2004.html},
  journal = {arXiv:2004.01673 [cs]},
  primaryClass = {cs}
}

@article{germainS2DNetLearningAccurate2020a,
  title = {{{S2DNet}}: {{Learning Accurate Correspondences}} for {{Sparse}}-to-{{Dense Feature Matching}}},
  shorttitle = {{{S2DNet}}},
  author = {Germain, Hugo and Bourmaud, Guillaume and Lepetit, Vincent},
  year = {2020},
  month = apr,
  abstract = {Establishing robust and accurate correspondences is a fundamental backbone to many computer vision algorithms. While recent learning-based feature matching methods have shown promising results in providing robust correspondences under challenging conditions, they are often limited in terms of precision. In this paper, we introduce S2DNet, a novel feature matching pipeline, designed and trained to efficiently establish both robust and accurate correspondences. By leveraging a sparse-to-dense matching paradigm, we cast the correspondence learning problem as a supervised classification task to learn to output highly peaked correspondence maps. We show that S2DNet achieves state-of-the-art results on the HPatches benchmark, as well as on several long-term visual localization datasets.},
  archivePrefix = {arXiv},
  eprint = {2004.01673},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/S2DNet-Germain et al-22.pdf;/Users/sunjiaming/Zotero/storage/FCU6TD8S/2004.html},
  journal = {arXiv:2004.01673 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{geSelfsupervisingFinegrainedRegion2020,
  title = {Self-Supervising {{Fine}}-Grained {{Region Similarities}} for {{Large}}-Scale {{Image Localization}}},
  author = {Ge, Yixiao and Wang, Haibo and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
  year = {2020},
  month = jul,
  abstract = {The task of large-scale retrieval-based image localization is to estimate the geographical location of a query image by recognizing its nearest reference images from a city-scale dataset. However, the general public benchmarks only provide noisy GPS labels associated with the training images, which act as weak supervisions for learning image-to-image similarities. Such label noise prevents deep neural networks from learning discriminative features for accurate localization. To tackle this challenge, we propose to self-supervise image-to-region similarities in order to fully explore the potential of difficult positive images alongside their sub-regions. The estimated image-to-region similarities can serve as extra training supervision for improving the network in generations, which could in turn gradually refine the fine-grained similarities to achieve optimal performance. Our proposed self-enhanced image-to-region similarity labels effectively deal with the training bottleneck in the state-of-the-art pipelines without any additional parameters or manual annotations in both training and inference. Our method outperforms state-of-the-arts on the standard localization benchmarks by noticeable margins and shows excellent generalization capability on multiple image retrieval datasets.},
  archivePrefix = {arXiv},
  eprint = {2006.03926},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-supervising Fine-grained Region Similarities for Large-scale Image-Ge et al-2020.pdf;/Users/sunjiaming/Zotero/storage/K2VMUNUY/2006.html},
  journal = {arXiv:2006.03926 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{geyerA2D2AudiAutonomous2020,
  title = {{{A2D2}}: {{Audi Autonomous Driving Dataset}}},
  shorttitle = {{{A2D2}}},
  author = {Geyer, Jakob and Kassahun, Yohannes and Mahmudi, Mentar and Ricou, Xavier and Durgesh, Rupesh and Chung, Andrew S. and Hauswald, Lorenz and Pham, Viet Hoang and M{\"u}hlegg, Maximilian and Dorn, Sebastian and Fernandez, Tiffany and J{\"a}nicke, Martin and Mirashi, Sudesh and Savani, Chiragkumar and Sturm, Martin and Vorobiov, Oleksandr and Oelker, Martin and Garreis, Sebastian and Schuberth, Peter},
  year = {2020},
  month = apr,
  abstract = {Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.},
  archivePrefix = {arXiv},
  eprint = {2004.06320},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A2D2-Geyer et al-2020.pdf;/Users/sunjiaming/Zotero/storage/4IHNMZNK/2004.html},
  journal = {arXiv:2004.06320 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{ghaouiImplicitDeepLearning2019,
  title = {Implicit {{Deep Learning}}},
  author = {Ghaoui, Laurent El and Gu, Fangda and Travacca, Bertrand and Askari, Armin},
  year = {2019},
  month = aug,
  abstract = {We define a new class of "implicit" deep learning prediction rules that generalize the recursive rules of feedforward neural networks. These models are based on the solution of a fixed-point equation involving a single a vector of hidden features, which is thus only implicitly defined. The new framework greatly simplifies the notation of deep learning, and opens up new possibilities, in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.},
  archivePrefix = {arXiv},
  eprint = {1908.06315},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Implicit Deep Learning-Ghaoui et al-2019.pdf;/Users/sunjiaming/Zotero/storage/JL86XWEU/1908.html},
  journal = {arXiv:1908.06315 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{ghoshDeepDepthPrior2020,
  title = {Deep {{Depth Prior}} for {{Multi}}-{{View Stereo}}},
  author = {Ghosh, Pallabi and Vineet, Vibhav and Davis, Larry S. and Shrivastava, Abhinav and Sinha, Sudipta and Joshi, Neel},
  year = {2020},
  month = jan,
  abstract = {It was recently shown that the structure of convolutional neural networks induces a strong prior favoring natural color images, a phenomena referred to as a deep image prior (DIP), which can be an effective regularizer in inverse problems such as image denoising, inpainting etc. In this paper, we investigate a similar idea for depth images, which we call a deep depth prior. Specifically, given a color image and a noisy and incomplete target depth map from the same viewpoint, we optimize a randomly initialized CNN model to reconstruct an RGB-D image where the depth channel gets restored by virtue of using the network structure as a prior. We propose using deep depth priors for refining and inpainting noisy depth maps within a multi-view stereo pipeline. We optimize the network parameters to minimize two losses 1) a RGB-D reconstruction loss based on the noisy depth map and 2) a multi-view photoconsistency-based loss, which is computed using images from a geometrically calibrated camera from nearby viewpoints. Our quantitative and qualitative evaluation shows that our refined depth maps are more accurate and complete, and after fusion, produces dense 3D models of higher quality.},
  archivePrefix = {arXiv},
  eprint = {2001.07791},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Depth Prior for Multi-View Stereo-Ghosh et al-2020.pdf;/Users/sunjiaming/Zotero/storage/PZD655IH/2001.html},
  journal = {arXiv:2001.07791 [cs]},
  primaryClass = {cs}
}

@article{giancolaLeveragingShapeCompletion2019,
  title = {Leveraging {{Shape Completion}} for {{3D Siamese Tracking}}},
  author = {Giancola, Silvio and Zarzar, Jesus and Ghanem, Bernard},
  year = {2019},
  month = mar,
  abstract = {Point clouds are challenging to process due to their sparsity, therefore autonomous vehicles rely more on appearance attributes than pure geometric features. However, 3D LIDAR perception can provide crucial information for urban navigation in challenging light or weather conditions. In this paper, we investigate the versatility of Shape Completion for 3D Object Tracking in LIDAR point clouds. We design a Siamese tracker that encodes model and candidate shapes into a compact latent representation. We regularize the encoding by enforcing the latent representation to decode into an object model shape. We observe that 3D object tracking and 3D shape completion complement each other. Learning a more meaningful latent representation shows better discriminatory capabilities, leading to improved tracking performance. We test our method on the KITTI Tracking set using car 3D bounding boxes. Our model reaches a 76.94\% Success rate and 81.38\% Precision for 3D Object Tracking, with the shape completion regularization leading to an improvement of 3\% in both metrics.},
  archivePrefix = {arXiv},
  eprint = {1903.01784},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Leveraging Shape Completion for 3D Siamese Tracking-Giancola et al-2019.pdf;/Users/sunjiaming/Zotero/storage/DI6LJXJ5/1903.html},
  journal = {arXiv:1903.01784 [cs]},
  keywords = {3d tracking},
  primaryClass = {cs}
}

@article{gkioxariMeshRCNN2019,
  title = {Mesh {{R}}-{{CNN}}},
  author = {Gkioxari, Georgia and Malik, Jitendra and Johnson, Justin},
  year = {2019},
  month = jun,
  abstract = {Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.},
  archivePrefix = {arXiv},
  eprint = {1906.02739},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Mesh R-CNN-Gkioxari et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4BYLQYJJ/1906.html},
  journal = {arXiv:1906.02739 [cs]},
  primaryClass = {cs}
}

@article{godardDiggingSelfSupervisedMonocular2018,
  title = {Digging {{Into Self}}-{{Supervised Monocular Depth Estimation}}},
  author = {Godard, Cl{\'e}ment and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel},
  year = {2018},
  month = jun,
  abstract = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.},
  archivePrefix = {arXiv},
  eprint = {1806.01260},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Digging Into Self-Supervised Monocular Depth Estimation-Godard et al-2018.pdf;/Users/sunjiaming/Zotero/storage/FCAXNI7L/1806.html},
  journal = {arXiv:1806.01260 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{godardUnsupervisedMonocularDepth2016,
  title = {Unsupervised {{Monocular Depth Estimation}} with {{Left}}-{{Right Consistency}}},
  author = {Godard, Cl{\'e}ment and Mac Aodha, Oisin and Brostow, Gabriel J.},
  year = {2016},
  month = sep,
  abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.},
  archivePrefix = {arXiv},
  eprint = {1609.03677},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Monocular Depth Estimation with Left-Right Consistency-Godard et al-2016.pdf;/Users/sunjiaming/Zotero/storage/K8IA8V5H/1609.html},
  journal = {arXiv:1609.03677 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{goelShapeViewpointKeypoints2020,
  title = {Shape and {{Viewpoint}} without {{Keypoints}}},
  author = {Goel, Shubham and Kanazawa, Angjoo and Malik, Jitendra},
  year = {2020},
  month = jul,
  abstract = {We present a learning framework that learns to recover the 3D shape, pose and texture from a single image, trained on an image collection without any ground truth 3D shape, multi-view, camera viewpoints or keypoint supervision. We approach this highly under-constrained problem in a "analysis by synthesis" framework where the goal is to predict the likely shape, texture and camera viewpoint that could produce the image with various learned category-specific priors. Our particular contribution in this paper is a representation of the distribution over cameras, which we call "camera-multiplex". Instead of picking a point estimate, we maintain a set of camera hypotheses that are optimized during training to best explain the image given the current shape and texture. We call our approach Unsupervised Category-Specific Mesh Reconstruction (U-CMR), and present qualitative and quantitative results on CUB, Pascal 3D and new web-scraped datasets. We obtain state-of-the-art camera prediction results and show that we can learn to predict diverse shapes and textures across objects using an image collection without any keypoint annotations or 3D ground truth. Project page: https://shubham-goel.github.io/ucmr},
  archivePrefix = {arXiv},
  eprint = {2007.10982},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Shape and Viewpoint without Keypoints-Goel et al-2020.pdf;/Users/sunjiaming/Zotero/storage/JN3LDSXW/2007.html},
  journal = {arXiv:2007.10982 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{gojcicLearningMultiview3D2020,
  title = {Learning Multiview {{3D}} Point Cloud Registration},
  author = {Gojcic, Zan and Zhou, Caifa and Wegner, Jan D. and Guibas, Leonidas J. and Birdal, Tolga},
  year = {2020},
  month = jan,
  abstract = {We present a novel, end-to-end learnable, multiview 3D point cloud registration algorithm. Registration of multiple scans typically follows a two-stage pipeline: the initial pairwise alignment and the globally consistent refinement. The former is often ambiguous due to the low overlap of neighboring point clouds, symmetries and repetitive scene parts. Therefore, the latter global refinement aims at establishing the cyclic consistency across multiple scans and helps in resolving the ambiguous cases. In this paper we propose, to the best of our knowledge, the first end-to-end algorithm for joint learning of both parts of this two-stage problem. Experimental evaluation on well accepted benchmark datasets shows that our approach outperforms the state-of-the-art by a significant margin, while being end-to-end trainable and computationally less costly. Moreover, we present detailed analysis and an ablation study that validate the novel components of our approach. The source code and pretrained models will be made publicly available under https: //github.com/zgojcic/3D\_multiview\_reg.},
  archivePrefix = {arXiv},
  eprint = {2001.05119},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning multiview 3D point cloud registration-Gojcic et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GLLXEYYK/2001.html},
  journal = {arXiv:2001.05119 [cs]},
  primaryClass = {cs}
}

@article{gojcicPerfectMatch3D2018,
  title = {The {{Perfect Match}}: {{3D Point Cloud Matching}} with {{Smoothed Densities}}},
  shorttitle = {The {{Perfect Match}}},
  author = {Gojcic, Zan and Zhou, Caifa and Wegner, Jan D. and Wieser, Andreas},
  year = {2018},
  month = nov,
  abstract = {We propose 3DSmoothNet, a full workflow to match 3D point clouds with a siamese deep learning architecture and fully convolutional layers using a voxelized smoothed density value (SDV) representation. The latter is computed per interest point and aligned to the local reference frame (LRF) to achieve rotation invariance. Our compact, learned, rotation invariant 3D point cloud descriptor achieves 94.9\% average recall on the 3DMatch benchmark data set, outperforming the state-of-the-art by more than 20 percent points with only 32 output dimensions. This very low output dimension allows for near real-time correspondence search with 0.1 ms per feature point on a standard PC. Our approach is sensor- and scene-agnostic because of SDV, LRF and learning highly descriptive features with fully convolutional layers. We show that 3DSmoothNet trained only on RGB-D indoor scenes of buildings achieves 79.0\% average recall on laser scans of outdoor vegetation, more than double the performance of our closest, learningbased competitors.},
  archivePrefix = {arXiv},
  eprint = {1811.06879},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Perfect Match-Gojcic et al-2018.pdf;/Users/sunjiaming/Zotero/storage/36HI2F3J/1811.html},
  journal = {arXiv:1811.06879 [cs]},
  primaryClass = {cs}
}

@article{gongSpiralNetFastHighly2019,
  title = {{{SpiralNet}}++: {{A Fast}} and {{Highly Efficient Mesh Convolution Operator}}},
  shorttitle = {{{SpiralNet}}++},
  author = {Gong, Shunwang and Chen, Lei and Bronstein, Michael and Zafeiriou, Stefanos},
  year = {2019},
  month = nov,
  abstract = {Intrinsic graph convolution operators with differentiable kernel functions play a crucial role in analyzing 3D shape meshes. In this paper, we present a fast and efficient intrinsic mesh convolution operator that does not rely on the intricate design of kernel function. We explicitly formulate the order of aggregating neighboring vertices, instead of learning weights between nodes, and then a fully connected layer follows to fuse local geometric structure information with vertex features. We provide extensive evidence showing that models based on this convolution operator are easier to train, and can efficiently learn invariant shape features. Specifically, we evaluate our method on three different types of tasks of dense shape correspondence, 3D facial expression classification, and 3D shape reconstruction, and show that it significantly outperforms state-of-the-art approaches while being significantly faster, without relying on shape descriptors. Our source code is available on GitHub.},
  archivePrefix = {arXiv},
  eprint = {1911.05856},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SpiralNet++-Gong et al-2019.pdf;/Users/sunjiaming/Zotero/storage/CH96MHID/1911.html},
  journal = {arXiv:1911.05856 [cs]},
  primaryClass = {cs}
}

@book{goodfellowDeepLearing2016,
  title = {Deep {{Learing}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Learing-Goodfellow et al-2016.pdf}
}

@article{gordonDepthVideosWild2019,
  title = {Depth from {{Videos}} in the {{Wild}}: {{Unsupervised Monocular Depth Learning}} from {{Unknown Cameras}}},
  shorttitle = {Depth from {{Videos}} in the {{Wild}}},
  author = {Gordon, Ariel and Li, Hanhan and Jonschkowski, Rico and Angelova, Anelia},
  year = {2019},
  month = apr,
  abstract = {We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos.},
  archivePrefix = {arXiv},
  eprint = {1904.04998},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Depth from Videos in the Wild-Gordon et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WYIHLDYN/1904.html},
  journal = {arXiv:1904.04998 [cs]},
  primaryClass = {cs}
}

@article{grabnerGeometricCorrespondenceFields2020,
  title = {Geometric {{Correspondence Fields}}: {{Learned Differentiable Rendering}} for {{3D Pose Refinement}} in the {{Wild}}},
  shorttitle = {Geometric {{Correspondence Fields}}},
  author = {Grabner, Alexander and Wang, Yaming and Zhang, Peizhao and Guo, Peihong and Xiao, Tong and Vajda, Peter and Roth, Peter M. and Lepetit, Vincent},
  year = {2020},
  month = jul,
  abstract = {We present a novel 3D pose refinement approach based on differentiable rendering for objects of arbitrary categories in the wild. In contrast to previous methods, we make two main contributions: First, instead of comparing real-world images and synthetic renderings in the RGB or mask space, we compare them in a feature space optimized for 3D pose refinement. Second, we introduce a novel differentiable renderer that learns to approximate the rasterization backward pass from data instead of relying on a hand-crafted algorithm. For this purpose, we predict deep cross-domain correspondences between RGB images and 3D model renderings in the form of what we call geometric correspondence fields. These correspondence fields serve as pixel-level gradients which are analytically propagated backward through the rendering pipeline to perform a gradient-based optimization directly on the 3D pose. In this way, we precisely align 3D models to objects in RGB images which results in significantly improved 3D pose estimates. We evaluate our approach on the challenging Pix3D dataset and achieve up to 55\% relative improvement compared to state-of-the-art refinement methods in multiple metrics.},
  archivePrefix = {arXiv},
  eprint = {2007.08939},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Geometric Correspondence Fields-Grabner et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GAEICH69/2007.html},
  journal = {arXiv:2007.08939 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{gramkowAveragingRotations,
  title = {On {{Averaging Rotations}}},
  author = {Gramkow, Claus},
  pages = {10},
  abstract = {In this paper two common approaches to averaging rotations are compared to a more advanced approach based on a Riemannian metric. Very often the barycenter of the quaternions or matrices that represent the rotations are used as an estimate of the mean. These methods neglect that rotations belong to a non-linear manifold and renormalization or orthogonalization must be applied to obtain proper rotations. These latter steps have been viewed as ad hoc corrections for the errors introduced by assuming a vector space. The article shows that the two approximative methods can be derived from natural approximations to the Riemannian metric, and that the subsequent corrections are inherent in the least squares estimation.},
  file = {/Users/sunjiaming/Zotero/storage/PGI9PAUU/Gramkow - On Averaging Rotations.pdf},
  language = {en}
}

@article{griffinLearningObjectDepth2020,
  title = {Learning {{Object Depth}} from {{Camera Motion}} and {{Video Object Segmentation}}},
  author = {Griffin, Brent A. and Corso, Jason J.},
  year = {2020},
  month = jul,
  abstract = {Video object segmentation, i.e., the separation of a target object from background in video, has made significant progress on real and challenging videos in recent years. To leverage this progress in 3D applications, this paper addresses the problem of learning to estimate the depth of segmented objects given some measurement of camera motion (e.g., from robot kinematics or vehicle odometry). We achieve this by, first, introducing a diverse, extensible dataset and, second, designing a novel deep network that estimates the depth of objects using only segmentation masks and uncalibrated camera movement. Our data-generation framework creates artificial object segmentations that are scaled for changes in distance between the camera and object, and our network learns to estimate object depth even with segmentation errors. We demonstrate our approach across domains using a robot camera to locate objects from the YCB dataset and a vehicle camera to locate obstacles while driving.},
  archivePrefix = {arXiv},
  eprint = {2007.05676},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Object Depth from Camera Motion and Video Object Segmentation-Griffin_Corso-2020.pdf;/Users/sunjiaming/Zotero/storage/X76ZNIPQ/2007.html},
  journal = {arXiv:2007.05676 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{griffithsFindingYour3D2020,
  title = {Finding {{Your}} ({{3D}}) {{Center}}: {{3D Object Detection Using}} a {{Learned Loss}}},
  shorttitle = {Finding {{Your}} ({{3D}}) {{Center}}},
  author = {Griffiths, David and Boehm, Jan and Ritschel, Tobias},
  year = {2020},
  month = apr,
  abstract = {Massive semantic labeling is readily available for 2D images, but much harder to achieve for 3D scenes. Objects in 3D repositories like ShapeNet are labeled, but regrettably only in isolation, so without context. 3D scenes can be acquired by range scanners on city-level scale, but much fewer with semantic labels. Addressing this disparity, we introduce a new optimization procedure, which allows training for 3D detection with raw 3D scans while using as little as 5\% of the object labels and still achieve comparable performance. Our optimization uses two networks. A scene network maps an entire 3D scene to a set of 3D object centers. As we assume the scene not to be labeled by centers, no classic loss, such as chamfer can be used to train it. Instead, we use another network to emulate the loss. This loss network is trained on a small labeled subset and maps a non-centered 3D object in the presence of distractions to its own center. This function is very similar - and hence can be used instead of - the gradient the supervised loss would have. Our evaluation documents competitive fidelity at a much lower level of supervision, respectively higher quality at comparable supervision. Supplementary material can be found at: https://dgriffiths3.github.io.},
  archivePrefix = {arXiv},
  eprint = {2004.02693},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Finding Your (3D) Center-Griffiths et al-2020.pdf;/Users/sunjiaming/Zotero/storage/YV7Q47HY/2004.html},
  journal = {arXiv:2004.02693 [cs]},
  primaryClass = {cs}
}

@article{grillBootstrapYourOwn2020,
  title = {Bootstrap {{Your Own Latent}}: {{A New Approach}} to {{Self}}-{{Supervised Learning}}},
  shorttitle = {Bootstrap {{Your Own Latent}}},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = jun,
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Bootstrap Your Own Latent-Grill et al-2020.pdf;/Users/sunjiaming/Zotero/storage/WUMFUGL8/2006.html},
  journal = {arXiv:2006.07733 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{grinvaldVolumetricInstanceAwareSemantic2019,
  title = {Volumetric {{Instance}}-{{Aware Semantic Mapping}} and {{3D Object Discovery}}},
  author = {Grinvald, Margarita and Furrer, Fadri and Novkovic, Tonci and Chung, Jen Jen and Cadena, Cesar and Siegwart, Roland and Nieto, Juan},
  year = {2019},
  month = mar,
  abstract = {To autonomously navigate and plan interactions in real-world environments, robots require the ability to robustly perceive and map complex, unstructured surrounding scenes. Besides building an internal representation of the observed scene geometry, the key insight towards a truly functional understanding of the environment is the usage of higher-level entities during mapping, such as individual object instances. We propose an approach to incrementally build volumetric object-centric maps during online scanning with a localized RGB-D camera. First, a per-frame segmentation scheme combines an unsupervised geometric approach with instance-aware semantic object predictions. This allows us to detect and segment elements both from the set of known classes and from other, previously unseen categories. Next, a data association step tracks the predicted instances across the different frames. Finally, a map integration strategy fuses information about their 3D shape, location, and, if available, semantic class into a global volume. Evaluation on a publicly available dataset shows that the proposed approach for building instance-level semantic maps is competitive with state-of-the-art methods, while additionally able to discover objects of unseen categories. The system is further evaluated within a real-world robotic mapping setup, for which qualitative results highlight the online nature of the method.},
  archivePrefix = {arXiv},
  eprint = {1903.00268},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery-Grinvald et al-2019.pdf;/Users/sunjiaming/Zotero/storage/JMQYFC8P/1903.html},
  journal = {arXiv:1903.00268 [cs]},
  primaryClass = {cs}
}

@article{grisettiG2oGeneralFramework,
  title = {G2o: {{A}} General {{Framework}} for ({{Hyper}}) {{Graph Optimization}}},
  author = {Grisetti, Giorgio and Kummerle, Rainer and Strasdat, Hauke and Konolige, Kurt},
  pages = {19},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/g2o-Grisetti et al-.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/g2o-Grisetti et al-2.pdf},
  language = {en}
}

@article{grisettiTutorialGraphBasedSLAM24,
  title = {A {{Tutorial}} on {{Graph}}-{{Based SLAM}}},
  author = {Grisetti, G and Kummerle, R and Stachniss, C and Burgard, W},
  year = {24},
  volume = {2},
  pages = {31--43},
  issn = {1939-1390},
  doi = {10.1109/MITS.2010.939925},
  abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based SLAM problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the SLAM problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A Tutorial on Graph-Based SLAM-Grisetti et al-2010.pdf},
  journal = {IEEE Intelligent Transportation Systems Magazine},
  language = {en},
  number = {4}
}

@article{groppImplicitGeometricRegularization2020,
  title = {Implicit {{Geometric Regularization}} for {{Learning Shapes}}},
  author = {Gropp, Amos and Yariv, Lior and Haim, Niv and Atzmon, Matan and Lipman, Yaron},
  year = {2020},
  month = feb,
  abstract = {Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.},
  archivePrefix = {arXiv},
  eprint = {2002.10099},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Implicit Geometric Regularization for Learning Shapes-Gropp et al-2020.pdf;/Users/sunjiaming/Zotero/storage/JP3HTNSC/2002.html},
  journal = {arXiv:2002.10099 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{guCascadeCostVolume2019,
  title = {Cascade {{Cost Volume}} for {{High}}-{{Resolution Multi}}-{{View Stereo}} and {{Stereo Matching}}},
  author = {Gu, Xiaodong and Fan, Zhiwen and Zhu, Siyu and Dai, Zuozhuo and Tan, Feitong and Tan, Ping},
  year = {2019},
  month = dec,
  abstract = {The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the output depth or disparity. These methods are limited when high-resolution outputs are needed since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a both memory and time efficient cost volume formulation that is complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a standard feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the depth (or disparity) map from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner. We apply the cascade cost volume to the representative MVS-Net, and obtain a 23.1\% improvement on DTU benchmark (1st place), with 50.6\% and 74.2\% reduction in GPU memory and run-time. It is also the state-of-the-art learning-based method on Tanks and Temples benchmark. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method.},
  archivePrefix = {arXiv},
  eprint = {1912.06378},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching-Gu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/W4HK9SAU/1912.html},
  journal = {arXiv:1912.06378 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{guHPLFlowNetHierarchicalPermutohedral2019,
  title = {{{HPLFlowNet}}: {{Hierarchical Permutohedral Lattice FlowNet}} for {{Scene Flow Estimation}} on {{Large}}-Scale {{Point Clouds}}},
  shorttitle = {{{HPLFlowNet}}},
  author = {Gu, Xiuye and Wang, Yijie and Wu, Chongruo and Lee, Yong-Jae and Wang, Panqu},
  year = {2019},
  month = jun,
  abstract = {We present a novel deep neural network architecture for end-to-end scene flow estimation that directly operates on large-scale 3D point clouds. Inspired by Bilateral Convolutional Layers (BCL), we propose novel DownBCL, UpBCL, and CorrBCL operations that restore structural information from unstructured point clouds, and fuse information from two consecutive point clouds. Operating on discrete and sparse permutohedral lattice points, our architectural design is parsimonious in computational cost. Our model can efficiently process a pair of point cloud frames at once with a maximum of 86K points per frame. Our approach achieves state-of-the-art performance on the FlyingThings3D and KITTI Scene Flow 2015 datasets. Moreover, trained on synthetic data, our approach shows great generalization ability on real-world data and on different point densities without fine-tuning.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/HPLFlowNet-Gu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/ZJHAWN8H/1906.html},
  language = {en}
}

@article{guImprovedTrajectoryPlanning2017,
  title = {Improved {{Trajectory Planning}} for {{On}}-{{Road Self}}-{{Driving Vehicles Via Combined Graph Search}}, {{Optimization}} \& {{Topology Analysis}}},
  author = {Gu, Tianyu},
  year = {2017},
  pages = {148},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Improved Trajectory Planning for On-Road Self-Driving Vehicles Via Combined-Gu-.pdf},
  journal = {Optimization \& Topology Analysis},
  language = {en}
}

@article{guiziliniPackNetSfM3DPacking2019,
  title = {{{PackNet}}-{{SfM}}: {{3D Packing}} for {{Self}}-{{Supervised Monocular Depth Estimation}}},
  shorttitle = {{{PackNet}}-{{SfM}}},
  author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Gaidon, Adrien},
  year = {2019},
  month = may,
  abstract = {Densely estimating the depth of a scene from a single image is an ill-posed inverse problem that is seeing exciting progress with self-supervision from strong geometric cues, in particular from training using stereo imagery. In this work, we investigate the more challenging structure-from-motion (SfM) setting, learning purely from monocular videos. We propose PackNet - a novel deep architecture that leverages new 3D packing and unpacking blocks to effectively capture fine details in monocular depth map predictions. Additionally, we propose a novel velocity supervision loss that allows our model to predict metrically accurate depths, thus alleviating the need for test-time ground-truth scaling. We show that our proposed scale-aware architecture achieves state-of-the-art results on the KITTI benchmark, significantly improving upon any approach trained on monocular video, and even achieves competitive performance to stereo-trained methods.},
  archivePrefix = {arXiv},
  eprint = {1905.02693},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PackNet-SfM-Guizilini et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VRKQ7XGT/1905.html},
  journal = {arXiv:1905.02693 [cs]},
  primaryClass = {cs}
}

@article{guoGroupwiseCorrelationStereo2019,
  title = {Group-Wise {{Correlation Stereo Network}}},
  author = {Guo, Xiaoyang and Yang, Kai and Yang, Wukui and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = mar,
  abstract = {Stereo matching estimates the disparity between a rectified image pair, which is of great importance to depth sensing, autonomous driving, and other related tasks. Previous works built cost volumes with cross-correlation or concatenation of left and right features across all disparity levels, and then a 2D or 3D convolutional neural network is utilized to regress the disparity maps. In this paper, we propose to construct the cost volume by group-wise correlation. The left features and the right features are divided into groups along the channel dimension, and correlation maps are computed among each group to obtain multiple matching cost proposals, which are then packed into a cost volume. Group-wise correlation provides efficient representations for measuring feature similarities and will not lose too much information like full correlation. It also preserves better performance when reducing parameters compared with previous methods. The 3D stacked hourglass network proposed in previous works is improved to boost the performance and decrease the inference computational cost. Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets. The code is available at https://github.com/xy-guo/GwcNet},
  archivePrefix = {arXiv},
  eprint = {1903.04025},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Group-wise Correlation Stereo Network-Guo et al-2019.pdf;/Users/sunjiaming/Zotero/storage/JW3JXJ6A/1903.html},
  journal = {arXiv:1903.04025 [cs]},
  primaryClass = {cs}
}

@article{guoRelightablesVolumetricPerformance2019,
  title = {The Relightables: Volumetric Performance Capture of Humans with Realistic Relighting},
  shorttitle = {The Relightables},
  author = {Guo, Kaiwen and Lincoln, Peter and Davidson, Philip and Busch, Jay and Yu, Xueming and Whalen, Matt and Harvey, Geoff and {Orts-Escolano}, Sergio and Pandey, Rohit and Dourgarian, Jason and Tang, Danhang and Tkach, Anastasia and Kowdle, Adarsh and Cooper, Emily and Dou, Mingsong and Fanello, Sean and Fyffe, Graham and Rhemann, Christoph and Taylor, Jonathan and Debevec, Paul and Izadi, Shahram},
  year = {2019},
  month = nov,
  volume = {38},
  pages = {1--19},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3355089.3356571},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The relightables-Guo et al-2019.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{guptaLearningRichFeatures2014,
  title = {Learning {{Rich Features}} from {{RGB}}-{{D Images}} for {{Object Detection}} and {{Segmentation}}},
  author = {Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
  year = {2014},
  month = jul,
  abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
  archivePrefix = {arXiv},
  eprint = {1407.5736},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Rich Features from RGB-D Images for Object Detection and Segmentation-Gupta et al-2014.pdf;/Users/sunjiaming/Zotero/storage/FY7ALRT5/1407.html},
  journal = {arXiv:1407.5736 [cs]},
  primaryClass = {cs}
}

@article{guWeaklysupervised3DShape2020,
  title = {Weakly-Supervised {{3D Shape Completion}} in the {{Wild}}},
  author = {Gu, Jiayuan and Ma, Wei-Chiu and Manivasagam, Sivabalan and Zeng, Wenyuan and Wang, Zihao and Xiong, Yuwen and Su, Hao and Urtasun, Raquel},
  year = {2020},
  month = aug,
  abstract = {3D shape completion for real data is important but challenging, since partial point clouds acquired by real-world sensors are usually sparse, noisy and unaligned. Different from previous methods, we address the problem of learning 3D complete shape from unaligned and real-world partial point clouds. To this end, we propose a weakly-supervised method to estimate both 3D canonical shape and 6-DoF pose for alignment, given multiple partial observations associated with the same instance. The network jointly optimizes canonical shapes and poses with multi-view geometry constraints during training, and can infer the complete shape given a single partial point cloud. Moreover, learned pose estimation can facilitate partial point cloud registration. Experiments on both synthetic and real data show that it is feasible and promising to learn 3D shape completion through large-scale data without shape and pose supervision.},
  archivePrefix = {arXiv},
  eprint = {2008.09110},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Weakly-supervised 3D Shape Completion in the Wild-Gu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MUZTCND4/2008.html},
  journal = {arXiv:2008.09110 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{gwakGenerativeSparseDetection2020,
  title = {Generative {{Sparse Detection Networks}} for {{3D Single}}-Shot {{Object Detection}}},
  author = {Gwak, JunYoung and Choy, Christopher and Savarese, Silvio},
  year = {2020},
  month = jun,
  abstract = {3D object detection has been widely studied due to its potential applicability to many promising areas such as robotics and augmented reality. Yet, the sparse nature of the 3D data poses unique challenges to this task. Most notably, the observable surface of the 3D point clouds is disjoint from the center of the instance to ground the bounding box prediction on. To this end, we propose Generative Sparse Detection Network (GSDN), a fully-convolutional single-shot sparse detection network that efficiently generates the support for object proposals. The key component of our model is a generative sparse tensor decoder, which uses a series of transposed convolutions and pruning layers to expand the support of sparse tensors while discarding unlikely object centers to maintain minimal runtime and memory footprint. GSDN can process unprecedentedly large-scale inputs with a single fully-convolutional feed-forward pass, thus does not require the heuristic post-processing stage that stitches results from sliding windows as other previous methods have. We validate our approach on three 3D indoor datasets including the large-scale 3D indoor reconstruction dataset where our method outperforms the state-of-the-art methods by a relative improvement of 7.14\% while being 3.78 times faster than the best prior work.},
  archivePrefix = {arXiv},
  eprint = {2006.12356},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Generative Sparse Detection Networks for 3D Single-shot Object Detection-Gwak et al-2020.pdf;/Users/sunjiaming/Zotero/storage/7UHLIUTP/2006.html},
  journal = {arXiv:2006.12356 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{halberRescanInductiveInstance2019,
  title = {Rescan: {{Inductive Instance Segmentation}} for {{Indoor RGBD Scans}}},
  shorttitle = {Rescan},
  author = {Halber, Maciej and Shi, Yifei and Xu, Kai and Funkhouser, Thomas},
  year = {2019},
  month = sep,
  abstract = {In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these "rescans" to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation.},
  archivePrefix = {arXiv},
  eprint = {1909.11268},
  eprinttype = {arxiv},
  journal = {arXiv:1909.11268 [cs]},
  primaryClass = {cs}
}

@article{hanDeepHoughTransform2020,
  title = {Deep {{Hough Transform}} for {{Semantic Line Detection}}},
  author = {Han, Qi and Zhao, Kai and Xu, Jun and Cheng, Mingg-Ming},
  year = {2020},
  month = mar,
  abstract = {In this paper, we put forward a simple yet effective method to detect meaningful straight lines, a.k.a. semantic lines, in given scenes. Prior methods take line detection as a special case of object detection, while neglect the inherent characteristics of lines, leading to less efficient and suboptimal results. We propose a one-shot end-to-end framework by incorporating the classical Hough transform into deeply learned representations. By parameterizing lines with slopes and biases, we perform Hough transform to translate deep representations to the parametric space and then directly detect lines in the parametric space. More concretely, we aggregate features along candidate lines on the feature map plane and then assign the aggregated features to corresponding locations in the parametric domain. Consequently, the problem of detecting semantic lines in the spatial domain is transformed to spotting individual points in the parametric domain, making the post-processing steps, \textbackslash ie non-maximal suppression, more efficient. Furthermore, our method makes it easy to extract contextual line features, that are critical to accurate line detection. Experimental results on a public dataset demonstrate the advantages of our method over state-of-the-arts.},
  archivePrefix = {arXiv},
  eprint = {2003.04676},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Hough Transform for Semantic Line Detection-Han et al-2020.pdf;/Users/sunjiaming/Zotero/storage/DSXLRAID/2003.html},
  journal = {arXiv:2003.04676 [cs]},
  primaryClass = {cs}
}

@inproceedings{haneClassSpecific3D2014,
  title = {Class {{Specific 3D Object Shape Priors Using Surface Normals}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Hane, Christian and Savinov, Nikolay and Pollefeys, Marc},
  year = {2014},
  month = jun,
  pages = {652--659},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.89},
  abstract = {Dense 3D reconstruction of real world objects containing textureless, reflective and specular parts is a challenging task. Using general smoothness priors such as surface area regularization can lead to defects in the form of disconnected parts or unwanted indentations. We argue that this problem can be solved by exploiting the object class specific local surface orientations, e.g. a car is always close to horizontal in the roof area. Therefore, we formulate an object class specific shape prior in the form of spatially varying anisotropic smoothness terms. The parameters of the shape prior are extracted from training data. We detail how our shape prior formulation directly fits into recently proposed volumetric multi-label reconstruction approaches. This allows a segmentation between the object and its supporting ground. In our experimental evaluation we show reconstructions using our trained shape prior on several challenging datasets.},
  file = {/Users/sunjiaming/Zotero/storage/QH6LISH8/Hane et al. - 2014 - Class Specific 3D Object Shape Priors Using Surfac.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@article{haneDenseSemantic3D2017,
  title = {Dense {{Semantic 3D Reconstruction}}},
  author = {Hane, Christian and Zach, Christopher and Cohen, Andrea and Pollefeys, Marc},
  year = {2017},
  month = sep,
  volume = {39},
  pages = {1730--1743},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2016.2613051},
  abstract = {Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being 'too noisy'. These priors generally yield overly smooth reconstructions and/or segmentations in certain regions while they fail to constrain the solution sufficiently in other areas. In this paper, we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other's task. As a consequence, we propose a mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. On the one hand knowing about the semantic class of the geometry provides information about the likelihood of the surface direction. On the other hand the surface direction provides information about the likelihood of the semantic class. Experimental results on several data sets highlight the advantages of our joint formulation. We show how weakly observed surfaces are reconstructed more faithfully compared to a geometry only reconstruction. Thanks to the volumetric nature of our formulation we also infer surfaces which cannot be directly observed for example the surface between the ground and a building. Finally, our method returns a semantic segmentation which is consistent across the whole dataset.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dense Semantic 3D Reconstruction-Hane et al-2017.pdf;/Users/sunjiaming/Zotero/storage/QJ7THGNA/glws4_15588.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {9}
}

@misc{hanFlashFusionRealtimeGlobally2018,
  title = {{{FlashFusion}}: {{Real}}-Time {{Globally Consistent Dense 3D Reconstruction}} Using {{CPU Computing}}},
  shorttitle = {{{FlashFusion}}},
  author = {Han, Lei and Fang, Lu},
  year = {2018},
  abstract = {Aiming at the practical usage of dense 3D reconstruction on portable devices, we propose FlashFusion, a Fast LArge-Scale High-resolution (sub-centimeter level) 3D reconstruction system without the use of GPU computing. It enables globally-consistent localization through a robust yet fast global bundle adjustment scheme, and realizes spatial hashing based volumetric fusion running at 300Hz and rendering at 25Hz via highly efficient valid chunk selection and mesh extraction schemes. Extensive experiments on both real world and synthetic datasets demonstrate that FlashFusion succeeds to enable realtime, globally consistent, high-resolution (5mm), and large-scale dense 3D reconstruction using highly-constrained computation, i.e., the CPU computing on portable device.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FlashFusion-Han_Fang-2018.pdf;/Users/sunjiaming/Zotero/storage/VD5YJQU8/a4a220ae3a1664a26ad4f5be4b7ef0278830ceb2.html},
  howpublished = {/paper/FlashFusion\%3A-Real-time-Globally-Consistent-Dense-3D-Han-Fang/a4a220ae3a1664a26ad4f5be4b7ef0278830ceb2},
  journal = {Robotics: Science and Systems},
  keywords = {reconstruction},
  language = {en}
}

@article{hanockaMeshCNNNetworkEdge2018,
  ids = {hanockaMeshCNNNetworkEdge2018a},
  title = {{{MeshCNN}}: {{A Network}} with an {{Edge}}},
  shorttitle = {{{MeshCNN}}},
  author = {Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and {Cohen-Or}, Daniel},
  year = {2018},
  month = sep,
  abstract = {Polygonal meshes provide an efficient representation for 3D shapes. They explicitly capture both shape surface and topology, and leverage non-uniformity to represent large flat regions as well as sharp, intricate features. This non-uniformity and irregularity, however, inhibits mesh analysis efforts using neural networks that combine convolution and pooling operations. In this paper, we utilize the unique properties of the mesh for a direct analysis of 3D shapes using MeshCNN, a convolutional neural network designed specifically for triangular meshes. Analogous to classic CNNs, MeshCNN combines specialized convolution and pooling layers that operate on the mesh edges, by leveraging their intrinsic geodesic connections. Convolutions are applied on edges and the four edges of their incident triangles, and pooling is applied via an edge collapse operation that retains surface topology, thereby, generating new mesh connectivity for the subsequent convolutions. MeshCNN learns which edges to collapse, thus forming a task-driven process where the network exposes and expands the important features while discarding the redundant ones. We demonstrate the effectiveness of our task-driven pooling on various learning tasks applied to 3D meshes.},
  archivePrefix = {arXiv},
  eprint = {1809.05910},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MeshCNN-Hanocka et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MeshCNN-Hanocka et al-22.pdf;/Users/sunjiaming/Zotero/storage/9YG9DV2Y/1809.html;/Users/sunjiaming/Zotero/storage/AWP7YSWM/1809.html},
  journal = {arXiv:1809.05910 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hanockaPoint2MeshSelfPriorDeformable2020,
  title = {{{Point2Mesh}}: {{A Self}}-{{Prior}} for {{Deformable Meshes}}},
  shorttitle = {{{Point2Mesh}}},
  author = {Hanocka, Rana and Metzer, Gal and Giryes, Raja and {Cohen-Or}, Daniel},
  year = {2020},
  month = may,
  abstract = {In this paper, we introduce Point2Mesh, a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e., unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity.},
  archivePrefix = {arXiv},
  eprint = {2005.11084},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Point2Mesh-Hanocka et al-2020.pdf;/Users/sunjiaming/Zotero/storage/KHWEP8RM/2005.html},
  journal = {arXiv:2005.11084 [cs]},
  primaryClass = {cs}
}

@article{hanRegNetLearningOptimization2018,
  title = {{{RegNet}}: {{Learning}} the {{Optimization}} of {{Direct Image}}-to-{{Image Pose Registration}}},
  shorttitle = {{{RegNet}}},
  author = {Han, Lei and Ji, Mengqi and Fang, Lu and Nie{\ss}ner, Matthias},
  year = {2018},
  month = dec,
  abstract = {Direct image-to-image alignment that relies on the optimization of photometric error metrics suffers from limited convergence range and sensitivity to lighting conditions. Deep learning approaches has been applied to address this problem by learning better feature representations using convolutional neural networks, yet still require a good initialization. In this paper, we demonstrate that the inaccurate numerical Jacobian limits the convergence range which could be improved greatly using learned approaches. Based on this observation, we propose a novel end-to-end network, RegNet, to learn the optimization of image-to-image pose registration. By jointly learning feature representation for each pixel and partial derivatives that replace handcrafted ones (e.g., numerical differentiation) in the optimization step, the neural network facilitates end-to-end optimization. The energy landscape is constrained on both the feature representation and the learned Jacobian, hence providing more flexibility for the optimization as a consequence leads to more robust and faster convergence. In a series of experiments, including a broad ablation study, we demonstrate that RegNet is able to converge for large-baseline image pairs with fewer iterations.},
  archivePrefix = {arXiv},
  eprint = {1812.10212},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/RegNet-Han et al-2018.pdf;/Users/sunjiaming/Zotero/storage/A6C38SC2/1812.html},
  journal = {arXiv:1812.10212 [cs]},
  keywords = {optimization},
  language = {en},
  primaryClass = {cs}
}

@article{haoDualSDFSemanticShape2020,
  title = {{{DualSDF}}: {{Semantic Shape Manipulation}} Using a {{Two}}-{{Level Representation}}},
  shorttitle = {{{DualSDF}}},
  author = {Hao, Zekun and {Averbuch-Elor}, Hadar and Snavely, Noah and Belongie, Serge},
  year = {2020},
  month = apr,
  abstract = {We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.},
  archivePrefix = {arXiv},
  eprint = {2004.02869},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DualSDF-Hao et al-2020.pdf;/Users/sunjiaming/Zotero/storage/DZTFFKQP/2004.html},
  journal = {arXiv:2004.02869 [cs]},
  primaryClass = {cs}
}

@article{harchol-balterApplyingPhPrograms,
  title = {Applying to {{Ph}}.{{D}}. {{Programs}} in {{Computer Science}}},
  author = {{Harchol-Balter}, Mor},
  pages = {22},
  file = {/Users/sunjiaming/Zotero/storage/8I44IGQ7/Harchol-Balter - Applying to Ph.D. Programs in Computer Science.pdf},
  language = {en}
}

@incollection{haRecurrentWorldModels2018,
  title = {Recurrent World Models Facilitate Policy Evolution},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {2450--2462},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Recurrent world models facilitate policy evolution-Ha_Schmidhuber-2018.pdf}
}

@article{harleyEmbodiedViewContrastive3D2019,
  title = {Embodied {{View}}-{{Contrastive 3D Feature Learning}}},
  author = {Harley, Adam W. and Li, Fangyu and Lakshmikanth, Shrinidhi K. and Zhou, Xian and Tung, Hsiao-Yu Fish and Fragkiadaki, Katerina},
  year = {2019},
  month = jun,
  abstract = {Humans can effortlessly imagine the occluded side of objects in a photograph. We do not simply see the photograph as a flat 2D surface, we perceive the 3D visual world captured in it, by using our imagination to inpaint the information lost during camera projection. We propose neural architectures that similarly learn to approximately imagine abstractions of the 3D world depicted in 2D images. We show that this capability suffices to localize moving objects in 3D, without using any human annotations. Our models are recurrent neural networks that consume RGB or RGB-D videos, and learn to predict novel views of the scene from queried camera viewpoints. They are equipped with a 3D representation bottleneck that learns an egomotion-stabilized and geometrically consistent deep feature map of the 3D world scene. They estimate camera motion from frame to frame, and cancel it from the extracted 2D features before fusing them in the latent 3D map. We handle multimodality and stochasticity in prediction using ranking-based contrastive losses, and show that they can scale to photorealistic imagery, in contrast to regression or VAE alternatives. Our model proposes 3D boxes for moving objects by estimating a 3D motion flow field between its temporally consecutive 3D imaginations, and thresholding motion magnitude: camera motion has been cancelled in the latent 3D space, and thus any non-zero motion is an indication of an independently moving object. Our work underlines the importance of 3D representations and egomotion stabilization for visual recognition, and proposes a viable computational model for learning 3D visual feature representations and 3D object bounding boxes supervised by moving and watching objects move.},
  archivePrefix = {arXiv},
  eprint = {1906.03764},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Harley et al_2019_Embodied View-Contrastive 3D Feature Learning.pdf;/Users/sunjiaming/Zotero/storage/PL74SCGA/1906.html},
  journal = {arXiv:1906.03764 [cs]},
  primaryClass = {cs}
}

@article{harleyVisualRepresentationLearning2019,
  title = {Visual {{Representation Learning}} with {{3D View}}-{{Contrastive Inverse Graphics Networks}}},
  author = {Harley, Adam W. and Li, Fangyu and Lakshmikanth, Shrinidhi K. and Zhou, Xian and Tung, Hsiao-Yu Fish and Fragkiadaki, Katerina},
  year = {2019},
  month = sep,
  abstract = {Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing visual information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our (2D) retinas. This paper explores the connection between view-predictive representation learning and its role in the development of 3D visual recognition. We propose inverse graphics networks, which take as input 2.5D video streams captured by a moving camera, and map to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model can also project its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses that can handle stochasticity of the visual input and can scale view-predictive learning to more photorealistic scenes than those considered in previous works. We show that the proposed model learns 3D visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a useful and scalable self-supervised task beneficial to 3D object detection.},
  archivePrefix = {arXiv},
  eprint = {1906.03764},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual Representation Learning with 3D View-Contrastive Inverse Graphics-Harley et al-2019.pdf;/Users/sunjiaming/Zotero/storage/J8C67WYC/1906.html},
  journal = {arXiv:1906.03764 [cs]},
  primaryClass = {cs}
}

@article{hartmannLearnedMultiPatchSimilarity2017,
  title = {Learned {{Multi}}-{{Patch Similarity}}},
  author = {Hartmann, Wilfried and Galliani, Silvano and Havlena, Michal and Van Gool, Luc and Schindler, Konrad},
  year = {2017},
  month = aug,
  abstract = {Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across {$>$}2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.},
  archivePrefix = {arXiv},
  eprint = {1703.08836},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learned Multi-Patch Similarity-Hartmann et al-2017.pdf;/Users/sunjiaming/Zotero/storage/7RANIG6Z/1703.html},
  journal = {arXiv:1703.08836 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{hassabisNeuroscienceInspiredArtificialIntelligence2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year = {2017},
  month = jul,
  volume = {95},
  pages = {245--258},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.06.011},
  file = {/Users/sunjiaming/Zotero/storage/XVGP7GAU/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{hassanResolving3DHuman,
  title = {Resolving {{3D Human Pose Ambiguities}} with {{3D Scene Constraints}}},
  author = {Hassan, Mohamed and Choutas, Vasileios and Tzionas, Dimitrios and Black, Michael J},
  pages = {18},
  file = {/Users/sunjiaming/Zotero/storage/9UZKU323/Hassan et al. - Resolving 3D Human Pose Ambiguities with 3D Scene .pdf},
  language = {en}
}

@article{hassonLearningJointReconstruction2019,
  title = {Learning Joint Reconstruction of Hands and Manipulated Objects},
  author = {Hasson, Yana and Varol, G{\"u}l and Tzionas, Dimitrios and Kalevatykh, Igor and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},
  year = {2019},
  month = apr,
  abstract = {Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.},
  archivePrefix = {arXiv},
  eprint = {1904.05767},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning joint reconstruction of hands and manipulated objects-Hasson et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Q2K9HZRT/1904.html},
  journal = {arXiv:1904.05767 [cs]},
  primaryClass = {cs}
}

@article{heBagTricksImage2018,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2018},
  month = dec,
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  archivePrefix = {arXiv},
  eprint = {1812.01187},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Bag of Tricks for Image Classification with Convolutional Neural Networks-He et al-2018.pdf},
  journal = {arXiv:1812.01187 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Residual Learning for Image Recognition-He et al-2015.pdf;/Users/sunjiaming/Zotero/storage/SIZZX4Y6/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  primaryClass = {cs}
}

@article{heEpipolarTransformers2020,
  title = {Epipolar {{Transformers}}},
  author = {He, Yihui and Yan, Rui and Fragkiadaki, Katerina and Yu, Shoou-I.},
  year = {2020},
  month = may,
  abstract = {A common approach to localize 3D human joints in a synchronized and calibrated multi-view setup consists of two-steps: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) perform robust triangulation on 2D detections from each view to acquire the 3D joint locations. However, in step 1, the 2D detector is limited to solving challenging cases which could potentially be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable "epipolar transformer", which enables the 2D detector to leverage 3D-aware features to improve 2D pose estimation. The intuition is: given a 2D location p in the current view, we would like to first find its corresponding point p' in a neighboring view, and then combine the features at p' with the features at p, thus leading to a 3D-aware feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p'. Experiments on InterHand and Human3.6M show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 backbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and achieves MPJPE 26.9 mm.},
  archivePrefix = {arXiv},
  eprint = {2005.04551},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Epipolar Transformers-He et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NTUVFKZV/2005.html},
  journal = {arXiv:2005.04551 [cs]},
  primaryClass = {cs}
}

@article{heGeoPIFuGeometryPixel2020,
  title = {Geo-{{PIFu}}: {{Geometry}} and {{Pixel Aligned Implicit Functions}} for {{Single}}-View {{Human Reconstruction}}},
  shorttitle = {Geo-{{PIFu}}},
  author = {He, Tong and Collomosse, John and Jin, Hailin and Soatto, Stefano},
  year = {2020},
  month = jun,
  abstract = {We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is \$10 \textbackslash times\$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by \$42.7\textbackslash\%\$ reduction in Chamfer and Point-to-Surface Distances, and \$19.4\textbackslash\%\$ reduction in normal estimation errors.},
  archivePrefix = {arXiv},
  eprint = {2006.08072},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Geo-PIFu-He et al-2020.pdf;/Users/sunjiaming/Zotero/storage/XVMXBRZ5/2006.html},
  journal = {arXiv:2006.08072 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{heldRobustRealtimeTracking2016,
  ids = {heldRobustRealtimeTracking2016a},
  title = {Robust Real-Time Tracking Combining {{3D}} Shape, Color, and Motion},
  author = {Held, David and Levinson, Jesse and Thrun, Sebastian and Savarese, Silvio},
  year = {2016},
  month = jan,
  volume = {35},
  pages = {30--49},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364915593399},
  abstract = {Real-time tracking algorithms often suffer from low accuracy and poor robustness when confronted with difficult, real-world data. We present a tracker that combines 3D shape, color (when available), and motion cues to accurately track moving objects in real-time. Our tracker allocates computational effort based on the shape of the posterior distribution. Starting with a coarse approximation to the posterior, the tracker successively refines this distribution, increasing in tracking accuracy over time. The tracker can thus be run for any amount of time, after which the current approximation to the posterior is returned. Even at a minimum runtime of 0.37 milliseconds per object, our method outperforms all of the baseline methods of similar speed by at least 25\% in RMS tracking error. If our tracker is allowed to run for longer, the accuracy continues to improve, and it continues to outperform all baseline methods. Our tracker is thus anytime, allowing the speed or accuracy to be optimized based on the needs of the application. By combining 3D shape, color (when available), and motion cues in a probabilistic framework, our tracker is able to robustly handle changes in viewpoint, occlusions, and lighting variations for moving objects of a variety of shapes, sizes, and distances.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust real-time tracking combining 3D shape, color, and motion-Held et al-22.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {1-3}
}

@article{heMaskRCNN2017,
  title = {Mask {{R}}-{{CNN}}},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2017},
  month = mar,
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.},
  archivePrefix = {arXiv},
  eprint = {1703.06870},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Mask R-CNN-He et al-2017.pdf},
  journal = {arXiv:1703.06870 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{heMomentumContrastUnsupervised2019,
  ids = {heMomentumContrastUnsupervised2019b},
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2019},
  month = nov,
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archivePrefix = {arXiv},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Momentum Contrast for Unsupervised Visual Representation Learning-He et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Momentum Contrast for Unsupervised Visual Representation Learning-He et al-3.pdf;/Users/sunjiaming/Zotero/storage/6UH7ACVY/1911.html;/Users/sunjiaming/Zotero/storage/B2NZVHHC/1911.html},
  journal = {arXiv:1911.05722 [cs]},
  primaryClass = {cs}
}

@article{heMono3DMonocular3D2019,
  title = {{{Mono3D}}++: {{Monocular 3D Vehicle Detection}} with {{Two}}-{{Scale 3D Hypotheses}} and {{Task Priors}}},
  shorttitle = {{{Mono3D}}++},
  author = {He, Tong and Soatto, Stefano},
  year = {2019},
  month = jan,
  abstract = {We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function.},
  archivePrefix = {arXiv},
  eprint = {1901.03446},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Mono3D++-He_Soatto-2019.pdf},
  journal = {arXiv:1901.03446 [cs]},
  keywords = {3d detection},
  language = {en},
  primaryClass = {cs}
}

@article{hendersonLearningSingleImage3D2019,
  title = {Learning {{Single}}-{{Image 3D Reconstruction}} by {{Generative Modelling}} of {{Shape}}, {{Pose}} and {{Shading}}},
  author = {Henderson, Paul and Ferrari, Vittorio},
  year = {2019},
  month = oct,
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01219-8},
  abstract = {We present a unified framework tackling two problems: class-specific 3D reconstruction from a single image, and generation of new 3D shape samples. These tasks have received considerable attention recently; however, most existing approaches rely on 3D supervision, annotation of 2D images with keypoints or poses, and/or training with multiple views of each object instance. Our framework is very general: it can be trained in similar settings to existing approaches, while also supporting weaker supervision. Importantly, it can be trained purely from 2D images, without pose annotations, and with only a single view per instance. We employ meshes as an output representation, instead of voxels used in most prior work. This allows us to reason over lighting parameters and exploit shading information during training, which previous 2D-supervised methods cannot. Thus, our method can learn to generate and reconstruct concave object classes. We evaluate our approach in various settings, showing that: (i) it learns to disentangle shape from pose and lighting; (ii) using shading in the loss improves performance compared to just silhouettes; (iii) when using a standard single white light, our model outperforms state-ofthe-art 2D-supervised methods, both with and without pose supervision, thanks to exploiting shading cues; (iv) performance improves further when using multiple coloured lights, even approaching that of state-of-the-art 3D-supervised methods; (v) shapes produced by our model capture smooth surfaces and fine details better than voxel-based approaches; and (vi) our approach supports concave classes such as bathtubs and sofas, which methods based on silhouettes cannot learn.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Single-Image 3D Reconstruction by Generative Modelling of Shape, Pose-Henderson_Ferrari-2019.pdf},
  journal = {International Journal of Computer Vision},
  language = {en}
}

@article{hendersonLeveraging2DData2020,
  title = {Leveraging {{2D Data}} to {{Learn Textured 3D Mesh Generation}}},
  author = {Henderson, Paul and Tsiminaki, Vagia and Lampert, Christoph H.},
  year = {2020},
  month = apr,
  abstract = {Numerous methods have been proposed for probabilistic generative modelling of 3D objects. However, none of these is able to produce textured objects, which renders them of limited use for practical tasks. In this work, we present the first generative model of textured 3D meshes. Training such a model would traditionally require a large dataset of textured meshes, but unfortunately, existing datasets of meshes lack detailed textures. We instead propose a new training methodology that allows learning from collections of 2D images without any 3D information. To do so, we train our model to explain a distribution of images by modelling each image as a 3D foreground object placed in front of a 2D background. Thus, it learns to generate meshes that when rendered, produce images similar to those in its training set. A well-known problem when generating meshes with deep networks is the emergence of self-intersections, which are problematic for many use-cases. As a second contribution we therefore introduce a new generation process for 3D meshes that guarantees no self-intersections arise, based on the physical intuition that faces should push one another out of the way as they move. We conduct extensive experiments on our approach, reporting quantitative and qualitative results on both synthetic data and natural images. These show our method successfully learns to generate plausible and diverse textured 3D samples for five challenging object classes.},
  archivePrefix = {arXiv},
  eprint = {2004.04180},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Leveraging 2D Data to Learn Textured 3D Mesh Generation-Henderson et al-2020.pdf;/Users/sunjiaming/Zotero/storage/VMXX88A9/2004.html},
  journal = {arXiv:2004.04180 [cs]},
  primaryClass = {cs}
}

@article{heneinExploitingRigidBody,
  title = {Exploiting {{Rigid Body Motion}} for {{SLAM}} in {{Dynamic Environments}}},
  author = {Henein, Mina and Kennedy, Gerard and Mahony, Robert and Ila, Viorela},
  pages = {8},
  abstract = {The limitations of existing localisation and mapping algorithms in handling highly dynamic environments is a key roadblock in the deployment of autonomous mobile robotic systems in a range of important real world situations. In this paper we propose a technique to integrate the motion of dynamic objects into a Simultaneous Localisation and Mapping (SLAM) algorithm without the need to know a-priori or model the geometry of the object, or even to explicitly estimate the pose of the object. The benefit of this approach lies in a simplification of the underlying SLAM state and a resulting simplification of the non-linear least squares optimisation solution. We demonstrate the performance of the algorithm on two scenarios; SLAM in an urban traffic scenario, and extrinsic calibration of a multi RGBD camera system observing a moving object. Our experiments show consistent improvement in robot localisation and mapping accuracy and demonstrate potential of the proposed algorithm.},
  file = {/Users/sunjiaming/Zotero/storage/KGJ8ZJYL/Henein et al. - Exploiting Rigid Body Motion for SLAM in Dynamic E.pdf},
  language = {en}
}

@article{higginsDefinitionDisentangledRepresentations2018,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archivePrefix = {arXiv},
  eprint = {1812.02230},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Towards a Definition of Disentangled Representations-Higgins et al-2018.pdf},
  journal = {arXiv:1812.02230 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hintonDoesBrainInverse,
  title = {Does the {{Brain}} Do {{Inverse Graphics}}?},
  author = {Hinton, Geoffrey and Krizhevsky, Alex and Jaitly, Navdeep and Tieleman, Tijmen and Tang, Yichuan},
  pages = {46},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Does the Brain do Inverse Graphics-Hinton et al-.pdf},
  language = {en}
}

@article{hintonMATRIXCAPSULESEM2018,
  title = {{{MATRIX CAPSULES WITH EM ROUTING}}},
  author = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
  year = {2018},
  pages = {15},
  abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/MATRIX CAPSULES WITH EM ROUTING-Hinton et al-2018.pdf},
  language = {en}
}

@article{hintonTakingInverseGraphics,
  title = {Taking {{Inverse Graphics Seriously}}},
  author = {Hinton, Geoffrey},
  pages = {38},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Taking Inverse Graphics Seriously-Hinton-.pdf},
  language = {en}
}

@article{hoAxialAttentionMultidimensional2019,
  title = {Axial {{Attention}} in {{Multidimensional Transformers}}},
  author = {Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim},
  year = {2019},
  month = dec,
  abstract = {We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.},
  archivePrefix = {arXiv},
  eprint = {1912.12180},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Axial Attention in Multidimensional Transformers-Ho et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YH4UE2R3/1912.html},
  journal = {arXiv:1912.12180 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{hodanPhotorealisticImageSynthesis2019,
  title = {Photorealistic {{Image Synthesis}} for {{Object Instance Detection}}},
  author = {Hodan, Tomas and Vineet, Vibhav and Gal, Ran and Shalev, Emanuel and Hanzelka, Jon and Connell, Treb and Urbina, Pedro and Sinha, Sudipta N. and Guenter, Brian},
  year = {2019},
  month = feb,
  abstract = {We present an approach to synthesize highly photorealistic images of 3D object models, which we use to train a convolutional neural network for detecting the objects in real images. The proposed approach has three key ingredients: (1) 3D object models are rendered in 3D models of complete scenes with realistic materials and lighting, (2) plausible geometric configuration of objects and cameras in a scene is generated using physics simulations, and (3) high photorealism of the synthesized images achieved by physically based rendering. When trained on images synthesized by the proposed approach, the Faster R-CNN object detector achieves a 24\% absolute improvement of mAP@.75IoU on Rutgers APC and 11\% on LineMod-Occluded datasets, compared to a baseline where the training images are synthesized by rendering object models on top of random photographs. This work is a step towards being able to effectively train object detectors without capturing or annotating any real images. A dataset of 600K synthetic images with ground truth annotations for various computer vision tasks will be released on the project website: thodan.github.io/objectsynth.},
  archivePrefix = {arXiv},
  eprint = {1902.03334},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Photorealistic Image Synthesis for Object Instance Detection-Hodan et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TWS49GKB/1902.html},
  journal = {arXiv:1902.03334 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{hodanSummary4thInternational2018,
  title = {A {{Summary}} of the 4th {{International Workshop}} on {{Recovering 6D Object Pose}}},
  author = {Hodan, Tomas and Kouskouridas, Rigas and Kim, Tae-Kyun and Tombari, Federico and Bekris, Kostas and Drost, Bertram and Groueix, Thibault and Walas, Krzysztof and Lepetit, Vincent and Leonardis, Ales and Steger, Carsten and Michel, Frank and Sahin, Caner and Rother, Carsten and Matas, Jiri},
  year = {2018},
  month = oct,
  abstract = {This document summarizes the 4th International Workshop on Recovering 6D Object Pose which was organized in conjunction with ECCV 2018 in Munich. The workshop featured four invited talks, oral and poster presentations of accepted workshop papers, and an introduction of the BOP benchmark for 6D object pose estimation. The workshop was attended by 100+ people working on relevant topics in both academia and industry who shared up-to-date advances and discussed open problems.},
  archivePrefix = {arXiv},
  eprint = {1810.03758},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Summary of the 4th International Workshop on Recovering 6D Object Pose-Hodan et al-2018.pdf;/Users/sunjiaming/Zotero/storage/VF5MLWN9/1810.html},
  journal = {arXiv:1810.03758 [cs]},
  primaryClass = {cs}
}

@article{hofingerFiveElementsFlow2019,
  title = {The {{Five Elements}} of {{Flow}}},
  author = {Hofinger, Markus and Bul{\`o}, Samuel Rota and Porzi, Lorenzo and Knapitsch, Arno and Pock, Thomas and Kontschieder, Peter},
  year = {2019},
  month = dec,
  abstract = {In this work we propose five concrete steps to improve the performance of optical flow algorithms. We carefully reviewed recently introduced innovations and well-established techniques in deep learning-based flow methods including i) pyramidal feature representations, ii) flow-based consistency checks, iii) cost volume construction practices or iv) distillation, and present extensions or alternatives to inhibiting factors we identified therein. We also show how changing the way gradients propagate in modern flow networks can lead to surprising boosts in performance. Finally, we contribute a novel feature that adaptively guides the learning process towards improving on under-performing flow predictions. Our findings are conceptually simple and easy to implement, yet result in compelling improvements on relevant error measures that we demonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel and KITTI. We establish new state-of-the-art results on the challenging Sintel and Kitti 2015 test datasets, and even show the portability of our findings to different optical flow and depth from stereo approaches.},
  archivePrefix = {arXiv},
  eprint = {1912.10739},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Five Elements of Flow-Hofinger et al-2019.pdf;/Users/sunjiaming/Zotero/storage/EGWJEVQK/1912.html},
  journal = {arXiv:1912.10739 [cs]},
  primaryClass = {cs}
}

@incollection{holzmannSemanticallyAwareUrban2018,
  title = {Semantically {{Aware Urban 3D Reconstruction}} with {{Plane}}-{{Based Regularization}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Holzmann, Thomas and Maurer, Michael and Fraundorfer, Friedrich and Bischof, Horst},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11218},
  pages = {487--503},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01264-9_29},
  abstract = {We propose a method for urban 3D reconstruction, which incorporates semantic information and plane priors within the reconstruction process in order to generate visually appealing 3D models. We introduce a plane detection algorithm using 3D lines, which detects a more complete and less spurious plane set compared to point-based methods in urban environments. Further, the proposed normalized visibility-based energy formulation eases the combination of several energy terms within a tetrahedra occupancy labeling algorithm and, hence, is well suited for combining it with class specific smoothness terms. As a result, we produce visually appealing and detailed building models (i.e., straight edges and planar surfaces) and a smooth reconstruction of the surroundings.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Semantically Aware Urban 3D Reconstruction with Plane-Based Regularization-Holzmann et al-2018.pdf},
  isbn = {978-3-030-01263-2 978-3-030-01264-9},
  language = {en}
}

@article{homayounfarLevelSetRCNNDeep2020,
  title = {{{LevelSet R}}-{{CNN}}: {{A Deep Variational Method}} for {{Instance Segmentation}}},
  shorttitle = {{{LevelSet R}}-{{CNN}}},
  author = {Homayounfar, Namdar and Xiong, Yuwen and Liang, Justin and Ma, Wei-Chiu and Urtasun, Raquel},
  year = {2020},
  month = jul,
  abstract = {Obtaining precise instance segmentation masks is of high importance in many modern applications such as robotic manipulation and autonomous driving. Currently, many state of the art models are based on the Mask R-CNN framework which, while very powerful, outputs masks at low resolutions which could result in imprecise boundaries. On the other hand, classic variational methods for segmentation impose desirable global and local data and geometry constraints on the masks by optimizing an energy functional. While mathematically elegant, their direct dependence on good initialization, non-robust image cues and manual setting of hyperparameters renders them unsuitable for modern applications. We propose LevelSet R-CNN, which combines the best of both worlds by obtaining powerful feature representations that are combined in an end-to-end manner with a variational segmentation framework. We demonstrate the effectiveness of our approach on COCO and Cityscapes datasets.},
  archivePrefix = {arXiv},
  eprint = {2007.15629},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LevelSet R-CNN-Homayounfar et al-2020.pdf;/Users/sunjiaming/Zotero/storage/H4DMDXRB/2007.html},
  journal = {arXiv:2007.15629 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{horaudShortTutorialThreeDimensional,
  title = {A {{Short Tutorial}} on {{Three}}-{{Dimensional Cameras}}},
  author = {Horaud, Radu and Horaud, Radu},
  pages = {44},
  file = {/Users/sunjiaming/Zotero/storage/UNNMCC9W/Horaud and Horaud - A Short Tutorial on Three-Dimensional Cameras.pdf},
  language = {en}
}

@article{hou3DSIC3DSemantic2019,
  title = {{{3D}}-{{SIC}}: {{3D Semantic Instance Completion}} for {{RGB}}-{{D Scans}}},
  shorttitle = {{{3D}}-{{SIC}}},
  author = {Hou, Ji and Dai, Angela and Nie{\ss}ner, Matthias},
  year = {2019},
  month = apr,
  abstract = {This paper focuses on the task of semantic instance completion: from an incomplete, RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and jointly infer their complete object geometry. This enables a semantically meaningful decomposition of a scanned scene into individual, complete 3D objects. This semantic instance completion of a 3D scene opens up many new possibilities in enabling meaningful interactions with a scene, for instance for virtual or robotic agents. Rather than considering 3D semantic instance segmentation and scan completion separately, we propose 3D-SIC, a new end-to-end 3D convolutional neural network which jointly learns to detect object instances and predict their complete geometry -- achieving significantly better performance than treating these tasks independently. 3D-SIC leverages joint color-geometry feature learning and a fully-convolutional 3D network to effectively infer semantic instance completion for 3D scans at scale. Our method runs at interactive rates, taking several seconds inference time on scenes of \$30\$m \$\textbackslash times\$ \$25\$m spatial extent. For the task of semantic instance completion, we additionally introduce a new semantic instance completion benchmark on real scan data, where we outperform alternative approaches by over 15 in mAP@0.5.},
  archivePrefix = {arXiv},
  eprint = {1904.12012},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-SIC-Hou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/8Y9WRBN2/1904.html},
  journal = {arXiv:1904.12012 [cs]},
  keywords = {3d segmentation},
  primaryClass = {cs}
}

@article{hou3DSIS3DSemantic2018,
  title = {{{3D}}-{{SIS}}: {{3D Semantic Instance Segmentation}} of {{RGB}}-{{D Scans}}},
  shorttitle = {{{3D}}-{{SIS}}},
  author = {Hou, Ji and Dai, Angela and Nie{\ss}ner, Matthias},
  year = {2018},
  month = dec,
  abstract = {We introduce 3D-SIS, a novel neural network architecture for 3D semantic instance segmentation in commodity RGB-D scans. The core idea of our method is to jointly learn from both geometric and color signal, thus enabling accurate instance predictions. Rather than operate solely on 2D frames, we observe that most computer vision applications have multi-view RGB-D input available, which we leverage to construct an approach for 3D instance segmentation that effectively fuses together these multi-modal inputs. Our network leverages high-resolution RGB input by associating 2D images with the volumetric grid based on the pose alignment of the 3D reconstruction. For each image, we first extract 2D features for each pixel with a series of 2D convolutions; we then backproject the resulting feature vector to the associated voxel in the 3D grid. This combination of 2D and 3D feature learning allows significantly higher accuracy object detection and instance segmentation than state-of-the-art alternatives. We show results on both synthetic and real-world public benchmarks, achieving an improvement in mAP of over 13 on real-world data.},
  archivePrefix = {arXiv},
  eprint = {1812.07003},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D-SIS-Hou et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D-SIS-Hou et al-22.pdf;/Users/sunjiaming/Zotero/storage/6ICZHQ6E/1812.html},
  journal = {arXiv:1812.07003 [cs]},
  keywords = {3d detection,3d segmentation},
  primaryClass = {cs}
}

@article{houMobilePoseRealTimePose2020,
  title = {{{MobilePose}}: {{Real}}-{{Time Pose Estimation}} for {{Unseen Objects}} with {{Weak Shape Supervision}}},
  shorttitle = {{{MobilePose}}},
  author = {Hou, Tingbo and Ahmadyan, Adel and Zhang, Liangkai and Wei, Jianing and Grundmann, Matthias},
  year = {2020},
  month = mar,
  abstract = {In this paper, we address the problem of detecting unseen objects from RGB images and estimating their poses in 3D. We propose two mobile friendly networks: MobilePose-Base and MobilePose-Shape. The former is used when there is only pose supervision, and the latter is for the case when shape supervision is available, even a weak one. We revisit shape features used in previous methods, including segmentation and coordinate map. We explain when and why pixel-level shape supervision can improve pose estimation. Consequently, we add shape prediction as an intermediate layer in the MobilePose-Shape, and let the network learn pose from shape. Our models are trained on mixed real and synthetic data, with weak and noisy shape supervision. They are ultra lightweight that can run in real-time on modern mobile devices (e.g. 36 FPS on Galaxy S20). Comparing with previous single-shot solutions, our method has higher accuracy, while using a significantly smaller model (2\textasciitilde 3\% in model size or number of parameters).},
  archivePrefix = {arXiv},
  eprint = {2003.03522},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MobilePose-Hou et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ABLLYM2E/2003.html},
  journal = {arXiv:2003.03522 [cs]},
  primaryClass = {cs}
}

@article{houMultiViewStereoTemporal2019,
  title = {Multi-{{View Stereo}} by {{Temporal Nonparametric Fusion}}},
  author = {Hou, Yuxin and Kannala, Juho and Solin, Arno},
  year = {2019},
  month = aug,
  abstract = {We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder--decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from previous views. We train the encoder--decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.},
  archivePrefix = {arXiv},
  eprint = {1904.06397},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-View Stereo by Temporal Nonparametric Fusion-Hou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/C29TXTKT/1904.html},
  journal = {arXiv:1904.06397 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper},
  primaryClass = {cs}
}

@misc{HowReadMathematics,
  title = {How to {{Read Mathematics}}},
  file = {/Users/sunjiaming/Zotero/storage/XICBRFMU/math-read.html},
  howpublished = {https://www.people.vcu.edu/\textasciitilde dcranston/490/handouts/math-read.html}
}

@article{huACNetAttentionBased2019,
  title = {{{ACNet}}: {{Attention Based Network}} to {{Exploit Complementary Features}} for {{RGBD Semantic Segmentation}}},
  shorttitle = {{{ACNet}}},
  author = {Hu, Xinxin and Yang, Kailun and Fei, Lei and Wang, Kaiwei},
  year = {2019},
  month = may,
  abstract = {Compared to RGB semantic segmentation, RGBD semantic segmentation can achieve better performance by taking depth information into consideration. However, it is still problematic for contemporary segmenters to effectively exploit RGBD information since the feature distributions of RGB and depth (D) images vary significantly in different scenes. In this paper, we propose an Attention Complementary Network (ACNet) that selectively gathers features from RGB and depth branches. The main contributions lie in the Attention Complementary Module (ACM) and the architecture with three parallel branches. More precisely, ACM is a channel attention-based module that extracts weighted features from RGB and depth branches. The architecture preserves the inference of the original RGB and depth branches, and enables the fusion branch at the same time. Based on the above structures, ACNet is capable of exploiting more high-quality features from different channels. We evaluate our model on SUN-RGBD and NYUDv2 datasets, and prove that our model outperforms state-of-the-art methods. In particular, a mIoU score of 48.3\textbackslash\% on NYUDv2 test set is achieved with ResNet50. We will release our source code based on PyTorch and the trained segmentation model at https://github.com/anheidelonghu/ACNet.},
  archivePrefix = {arXiv},
  eprint = {1905.10089},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ACNet-Hu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4IW9XRDJ/1905.html},
  journal = {arXiv:1905.10089 [cs]},
  primaryClass = {cs}
}

@article{huangAdversarialTextureOptimization2020,
  title = {Adversarial {{Texture Optimization}} from {{RGB}}-{{D Scans}}},
  author = {Huang, Jingwei and Thies, Justus and Dai, Angela and Kundu, Abhijit and Jiang, Chiyu Max and Guibas, Leonidas and Nie{\ss}ner, Matthias and Funkhouser, Thomas},
  year = {2020},
  month = mar,
  abstract = {Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts. In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors. The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments. Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism. We train the discriminator by providing as `real' examples pairs of input views and their misaligned versions -- so that the learned adversarial loss will tolerate errors from the scans. Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art. Our code is publicly available with video demonstration.},
  archivePrefix = {arXiv},
  eprint = {2003.08400},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Adversarial Texture Optimization from RGB-D Scans-Huang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/SV2D76PG/2003.html},
  journal = {arXiv:2003.08400 [cs]},
  primaryClass = {cs}
}

@article{huangARCHAnimatableReconstruction2020,
  title = {{{ARCH}}: {{Animatable Reconstruction}} of {{Clothed Humans}}},
  shorttitle = {{{ARCH}}},
  author = {Huang, Zeng and Xu, Yuanlu and Lassner, Christoph and Li, Hao and Tung, Tony},
  year = {2020},
  month = apr,
  abstract = {In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50\% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.},
  archivePrefix = {arXiv},
  eprint = {2004.04572},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ARCH-Huang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FH6JEIPV/2004.html},
  journal = {arXiv:2004.04572 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{huangCooperativeHolisticScene,
  title = {Cooperative {{Holistic Scene Understanding}}: {{Unifying 3D Object}}, {{Layout}}, and {{Camera Pose Estimation}}},
  author = {Huang, Siyuan and Qi, Siyuan and Xiao, Yinxue and Zhu, Yixin and Wu, Ying Nian and Zhu, Song-Chun},
  pages = {12},
  abstract = {Holistic 3D indoor scene understanding refers to jointly recovering the i) object bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The existing methods either are ineffective or only tackle the problem partially. In this paper, we propose an end-to-end model that simultaneously solves all three tasks in realtime given only a single RGB image. The essence of the proposed method is to improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of directly estimating the targets, and ii) cooperative training across different modules in contrast to training these modules individually. Specifically, we parametrize the 3D object bounding boxes by the predictions from several modules, i.e., 3D camera pose and object attributes. The proposed method provides two major advantages: i) The parametrization helps maintain the consistency between the 2D image and the 3D world, thus largely reducing the prediction variances in 3D coordinates. ii) Constraints can be imposed on the parametrization to train different modules simultaneously. We call these constraints "cooperative losses" as they enable the joint training and inference. We employ three cooperative losses for 3D bounding boxes, 2D projections, and physical constraints to estimate a geometrically consistent and physically plausible 3D scene. Experiments on the SUN RGB-D dataset shows that the proposed method significantly outperforms prior approaches on 3D object detection, 3D layout estimation, 3D camera pose estimation, and holistic scene understanding.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Cooperative Holistic Scene Understanding-Huang et al-.pdf},
  language = {en}
}

@article{huangDynamicContextCorrespondence2019,
  title = {Dynamic {{Context Correspondence Network}} for {{Semantic Alignment}}},
  author = {Huang, Shuaiyi and Wang, Qiuyue and Zhang, Songyang and Yan, Shipeng and He, Xuming},
  year = {2019},
  month = sep,
  abstract = {Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality.},
  archivePrefix = {arXiv},
  eprint = {1909.03444},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dynamic Context Correspondence Network for Semantic Alignment-Huang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VAPTQF9D/1909.html},
  journal = {arXiv:1909.03444 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{huangEPNetEnhancingPoint2020,
  title = {{{EPNet}}: {{Enhancing Point Features}} with {{Image Semantics}} for {{3D Object Detection}}},
  shorttitle = {{{EPNet}}},
  author = {Huang, Tengteng and Liu, Zhe and Chen, Xiwu and Bai, Xiang},
  year = {2020},
  month = jul,
  abstract = {In this paper, we aim at addressing two critical issues in the 3D detection task, including the exploitation of multiple sensors\textasciitilde (namely LiDAR point cloud and camera image), as well as the inconsistency between the localization and classification confidence. To this end, we propose a novel fusion module to enhance the point features with semantic image features in a point-wise manner without any image annotations. Besides, a consistency enforcing loss is employed to explicitly encourage the consistency of both the localization and classification confidence. We design an end-to-end learnable framework named EPNet to integrate these two components. Extensive experiments on the KITTI and SUN-RGBD datasets demonstrate the superiority of EPNet over the state-of-the-art methods. Codes and models are available at: \textbackslash url\{https://github.com/happinesslz/EPNet\}.},
  archivePrefix = {arXiv},
  eprint = {2007.08856},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/EPNet-Huang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/XIZX3MAS/2007.html},
  journal = {arXiv:2007.08856 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{huangFrameNetLearningLocal2019,
  ids = {huangFrameNetLearningLocal},
  title = {{{FrameNet}}: {{Learning Local Canonical Frames}} of {{3D Surfaces}} from a {{Single RGB Image}}},
  shorttitle = {{{FrameNet}}},
  author = {Huang, Jingwei and Zhou, Yichao and Funkhouser, Thomas and Guibas, Leonidas},
  year = {2019},
  month = mar,
  abstract = {In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image corresponds to a surface in the underlying 3D geometry, where a canonical frame can be identified as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB. Our first insight is that canonical frames computed automatically with recently introduced direction field synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3D canonical frames through projective geometry and orthogonality constraints. In our experiments, we find that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality.},
  archivePrefix = {arXiv},
  eprint = {1903.12305},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FrameNet-Huang et al-.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FrameNet-Huang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AKBQRMI9/1903.html},
  journal = {arXiv:1903.12305 [cs]},
  primaryClass = {cs}
}

@article{huangMeshODERobustScalable2020,
  title = {{{MeshODE}}: {{A Robust}} and {{Scalable Framework}} for {{Mesh Deformation}}},
  shorttitle = {{{MeshODE}}},
  author = {Huang, Jingwei and Jiang, Chiyu Max and Leng, Baiqiang and Wang, Bin and Guibas, Leonidas},
  year = {2020},
  month = may,
  abstract = {We present MeshODE, a scalable and robust framework for pairwise CAD model deformation without prespecified correspondences. Given a pair of shapes, our framework provides a novel shape feature-preserving mapping function that continuously deforms one model to the other by minimizing fitting and rigidity losses based on the non-rigid iterative-closest-point (ICP) algorithm. We address two challenges in this problem, namely the design of a powerful deformation function and obtaining a feature-preserving CAD deformation. While traditional deformation directly optimizes for the coordinates of the mesh vertices or the vertices of a control cage, we introduce a deep bijective mapping that utilizes a flow model parameterized as a neural network. Our function has the capacity to handle complex deformations, produces deformations that are guaranteed free of self-intersections, and requires low rigidity constraining for geometry preservation, which leads to a better fitting quality compared with existing methods. It additionally enables continuous deformation between two arbitrary shapes without supervision for intermediate shapes. Furthermore, we propose a robust preprocessing pipeline for raw CAD meshes using feature-aware subdivision and a uniform graph template representation to address artifacts in raw CAD models including self-intersections, irregular triangles, topologically disconnected components, non-manifold edges, and nonuniformly distributed vertices. This facilitates a fast deformation optimization process that preserves global and local details. Our code is publicly available.},
  archivePrefix = {arXiv},
  eprint = {2005.11617},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/WNESC9FP/Huang et al. - 2020 - MeshODE A Robust and Scalable Framework for Mesh .pdf;/Users/sunjiaming/Zotero/storage/A5HGSMPU/2005.html},
  journal = {arXiv:2005.11617 [cs]},
  keywords = {Computer Science - Computational Geometry,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{huangOperatorNetRecovering3D2019,
  title = {{{OperatorNet}}: {{Recovering 3D Shapes From Difference Operators}}},
  shorttitle = {{{OperatorNet}}},
  author = {Huang, Ruqi and Rakotosaona, Marie-Julie and Achlioptas, Panos and Guibas, Leonidas and Ovsjanikov, Maks},
  year = {2019},
  month = aug,
  abstract = {This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications.},
  archivePrefix = {arXiv},
  eprint = {1904.10754},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/OperatorNet-Huang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VK3SBIMZ/1904.html},
  journal = {arXiv:1904.10754 [cs]},
  primaryClass = {cs}
}

@article{huangQuadriFlowScalableRobust2018,
  title = {{{QuadriFlow}}: {{A Scalable}} and {{Robust Method}} for {{Quadrangulation}}},
  shorttitle = {{{QuadriFlow}}},
  author = {Huang, Jingwei and Zhou, Yichao and Niessner, Matthias and Shewchuk, Jonathan Richard and Guibas, Leonidas J.},
  year = {2018},
  month = aug,
  volume = {37},
  pages = {147--160},
  issn = {01677055},
  doi = {10.1111/cgf.13498},
  abstract = {QuadriFlow is a scalable algorithm for generating quadrilateral surface meshes based on the Instant Field-Aligned Meshes of Jakob et al. (ACM Trans. Graph. 34(6):189, 2015). We modify the original algorithm such that it efficiently produces meshes with many fewer singularities. Singularities in quadrilateral meshes cause problems for many applications, including parametrization and rendering with Catmull\textendash Clark subdivision surfaces. Singularities can rarely be entirely eliminated, but it is possible to keep their number small. Local optimization algorithms usually produce meshes with many singularities, whereas the best algorithms tend to require non-local optimization, and therefore are slow. We propose an efficient method to minimize singularities by combining the Instant Meshes objective with a system of linear and quadratic constraints. These constraints are enforced by solving a global minimum-cost network flow problem and local boolean satisfiability problems. We have verified the robustness and efficiency of our method on a subset of ShapeNet comprising 17,791 3D objects in the wild. Our evaluation shows that the quality of the quadrangulations generated by our method is as good as, if not better than, those from other methods, achieving about four times fewer singularities than Instant Meshes. Other algorithms that produce similarly few singularities are much slower; we take less than ten seconds to process each model. Our source code is publicly available.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/QuadriFlow-Huang et al-2018.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {5}
}

@article{huangTensorMapsSynchronizing2019,
  title = {Tensor {{Maps}} for {{Synchronizing Heterogeneous Shape Collections}}},
  author = {Huang, Qixing and Liang, Zhenxiao and Wang, Haoyun and Zuo, Simiao and Bajaj, Chandrajit},
  year = {2019},
  month = jul,
  volume = {38},
  pages = {106:1--106:18},
  issn = {0730-0301},
  doi = {10.1145/3306346.3322944},
  abstract = {Establishing high-quality correspondence maps between geometric shapes has been shown to be the fundamental problem in managing geometric shape collections. Prior work has focused on computing efficient maps between pairs of shapes, and has shown a quantifiable benefit of joint map synchronization, where a collection of shapes are used to improve (denoise) the pairwise maps for consistency and correctness. However, these existing map synchronization techniques place very strong assumptions on the input shapes collection such as all the input shapes fall into the same category and/or the majority of the input pairwise maps are correct. In this paper, we present a multiple map synchronization approach that takes a heterogeneous shape collection as input and simultaneously outputs consistent dense pairwise shape maps. We achieve our goal by using a novel tensor-based representation for map synchronization, which is efficient and robust than all prior matrix-based representations. We demonstrate the usefulness of this approach across a wide range of geometric shape datasets and the applications in shape clustering and shape co-segmentation.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tensor Maps for Synchronizing Heterogeneous Shape Collections-Huang et al-2019.pdf},
  journal = {ACM Trans. Graph.},
  number = {4}
}

@article{huangTextureNetConsistentLocal2018,
  title = {{{TextureNet}}: {{Consistent Local Parametrizations}} for {{Learning}} from {{High}}-{{Resolution Signals}} on {{Meshes}}},
  shorttitle = {{{TextureNet}}},
  author = {Huang, Jingwei and Zhang, Haotian and Yi, Li and Funkhouser, Thomas and Nie{\ss}ner, Matthias and Guibas, Leonidas},
  year = {2018},
  month = nov,
  abstract = {We introduce, TextureNet, a neural network architecture designed to extract features from high-resolution signals associated with 3D surface meshes (e.g., color texture maps). The key idea is to utilize a 4-rotational symmetric (4-RoSy) field to define a domain for convolution on a surface. Though 4-RoSy fields have several properties favorable for convolution on surfaces (low distortion, few singularities, consistent parameterization, etc.), orientations are ambiguous up to 4-fold rotation at any sample point. So, we introduce a new convolutional operator invariant to the 4-RoSy ambiguity and use it in a network to extract features from high-resolution signals on geodesic neighborhoods of a surface. In comparison to alternatives, such as PointNet based methods which lack a notion of orientation, the coherent structure given by these neighborhoods results in significantly stronger features. As an example application, we demonstrate the benefits of our architecture for 3D semantic segmentation of textured 3D meshes. The results show that our method outperforms all existing methods on the basis of mean IoU by a significant margin in both geometry-only (6.4\%) and RGB+Geometry (6.9-8.2\%) settings.},
  archivePrefix = {arXiv},
  eprint = {1812.00020},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TextureNet-Huang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/24GT3S6C/1812.html},
  journal = {arXiv:1812.00020 [cs]},
  primaryClass = {cs}
}

@inproceedings{huaSceneNNSceneMeshes2016,
  title = {{{SceneNN}}: {{A Scene Meshes Dataset}} with {{aNNotations}}},
  shorttitle = {{{SceneNN}}},
  booktitle = {2016 {{Fourth International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Hua, Binh-Son and Pham, Quang-Hieu and Nguyen, Duc Thanh and Tran, Minh-Khoi and Yu, Lap-Fai and Yeung, Sai-Kit},
  year = {2016},
  month = oct,
  pages = {92--101},
  publisher = {{IEEE}},
  address = {{Stanford, CA, USA}},
  doi = {10.1109/3DV.2016.18},
  abstract = {Several RGB-D datasets have been publicized over the past few years for facilitating research in computer vision and robotics. However, the lack of comprehensive and fine-grained annotation in these RGB-D datasets has posed challenges to their widespread usage. In this paper, we introduce SceneNN, an RGB-D scene dataset consisting of 100 scenes. All scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. We further enriched the dataset with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses. We used the dataset as a benchmark to evaluate the state-of-the-art methods on relevant research problems such as intrinsic decomposition and shape completion. Our dataset and annotation tools are available at http://www.scenenn.net.},
  file = {/Users/sunjiaming/Zotero/storage/RUUPQFPR/Hua et al. - 2016 - SceneNN A Scene Meshes Dataset with aNNotations.pdf},
  isbn = {978-1-5090-5407-7},
  language = {en}
}

@article{huDeepSLAMObjectlevelRGBD2019,
  title = {Deep-{{SLAM}}++: {{Object}}-Level {{RGBD SLAM}} Based on Class-Specific Deep Shape Priors},
  shorttitle = {Deep-{{SLAM}}++},
  author = {Hu, Lan and Xu, Wanting and Huang, Kun and Kneip, Laurent},
  year = {2019},
  month = jul,
  abstract = {In an effort to increase the capabilities of SLAM systems and produce object-level representations, the community increasingly investigates the imposition of higher-level priors into the estimation process. One such example is given by employing object detectors to load and register full CAD models. Our work extends this idea to environments with unknown objects and imposes object priors by employing modern class-specific neural networks to generate complete model geometry proposals. The difficulty of using such predictions in a real SLAM scenario is that the prediction performance depends on the view-point and measurement quality, with even small changes of the input data sometimes leading to a large variability in the network output. We propose a discrete selection strategy that finds the best among multiple proposals from different registered views by re-enforcing the agreement with the online depth measurements. The result is an effective object-level RGBD SLAM system that produces compact, high-fidelity, and dense 3D maps with semantic annotations. It outperforms traditional fusion strategies in terms of map completeness and resilience against degrading measurement quality.},
  archivePrefix = {arXiv},
  eprint = {1907.09691},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep-SLAM++-Hu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/5JJ7LEKJ/1907.html},
  journal = {arXiv:1907.09691 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{huiLightweightOpticalFlow2019,
  title = {A {{Lightweight Optical Flow CNN}} -- {{Revisiting Data Fidelity}} and {{Regularization}}},
  author = {Hui, Tak-Wai and Tang, Xiaoou and Loy, Chen Change},
  year = {2019},
  month = mar,
  abstract = {Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2 [1], the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the footprint and 3.1 times faster in the running speed. LiteFlowNet2 which is built on the foundation laid by conventional methods has marked a milestone to achieve the corresponding roles as data fidelity and regularization in variational methods. We present an effective flow inference approach at each pyramid level through a novel lightweight cascaded network. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. A novel flow regularization layer is used to ameliorate the issue of outliers and vague flow boundaries through a novel feature-driven local convolution. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Comparing to our earlier work, LiteFlowNet2 improves the optical flow accuracy on Sintel clean pass by 24\%, Sintel final pass by 8.9\%, KITTI 2012 by 16.8\%, and KITTI 2015 by 17.5\%. Our network protocol and trained models will be made publicly available on https://github.com/twhui/LiteFlowNet2.},
  archivePrefix = {arXiv},
  eprint = {1903.07414},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Lightweight Optical Flow CNN -- Revisiting Data Fidelity and Regularization-Hui et al-2019.pdf},
  journal = {arXiv:1903.07414 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{huJointMonocular3D2018,
  title = {Joint {{Monocular 3D Vehicle Detection}} and {{Tracking}}},
  author = {Hu, Hou-Ning and Cai, Qi-Zhi and Wang, Dequan and Lin, Ji and Sun, Min and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor and Yu, Fisher},
  year = {2018},
  month = nov,
  abstract = {3D vehicle detection and tracking from a monocular camera requires detecting and associating vehicles, and estimating their locations and extents together. It is challenging because vehicles are in constant motion and it is practically impossible to recover the 3D positions from a single image. In this paper, we propose a novel framework that jointly detects and tracks 3D vehicle bounding boxes. Our approach leverages 3D pose estimation to learn 2D patch association overtime and uses temporal information from tracking to obtain stable 3D estimation. Our method also leverages 3D box depth ordering and motion to link together the tracks of occluded objects. We train our system on realistic 3D virtual environments, collecting a new diverse, large-scale and densely annotated dataset with accurate 3D trajectory annotations. Our experiments demonstrate that our method benefits from inferring 3D for both data association and tracking robustness, leveraging our dynamic 3D tracking dataset.},
  archivePrefix = {arXiv},
  eprint = {1811.10742},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Monocular 3D Vehicle Detection and Tracking-Hu et al-3.pdf;/Users/sunjiaming/Zotero/storage/MGA8R525/1811.html},
  journal = {arXiv:1811.10742 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{huLearningSegmentEvery2017,
  title = {Learning to {{Segment Every Thing}}},
  author = {Hu, Ronghang and Doll{\'a}r, Piotr and He, Kaiming and Darrell, Trevor and Girshick, Ross},
  year = {2017},
  month = nov,
  abstract = {Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to \textasciitilde 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.},
  archivePrefix = {arXiv},
  eprint = {1711.10370},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Segment Every Thing-Hu et al-2017.pdf;/Users/sunjiaming/Zotero/storage/2VR3ZZ3K/1711.html},
  journal = {arXiv:1711.10370 [cs]},
  keywords = {2d segmentation},
  primaryClass = {cs}
}

@article{humenbergerRobustImageRetrievalbased2020,
  title = {Robust {{Image Retrieval}}-Based {{Visual Localization}} Using {{Kapture}}},
  author = {Humenberger, Martin and Cabon, Yohann and Guerin, Nicolas and Morat, Julien and Revaud, J{\'e}r{\^o}me and Rerole, Philippe and Pion, No{\'e} and {de Souza}, Cesar and Leroy, Vincent and Csurka, Gabriela},
  year = {2020},
  month = jul,
  abstract = {In this paper, we present a versatile method for visual localization. It is based on robust image retrieval for coarse camera pose estimation and robust local features for accurate pose refinement. Our method is top ranked on various public datasets showing its ability of generalization and its great variety of applications. To facilitate experiments, we introduce kapture, a flexible data format and processing pipeline for structure from motion and visual localization that is released open source. We furthermore provide all datasets used in this paper in the kapture format to facilitate research and data processing. The code can be found at https://github.com/naver/kapture, the datasets as well as more information, updates, and news can be found at https://europe.naverlabs.com/research/3d-vision/kapture.},
  archivePrefix = {arXiv},
  eprint = {2007.13867},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust Image Retrieval-based Visual Localization using Kapture-Humenberger et al-2020.pdf;/Users/sunjiaming/Zotero/storage/4J4TD2SA/2007.html},
  journal = {arXiv:2007.13867 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{hurIterativeResidualRefinement2019,
  title = {Iterative {{Residual Refinement}} for {{Joint Optical Flow}} and {{Occlusion Estimation}}},
  author = {Hur, Junhwa and Roth, Stefan},
  year = {2019},
  month = apr,
  abstract = {Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.},
  archivePrefix = {arXiv},
  eprint = {1904.05290},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation-Hur_Roth-2019.pdf;/Users/sunjiaming/Zotero/storage/26P88NNG/1904.html},
  journal = {arXiv:1904.05290 [cs]},
  primaryClass = {cs}
}

@article{hurlPreciseSyntheticImage2019,
  title = {Precise {{Synthetic Image}} and {{LiDAR}} ({{PreSIL}}) {{Dataset}} for {{Autonomous Vehicle Perception}}},
  author = {Hurl, Braden and Czarnecki, Krzysztof and Waslander, Steven},
  year = {2019},
  month = apr,
  abstract = {We introduce the Precise Synthetic Image and LiDAR (PreSIL) dataset for autonomous vehicle perception. Grand Theft Auto V (GTA V), a commercial video game, has a large detailed world with realistic graphics, which provides a diverse data collection environment. Existing work creating synthetic data for autonomous driving with GTA V have not released their datasets and rely on an in-game raycasting function which represents people as cylinders and can fail to capture vehicles past 30 metres. Our work creates a precise LiDAR simulator within GTA V which collides with detailed models for all entities no matter the type or position. The PreSIL dataset consists of over 50,000 instances and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), ground point labels (point clouds), and detailed annotations for all vehicles and people. Collecting additional data with our framework is entirely automatic and requires no human annotation of any kind. We demonstrate the effectiveness of our dataset by showing an improvement of up to 5\% average precision on the KITTI 3D Object Detection benchmark challenge when state-of-the-art 3D object detection networks are pre-trained with our data. The data and code are available at https://uwaterloo.ca/waterloo-intelligent-systems-engineering-lab/projects/precise-synthetic-image-and-lidar-presil-dataset-autonomous},
  archivePrefix = {arXiv},
  eprint = {1905.00160},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle-Hurl et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4DG2GCUJ/1905.html},
  journal = {arXiv:1905.00160 [cs]},
  primaryClass = {cs}
}

@article{hurOpticalFlowEstimation2020,
  title = {Optical {{Flow Estimation}} in the {{Deep Learning Age}}},
  author = {Hur, Junhwa and Roth, Stefan},
  year = {2020},
  month = apr,
  abstract = {Akin to many subareas of computer vision, the recent advances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by classical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the developments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., unsupervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements.},
  archivePrefix = {arXiv},
  eprint = {2004.02853},
  eprinttype = {arxiv},
  journal = {arXiv:2004.02853 [cs]},
  primaryClass = {cs}
}

@article{huSqueezeandExcitationNetworks2017,
  title = {Squeeze-and-{{Excitation Networks}}},
  author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  year = {2017},
  month = sep,
  abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at minimal additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of \textasciitilde 25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  archivePrefix = {arXiv},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Squeeze-and-Excitation Networks-Hu et al-2017.pdf;/Users/sunjiaming/Zotero/storage/P3UJ6V96/1709.html},
  journal = {arXiv:1709.01507 [cs]},
  primaryClass = {cs}
}

@article{huTaichiLanguageHighperformance2019,
  title = {Taichi: A Language for High-Performance Computation on Spatially Sparse Data Structures},
  shorttitle = {Taichi},
  author = {Hu, Yuanming and Li, Tzu-Mao and Anderson, Luke and {Ragan-Kelley}, Jonathan and Durand, Fr{\'e}do},
  year = {2019},
  month = nov,
  volume = {38},
  pages = {1--16},
  issn = {07300301},
  doi = {10.1145/3355089.3356506},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Taichi-Hu et al-2019.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{huWhatYouSee2019,
  title = {What {{You See}} Is {{What You Get}}: {{Exploiting Visibility}} for {{3D Object Detection}}},
  shorttitle = {What {{You See}} Is {{What You Get}}},
  author = {Hu, Peiyun and Ziglar, Jason and Held, David and Ramanan, Deva},
  year = {2019},
  month = dec,
  abstract = {Recent advances in 3D sensing have created unique challenges for computer vision. One fundamental challenge is finding a good representation for 3D sensor data. Most popular representations (such as PointNet) are proposed in the context of processing truly 3D data (e.g. points sampled from mesh models), ignoring the fact that 3D sensored data such as a LiDAR sweep is in fact 2.5D. We argue that representing 2.5D data as collections of (x, y, z) points fundamentally destroys hidden information about freespace. In this paper, we demonstrate such knowledge can be efficiently recovered through 3D raycasting and readily incorporated into batch-based gradient learning. We describe a simple approach to augmenting voxel-based networks with visibility: we add a voxelized visibility map as an additional input stream. In addition, we show that visibility can be combined with two crucial modifications common to state-of-the-art 3D detectors: synthetic data augmentation of virtual objects and temporal aggregation of LiDAR sweeps over multiple time frames. On the NuScenes 3D detection benchmark, we show that, by adding an additional stream for visibility input, we can significantly improve the overall detection accuracy of a state-of-the-art 3D detector.},
  archivePrefix = {arXiv},
  eprint = {1912.04986},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What You See is What You Get-Hu et al-22.pdf;/Users/sunjiaming/Zotero/storage/6KBXEZEY/1912.html},
  journal = {arXiv:1912.04986 [cs]},
  primaryClass = {cs}
}

@article{hwangSegSortSegmentationDiscriminative2019,
  title = {{{SegSort}}: {{Segmentation}} by {{Discriminative Sorting}} of {{Segments}}},
  shorttitle = {{{SegSort}}},
  author = {Hwang, Jyh-Jing and Yu, Stella X. and Shi, Jianbo and Collins, Maxwell D. and Yang, Tien-Ju and Zhang, Xiao and Chen, Liang-Chieh},
  year = {2019},
  month = oct,
  abstract = {Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving \$76\textbackslash\%\$ performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.},
  archivePrefix = {arXiv},
  eprint = {1910.06962},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SegSort-Hwang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/KMEQSF3U/1910.html},
  journal = {arXiv:1910.06962 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{igarashiAsRigidAsPossibleShapeManipulation,
  title = {As-{{Rigid}}-{{As}}-{{Possible Shape Manipulation}}},
  author = {Igarashi, Takeo and Moscovich, Tomer and Hughes, John F},
  pages = {8},
  abstract = {We present an interactive system that lets a user move and deform a two-dimensional shape without manually establishing a skeleton or freeform deformation (FFD) domain beforehand. The shape is represented by a triangle mesh and the user moves several vertices of the mesh as constrained handles. The system then computes the positions of the remaining free vertices by minimizing the distortion of each triangle. While physically based simulation or iterative refinement can also be used for this purpose, they tend to be slow. We present a two-step closed-form algorithm that achieves real-time interaction. The first step finds an appropriate rotation for each triangle and the second step adjusts its scale. The key idea is to use quadratic error metrics so that each minimization problem becomes a system of linear equations. After solving the simultaneous equations at the beginning of interaction, we can quickly find the positions of free vertices during interactive manipulation. Our approach successfully conveys a sense of rigidity of the shape, which is difficult in space-warp approaches. With a multiple-point input device, even beginners can easily move, rotate, and deform shapes at will.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/As-Rigid-As-Possible Shape Manipulation-Igarashi et al-.pdf},
  language = {en}
}

@article{ilyasAdversarialExamplesAre2019,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  month = may,
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archivePrefix = {arXiv},
  eprint = {1905.02175},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Adversarial Examples Are Not Bugs, They Are Features-Ilyas et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VUG4KZMA/1905.html},
  journal = {arXiv:1905.02175 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{imDPSNetEndtoendDeep2019,
  title = {{{DPSNet}}: {{End}}-to-End {{Deep Plane Sweep Stereo}}},
  shorttitle = {{{DPSNet}}},
  author = {Im, Sunghoon and Jeon, Hae-Gon and Lin, Stephen and Kweon, In So},
  year = {2019},
  month = may,
  abstract = {Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.00538},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DPSNet-Im et al-2019.pdf;/Users/sunjiaming/Zotero/storage/IVVUQHF9/1905.html},
  journal = {arXiv:1905.00538 [cs]},
  primaryClass = {cs}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Batch Normalization-Ioffe_Szegedy-2015.pdf;/Users/sunjiaming/Zotero/storage/F6QWWCWB/1502.html},
  journal = {arXiv:1502.03167 [cs]},
  primaryClass = {cs}
}

@article{iskakovLearnableTriangulationHuman2019,
  title = {Learnable {{Triangulation}} of {{Human Pose}}},
  author = {Iskakov, Karim and Burkov, Egor and Lempitsky, Victor and Malkov, Yury},
  year = {2019},
  month = may,
  abstract = {We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page (https://saic-violet.github.io/learnable-triangulation).},
  archivePrefix = {arXiv},
  eprint = {1905.05754},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learnable Triangulation of Human Pose-Iskakov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/JXPASWAX/1905.html},
  journal = {arXiv:1905.05754 [cs]},
  primaryClass = {cs}
}

@article{izadiniaSceneRecompositionLearningbased2018,
  title = {Scene {{Recomposition}} by {{Learning}}-Based {{ICP}}},
  author = {Izadinia, Hamid and Seitz, Steven M.},
  year = {2018},
  month = dec,
  abstract = {By moving a depth sensor around a room, we compute a 3D CAD model of the environment, capturing the room shape and contents such as chairs, desks, sofas, and tables. Rather than reconstructing geometry, we match, place, and align each object in the scene to thousands of CAD models of objects. In addition to the end-to-end system, the key technical contribution is a novel approach for aligning CAD models to 3D scans, based on deep reinforcement learning. This approach, which we call Learning-based ICP, outperforms prior ICP methods in the literature, by learning the best points to match and conditioning on object viewpoint. LICP learns to align using only synthetic data and does not require ground-truth annotation of object pose or keypoint pair matching in real scene scans. While LICP is trained on synthetic data and without 3D real scene annotations, it outperforms both learned local deep feature matching and geometric based alignment methods in real scenes. Proposed method is evaluated on publicly available real scenes datasets of SceneNN and ScanNet as well as synthetic scenes of SUNCG. High quality results are demonstrated on a range of real world scenes, with robustness to clutter, viewpoint, and occlusion.},
  archivePrefix = {arXiv},
  eprint = {1812.05583},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scene Recomposition by Learning-based ICP-Izadinia_Seitz-2018.pdf;/Users/sunjiaming/Zotero/storage/UKJ7EKX9/1812.html},
  journal = {arXiv:1812.05583 [cs]},
  keywords = {reconstruction},
  primaryClass = {cs}
}

@article{jaehneVisualLocalizationLines,
  title = {{Visual Localization with Lines}},
  author = {J{\"a}hne, Dr Bernd},
  pages = {154},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual Localization with Lines-Jähne-.pdf},
  language = {de}
}

@article{jakobInstantFieldalignedMeshes2015,
  title = {Instant Field-Aligned Meshes},
  author = {Jakob, Wenzel and Tarini, Marco and Panozzo, Daniele and {Sorkine-Hornung}, Olga},
  year = {2015},
  month = oct,
  volume = {34},
  pages = {1--15},
  issn = {07300301},
  doi = {10.1145/2816795.2818078},
  abstract = {We present a novel approach to remesh a surface into an isotropic triangular or quad-dominant mesh using a unified local smoothing operator that optimizes both the edge orientations and vertex positions in the output mesh. Our algorithm produces meshes with high isotropy while naturally aligning and snapping edges to sharp features. The method is simple to implement and parallelize, and it can process a variety of input surface representations, such as point clouds, range scans and triangle meshes. Our full pipeline executes instantly (less than a second) on meshes with hundreds of thousands of faces, enabling new types of interactive workflows. Since our algorithm avoids any global optimization, and its key steps scale linearly with input size, we are able to process extremely large meshes and point clouds, with sizes exceeding several hundred million elements. To demonstrate the robustness and effectiveness of our method, we apply it to hundreds of models of varying complexity and provide our cross-platform reference implementation in the supplemental material.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Instant field-aligned meshes-Jakob et al-2015.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{jampaniSuperpixelSamplingNetworks,
  title = {Superpixel {{Sampling Networks}}},
  author = {Jampani, Varun and Sun, Deqing and Liu, Ming-Yu and Yang, Ming-Hsuan and Kautz, Jan},
  pages = {17},
  abstract = {Superpixels provide an efficient low/mid-level representation of image data, which greatly reduces the number of image primitives for subsequent vision tasks. Existing superpixel algorithms are not differentiable, making them difficult to integrate into otherwise end-to-end trainable deep neural networks. We develop a new differentiable model for superpixel sampling that leverages deep networks for learning superpixel segmentation. The resulting Superpixel Sampling Network (SSN) is end-to-end trainable, which allows learning task-specific superpixels with flexible loss functions and has fast runtime. Extensive experimental analysis indicates that SSNs not only outperform existing superpixel algorithms on traditional segmentation benchmarks, but can also learn superpixels for other tasks. In addition, SSNs can be easily integrated into downstream deep networks resulting in performance improvements.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Superpixel Sampling Networks-Jampani et al-.pdf},
  language = {en}
}

@article{jatavallabhulaGradSLAMDenseSLAM2019,
  title = {{{gradSLAM}} - {{Dense SLAM}} Meets {{Automatic Differentiation}}},
  author = {Jatavallabhula, Krishna Murthy},
  year = {2019},
  abstract = {The question of ``representation'' is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable. In this work, we present gradSLAM, a differentiable computational graph take on SLAM. Leveraging the automatic differentiation capabilities of computational graphs, gradSLAM enables the design of SLAM systems that allow for gradient-based learning across each of their components, or the system as a whole. This is achieved by creating differentiable alternatives for each non-differentiable component in a typical dense SLAM system. Specifically, we demonstrate how to design differentiable trust-region optimizers, surface measurement and fusion schemes, as well as differentiate over rays, without sacrificing performance. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/gradSLAM - Dense SLAM meets Automatic Differentiation-Jatavallabhula-2019.pdf}
}

@article{jatavallabhulaKaolinPyTorchLibrary2019,
  title = {Kaolin: {{A PyTorch Library}} for {{Accelerating 3D Deep Learning Research}}},
  shorttitle = {Kaolin},
  author = {Jatavallabhula, Krishna Murthy and Smith, Edward and Lafleche, Jean-Francois and Tsang, Clement Fuji and Rozantsev, Artem and Chen, Wenzheng and Xiang, Tommy},
  year = {2019},
  month = nov,
  abstract = {We present Kaolin, a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations of differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular 3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin mitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules including rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation metrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate a comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point for future research endeavours. Kaolin is available as open-source software at https://github.com/NVIDIAGameWorks/kaolin/.},
  archivePrefix = {arXiv},
  eprint = {1911.05063},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Kaolin-Jatavallabhula et al-2019.pdf;/Users/sunjiaming/Zotero/storage/25LT4C99/1911.html},
  journal = {arXiv:1911.05063 [cs]},
  primaryClass = {cs}
}

@article{jauDeepKeypointBasedCamera2020,
  title = {Deep {{Keypoint}}-{{Based Camera Pose Estimation}} with {{Geometric Constraints}}},
  author = {Jau, You-Yi and Zhu, Rui and Su, Hao and Chandraker, Manmohan},
  year = {2020},
  month = jul,
  abstract = {Estimating relative camera poses from consecutive frames is a fundamental problem in visual odometry (VO) and simultaneous localization and mapping (SLAM), where classic methods consisting of hand-crafted features and sampling-based outlier rejection have been a dominant choice for over a decade. Although multiple works propose to replace these modules with learning-based counterparts, most have not yet been as accurate, robust and generalizable as conventional methods. In this paper, we design an end-to-end trainable framework consisting of learnable modules for detection, feature extraction, matching and outlier rejection, while directly optimizing for the geometric pose objective. We show both quantitatively and qualitatively that pose estimation performance may be achieved on par with the classic pipeline. Moreover, we are able to show by end-to-end training, the key components of the pipeline could be significantly improved, which leads to better generalizability to unseen datasets compared to existing learning-based methods.},
  archivePrefix = {arXiv},
  eprint = {2007.15122},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints-Jau et al-2020.pdf;/Users/sunjiaming/Zotero/storage/I85CY2CK/2007.html},
  journal = {arXiv:2007.15122 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryClass = {cs}
}

@inproceedings{jeongRoadSLAMRoadMarking2017,
  title = {Road-{{SLAM}} : {{Road}} Marking Based {{SLAM}} with Lane-Level Accuracy},
  shorttitle = {Road-{{SLAM}}},
  booktitle = {2017 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Jeong, Jinyong and Cho, Younggun and Kim, Ayoung},
  year = {2017},
  month = jun,
  pages = {1736--1473},
  publisher = {{IEEE}},
  address = {{Los Angeles, CA, USA}},
  doi = {10.1109/IVS.2017.7995958},
  abstract = {In this paper, we propose the Road-SLAM algorithm, which robustly exploits road markings obtained from camera images. Road markings are well categorized and informative but susceptible to visual aliasing for global localization. To enable loop-closures using road marking matching, our method defines a feature consisting of road markings and surrounding lanes as a sub-map. The proposed method uses random forest method to improve the accuracy of matching using a sub-map containing road information. The random forest classifies road markings into six classes and only incorporates informative classes to avoid ambiguity. The proposed method is validated by comparing the SLAM result with RTKGlobal Positioning System (GPS) data. Accurate loop detection improves global accuracy by compensating for cumulative errors in odometry sensors. This method achieved an average global accuracy of 1.098 m over 4.7 km of path length, while running at real-time performance.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Road-SLAM -Jeong et al-2017.pdf},
  isbn = {978-1-5090-4804-5},
  keywords = {semantic slam},
  language = {en}
}

@article{jeonJointLearningSemantic2019,
  title = {Joint {{Learning}} of {{Semantic Alignment}} and {{Object Landmark Detection}}},
  author = {Jeon, Sangryul and Min, Dongbo and Kim, Seungryong and Sohn, Kwanghoon},
  year = {2019},
  month = oct,
  abstract = {Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several standard benchmarks for semantic matching and landmark detection, including a newly introduced dataset, JLAD, which contains larger number of challenging image pairs than existing datasets.},
  archivePrefix = {arXiv},
  eprint = {1910.00754},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Learning of Semantic Alignment and Object Landmark Detection-Jeon et al-2019.pdf},
  journal = {arXiv:1910.00754 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{jiangAcquisitionLocalizationConfidence2018,
  title = {Acquisition of {{Localization Confidence}} for {{Accurate Object Detection}}},
  author = {Jiang, Borui and Luo, Ruixuan and Mao, Jiayuan and Xiao, Tete and Jiang, Yuning},
  year = {2018},
  month = jul,
  abstract = {Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors.},
  archivePrefix = {arXiv},
  eprint = {1807.11590},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Acquisition of Localization Confidence for Accurate Object Detection-Jiang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/C3DSHVM3/1807.html},
  journal = {arXiv:1807.11590 [cs]},
  keywords = {2d detection},
  primaryClass = {cs}
}

@article{jiangLocalImplicitGrid2020,
  title = {Local {{Implicit Grid Representations}} for {{3D Scenes}}},
  author = {Jiang, Chiyu Max and Sud, Avneesh and Makadia, Ameesh and Huang, Jingwei and Nie{\ss}ner, Matthias and Funkhouser, Thomas},
  year = {2020},
  month = mar,
  abstract = {Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale -- i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing significantly better results than alternative approaches.},
  archivePrefix = {arXiv},
  eprint = {2003.08981},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Local Implicit Grid Representations for 3D Scenes-Jiang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/AJKQTLFQ/2003.html},
  journal = {arXiv:2003.08981 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{jiangMNNUniversalEfficient,
  title = {{{MNN}}: {{A Universal}} and {{Efficient Inference Engine}}},
  author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},
  pages = {13},
  abstract = {Deploying deep learning models on mobile devices draws more and more attention recently. However, designing an efficient inference engine on devices is under the great challenges of model compatibility, device diversity, and resource limitation. To deal with these challenges, we propose Mobile Neural Network (MNN), a universal and efficient inference engine tailored to mobile applications. In this paper, the contributions of MNN include: (1) presenting a mechanism called pre-inference that manages to conduct runtime optimization; (2) delivering thorough kernel optimization on operators to achieve optimal computation performance; (3) introducing backend abstraction module which enables hybrid scheduling and keeps the engine lightweight. Extensive benchmark experiments demonstrate that MNN performs favorably against other popular lightweight deep learning frameworks. MNN is available to public at: https://github.com/alibaba/MNN.},
  file = {/Users/sunjiaming/Zotero/storage/32I7NUNH/Jiang et al. - MNN A Universal and Efficient Inference Engine.pdf},
  language = {en}
}

@article{jiangSDFDiffDifferentiableRendering2019,
  ids = {jiangSDFDiffDifferentiableRendering2019a},
  title = {{{SDFDiff}}: {{Differentiable Rendering}} of {{Signed Distance Fields}} for {{3D Shape Optimization}}},
  shorttitle = {{{SDFDiff}}},
  author = {Jiang, Yue and Ji, Dantong and Han, Zhizhong and Zwicker, Matthias},
  year = {2019},
  month = dec,
  abstract = {We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDF). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {1912.07109},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SDFDiff-Jiang et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SDFDiff-Jiang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/3EDHUPYJ/1912.html;/Users/sunjiaming/Zotero/storage/6U468PS3/1912.html},
  journal = {arXiv:1912.07109 [cs]},
  primaryClass = {cs}
}

@article{jiangShapeFlowLearnableDeformations2020,
  title = {{{ShapeFlow}}: {{Learnable Deformations Among 3D Shapes}}},
  shorttitle = {{{ShapeFlow}}},
  author = {Jiang, Chiyu "Max" and Huang, Jingwei and Tagliasacchi, Andrea and Guibas, Leonidas},
  year = {2020},
  month = jun,
  abstract = {We present ShapeFlow, a flow-based model for learning a deformation space for entire classes of 3D shapes with large intra-class variations. ShapeFlow allows learning a multi-template deformation space that is agnostic to shape topology, yet preserves fine geometric details. Different from a generative space where a latent vector is directly decoded into a shape, a deformation space decodes a vector into a continuous flow that can advect a source shape towards a target. Such a space naturally allows the disentanglement of geometric style (coming from the source) and structural pose (conforming to the target). We parametrize the deformation between geometries as a learned continuous flow field via a neural network and show that such deformations can be guaranteed to have desirable properties, such as be bijectivity, freedom from self-intersections, or volume preservation. We illustrate the effectiveness of this learned deformation space for various downstream applications, including shape generation via deformation, geometric style transfer, unsupervised learning of a consistent parameterization for entire classes of shapes, and shape interpolation.},
  archivePrefix = {arXiv},
  eprint = {2006.07982},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/S678H56G/2006.html},
  journal = {arXiv:2006.07982 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{jinDRKFDDifferentiableVisual,
  title = {{{DR}}-{{KFD}}: {{A Differentiable Visual Metric}} for {{3D Shape Reconstruction}}},
  author = {Jin, Jiongchao and Patil, Akshay Gadi},
  pages = {10},
  abstract = {We advocate the use of differential visual shape metrics to train deep neural networks for 3D reconstruction. We introduce such a metric which compares two 3D shapes by measuring visual, image-space differences between multiview images differentiably rendered from the shapes. Furthermore, we develop a differentiable image-space distance based on mean-squared errors defined over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape metric can be easily plugged into various reconstruction networks, replacing the object-space distortion measures, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstruction results with better structural fidelity and visual quality. We demonstrate this both objectively, using well-known visual shape metrics for retrieval and classification tasks that are independent from our new metric, and subjectively through a perceptual study.},
  file = {/Users/sunjiaming/Zotero/storage/YPYM4HNK/Jin and Patil - DR-KFD A Differentiable Visual Metric for 3D Shap.pdf},
  language = {en}
}

@article{jinImageMatchingWide2020,
  title = {Image {{Matching}} across {{Wide Baselines}}: {{From Paper}} to {{Practice}}},
  shorttitle = {Image {{Matching}} across {{Wide Baselines}}},
  author = {Jin, Yuhe and Mishkin, Dmytro and Mishchuk, Anastasiia and Matas, Jiri and Fua, Pascal and Yi, Kwang Moo and Trulls, Eduard},
  year = {2020},
  month = mar,
  abstract = {We introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the downstream task -- the accuracy of the reconstructed camera pose -- as our primary metric. Our pipeline's modular structure allows us to easily integrate, configure, and combine methods and heuristics. We demonstrate this by embedding dozens of popular algorithms and evaluating them, from seminal works to the cutting edge of machine learning research. We show that with proper settings, classical solutions may still outperform the perceived state of the art. Besides establishing the actual state of the art, the experiments conducted in this paper reveal unexpected properties of SfM pipelines that can be exploited to help improve their performance, for both algorithmic and learned methods. Data and code are online https://github.com/vcg-uvic/image-matching-benchmark, providing an easy-to-use and flexible framework for the benchmarking of local feature and robust estimation methods, both alongside and against top-performing methods. This work provides the basis for an open challenge on wide-baseline image matching https://vision.uvic.ca/image-matching-challenge .},
  archivePrefix = {arXiv},
  eprint = {2003.01587},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Image Matching across Wide Baselines-Jin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/28BHCIQ7/2003.html},
  journal = {arXiv:2003.01587 [cs]},
  primaryClass = {cs}
}

@article{jiSurfaceNetEndtoend3D2017,
  title = {{{SurfaceNet}}: {{An End}}-to-End {{3D Neural Network}} for {{Multiview Stereopsis}}},
  shorttitle = {{{SurfaceNet}}},
  author = {Ji, Mengqi and Gall, Juergen and Zheng, Haitian and Liu, Yebin and Fang, Lu},
  year = {2017},
  month = oct,
  pages = {2326--2334},
  doi = {10.1109/ICCV.2017.253},
  abstract = {This paper proposes an end-to-end learning framework for multiview stereopsis. We term the network SurfaceNet. It takes a set of images and their corresponding camera parameters as input and directly infers the 3D model. The key advantage of the framework is that both photo-consistency as well geometric relations of the surface structure can be directly learned for the purpose of multiview stereopsis in an end-to-end fashion. SurfaceNet is a fully 3D convolutional network which is achieved by encoding the camera parameters together with the images in a 3D voxel representation. We evaluate SurfaceNet on the large-scale DTU benchmark.},
  archivePrefix = {arXiv},
  eprint = {1708.01749},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SurfaceNet-Ji et al-2017.pdf;/Users/sunjiaming/Zotero/storage/62CN2VMU/1708.html},
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper}
}

@article{jiSurfaceNetEndtoend3D2020,
  title = {{{SurfaceNet}}+: {{An End}}-to-End {{3D Neural Network}} for {{Very Sparse Multi}}-View {{Stereopsis}}},
  shorttitle = {{{SurfaceNet}}+},
  author = {Ji, Mengqi and Zhang, Jinzhi and Dai, Qionghai and Fang, Lu},
  year = {2020},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2996798},
  abstract = {Multi-view stereopsis (MVS) tries to recover the 3D model from 2D images. As the observations become sparser, the significant 3D information loss makes the MVS problem more challenging. Instead of only focusing on densely sampled conditions, we investigate sparse-MVS with large baseline angles since the sparser sensation is more practical and more cost-efficient. By investigating various observation sparsities, we show that the classical depth-fusion pipeline becomes powerless for the case with a larger baseline angle that worsens the photo-consistency check. As another line of the solution, we present SurfaceNet+, a volumetric method to handle the 'incompleteness' and the 'inaccuracy' problems induced by a very sparse MVS setup. Specifically, the former problem is handled by a novel volume-wise view selection approach. It owns superiority in selecting valid views while discarding invalid occluded views by considering the geometric prior. Furthermore, the latter problem is handled via a multi-scale strategy that consequently refines the recovered geometry around the region with the repeating pattern. The experiments demonstrate the tremendous performance gap between SurfaceNet+ and state-of-the-art methods in terms of precision and recall. Under the extreme sparse-MVS settings in two datasets, where existing methods can only return very few points, SurfaceNet+ still works as well as in the dense MVS setting. The benchmark and the implementation are publicly available at https://github.com/mjiUST/SurfaceNet-plus.},
  archivePrefix = {arXiv},
  eprint = {2005.12690},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SurfaceNet+-Ji et al-2020.pdf;/Users/sunjiaming/Zotero/storage/VIE9HDD7/2005.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,neufu_paper}
}

@article{johnsonPerceptualLossesRealTime2016,
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  month = mar,
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archivePrefix = {arXiv},
  eprint = {1603.08155},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Perceptual Losses for Real-Time Style Transfer and Super-Resolution-Johnson et al-2016.pdf},
  journal = {arXiv:1603.08155 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{jonschkowskiWhatMattersUnsupervised2020,
  title = {What {{Matters}} in {{Unsupervised Optical Flow}}},
  author = {Jonschkowski, Rico and Stone, Austin and Barron, Jonathan T. and Gordon, Ariel and Konolige, Kurt and Angelova, Anelia},
  year = {2020},
  month = jun,
  abstract = {We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.},
  archivePrefix = {arXiv},
  eprint = {2006.04902},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What Matters in Unsupervised Optical Flow-Jonschkowski et al-2020.pdf;/Users/sunjiaming/Zotero/storage/DUHWBIMZ/2006.html},
  journal = {arXiv:2006.04902 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{jonschkowskiWhatMattersUnsupervised2020a,
  title = {What {{Matters}} in {{Unsupervised Optical Flow}}},
  author = {Jonschkowski, Rico and Stone, Austin and Barron, Jonathan T. and Gordon, Ariel and Konolige, Kurt and Angelova, Anelia},
  year = {2020},
  month = jun,
  abstract = {We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.},
  archivePrefix = {arXiv},
  eprint = {2006.04902},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What Matters in Unsupervised Optical Flow-Jonschkowski et al-22.pdf;/Users/sunjiaming/Zotero/storage/PYMJHT4K/2006.html},
  journal = {arXiv:2006.04902 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{jylankiExactAlgorithmFinding,
  title = {An {{Exact Algorithm}} for {{Finding Minimum Oriented Bounding Boxes}}},
  author = {Jyl{\"a}nki, Jukka},
  pages = {13},
  abstract = {A new method is presented for computing tight-fitting enclosing bounding boxes for point sets in three dimensions. The algorithm is based on enumerating all box orientations that are uniquely determined by combinations of edges in the convex hull of the input point set. By using a graph search technique over the vertex graph of the hull, the iteration can be done quickly in expected O(n3/2(log n)2) time for input point sets that have a uniform distribution of directions on their convex hull. Under very specific conditions, the algorithm runs in worst case O(n3 log n) complexity. Surprisingly, empirical evidence shows that this process always yields the globally minimum bounding box by volume, which leads to a conjecture that this method is in fact optimal.},
  file = {/Users/sunjiaming/Zotero/storage/3PU4TWSA/Jylänki - An Exact Algorithm for Finding Minimum Oriented Bo.pdf},
  keywords = {graphics},
  language = {en}
}

@article{kalogerakisLearning3DMesh2010,
  title = {Learning {{3D Mesh Segmentation}} and {{Labeling}}},
  author = {Kalogerakis, Evangelos and Hertzmann, Aaron and Singh, Karan},
  year = {2010},
  pages = {12},
  abstract = {This paper presents a data-driven approach to simultaneous segmentation and labeling of parts in 3D meshes. An objective function is formulated as a Conditional Random Field model, with terms assessing the consistency of faces with labels, and terms between labels of neighboring faces. The objective function is learned from a collection of labeled training meshes. The algorithm uses hundreds of geometric and contextual label features and learns different types of segmentations for different tasks, without requiring manual parameter tuning. Our algorithm achieves a significant improvement in results over the state-of-the-art when evaluated on the Princeton Segmentation Benchmark, often producing segmentations and labelings comparable to those produced by humans.},
  file = {/Users/sunjiaming/Zotero/storage/XYJM5TUH/Kalogerakis et al. - 2010 - Learning 3D Mesh Segmentation and Labeling.pdf},
  language = {en}
}

@article{kanazawaLearning3DHuman2018,
  title = {Learning {{3D Human Dynamics}} from {{Video}}},
  author = {Kanazawa, Angjoo and Zhang, Jason Y. and Felsen, Panna and Malik, Jitendra},
  year = {2018},
  month = dec,
  abstract = {From an image of a person in action, we can easily guess the 3D motion of the person in the immediate past and future. This is because we have a mental model of 3D human dynamics that we have acquired from observing visual sequences of humans in motion. We present a framework that can similarly learn a representation of 3D dynamics of humans from video via a simple but effective temporal encoding of image features. At test time, from video, the learned temporal representation can recover smooth 3D mesh predictions. From a single image, our model can recover the current 3D mesh as well as its 3D past and future motion. Our approach is designed so it can learn from videos with 2D pose annotations in a semi-supervised manner. However, annotated data is always limited. On the other hand, there are millions of videos uploaded daily on the Internet. In this work, we harvest this Internet-scale source of unlabeled data by training our model on them with pseudo-ground truth 2D pose obtained from an off-the-shelf 2D pose detector. Our experiments show that adding more videos with pseudo-ground truth 2D pose monotonically improves 3D prediction performance. We evaluate our model on the recent challenging dataset of 3D Poses in the Wild and obtain state-of-the-art performance on the 3D prediction task without any fine-tuning. The project website with video can be found at https://akanazawa.github.io/human\_dynamics/.},
  archivePrefix = {arXiv},
  eprint = {1812.01601},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning 3D Human Dynamics from Video-Kanazawa et al-2018.pdf;/Users/sunjiaming/Zotero/storage/AD9VVHWY/1812.html},
  journal = {arXiv:1812.01601 [cs]},
  primaryClass = {cs}
}

@article{kanekoMeshDepthDisconnectedMeshbased2019,
  title = {{{MeshDepth}}: {{Disconnected Mesh}}-Based {{Deep Depth Prediction}}},
  shorttitle = {{{MeshDepth}}},
  author = {Kaneko, Masaya and Sakurada, Ken and Aizawa, Kiyoharu},
  year = {2019},
  month = may,
  abstract = {We propose a novel method for mesh-based single-view depth estimation using Convolutional Neural Networks (CNNs). Conventional CNN-based methods are only suitable for representing simple 3D objects because they estimate the deformation from a predefined simple mesh such as a cube or sphere. As a 3D scene representation, we introduce a disconnected mesh made of 2D mesh adaptively determined on the input image. We made a CNN-based framework to compute depths and normals of faces of the mesh. Because of the representation, our method can handle complex indoor scenes. Using common RGBD datasets, we show that our model achieved best or comparable performance comparing to the state-of-the-art pixel-wise dense methods. It should be noted that our method significantly reduces the number of the parameter representing the 3D structure.},
  archivePrefix = {arXiv},
  eprint = {1905.01312},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MeshDepth-Kaneko et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RKDGMSBF/1905.html},
  journal = {arXiv:1905.01312 [cs]},
  primaryClass = {cs}
}

@inproceedings{kangObjectDetectionVideo2016,
  title = {Object {{Detection}} from {{Video Tubelets}} with {{Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kang, Kai and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
  year = {2016},
  month = jun,
  pages = {817--825},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.95},
  abstract = {Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (RCNN)). The lately introduced ImageNet [6] task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task. Code is available at https://github.com/ myfavouritekk/vdetlib.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object Detection from Video Tubelets with Convolutional Neural Networks-Kang et al-22.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}

@article{kangObjectDetectionVideos2017,
  title = {Object {{Detection}} in {{Videos}} with {{Tubelet Proposal Networks}}},
  author = {Kang, Kai and Li, Hongsheng and Xiao, Tong and Ouyang, Wanli and Yan, Junjie and Liu, Xihui and Wang, Xiaogang},
  year = {2017},
  month = jul,
  pages = {889--897},
  doi = {10.1109/CVPR.2017.101},
  abstract = {Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos is vital for object detection. To fully utilize temporal information, state-of-the-art methods are based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets efficiently, but the lengths are generally only several frames, which is not optimal for incorporating long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computationally expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. Experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.},
  archivePrefix = {arXiv},
  eprint = {1702.06355},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object Detection in Videos with Tubelet Proposal Networks-Kang et al-2017.pdf;/Users/sunjiaming/Zotero/storage/73A3ZRYT/1702.html},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}

@inproceedings{karCategoryspecificObjectReconstruction2015,
  title = {Category-Specific Object Reconstruction from a Single Image},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kar, Abhishek and Tulsiani, Shubham and Carreira, Joao and Malik, Jitendra},
  year = {2015},
  month = jun,
  pages = {1966--1974},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298807},
  abstract = {Object reconstruction from a single image \textendash{} in the wild \textendash{} is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Category-specific object reconstruction from a single image-Kar et al-2015.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@article{karEvidenceThatRecurrent2019,
  title = {Evidence That Recurrent Circuits Are Critical to the Ventral Stream's Execution of Core Object Recognition Behavior},
  author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  year = {2019},
  month = apr,
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0392-5},
  file = {/Users/sunjiaming/Zotero/storage/I53HBNVJ/Kar et al. - 2019 - Evidence that recurrent circuits are critical to t.pdf},
  journal = {Nature Neuroscience},
  language = {en}
}

@article{karkusDifferentiableMappingNetworks2020,
  title = {Differentiable {{Mapping Networks}}: {{Learning Structured Map Representations}} for {{Sparse Visual Localization}}},
  shorttitle = {Differentiable {{Mapping Networks}}},
  author = {Karkus, Peter and Angelova, Anelia and Vanhoucke, Vincent and Jonschkowski, Rico},
  year = {2020},
  month = may,
  abstract = {Mapping and localization, preferably from a small number of observations, are fundamental tasks in robotics. We address these tasks by combining spatial structure (differentiable mapping) and end-to-end learning in a novel neural network architecture: the Differentiable Mapping Network (DMN). The DMN constructs a spatially structured view-embedding map and uses it for subsequent visual localization with a particle filter. Since the DMN architecture is end-to-end differentiable, we can jointly learn the map representation and localization using gradient descent. We apply the DMN to sparse visual localization, where a robot needs to localize in a new environment with respect to a small number of images from known viewpoints. We evaluate the DMN using simulated environments and a challenging real-world Street View dataset. We find that the DMN learns effective map representations for visual localization. The benefit of spatial structure increases with larger environments, more viewpoints for mapping, and when training data is scarce. Project website: http://sites.google.com/view/differentiable-mapping},
  archivePrefix = {arXiv},
  eprint = {2005.09530},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Differentiable Mapping Networks-Karkus et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9B9MPSDJ/2005.html},
  journal = {arXiv:2005.09530 [cs]},
  primaryClass = {cs}
}

@article{karLearningMultiViewStereo,
  title = {Learning a {{Multi}}-{{View Stereo Machine}}},
  author = {Kar, Abhishek and H{\"a}ne, Christian and Malik, Jitendra},
  pages = {12},
  abstract = {We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.},
  file = {/Users/sunjiaming/Zotero/storage/NHGAYPAB/Kar et al. - Learning a Multi-View Stereo Machine.pdf},
  keywords = {neufu_paper},
  language = {en}
}

@article{kaskmanHomebrewedDBRGBDDataset2019,
  title = {{{HomebrewedDB}}: {{RGB}}-{{D Dataset}} for {{6D Pose Estimation}} of {{3D Objects}}},
  shorttitle = {{{HomebrewedDB}}},
  author = {Kaskman, Roman and Zakharov, Sergey and Shugurov, Ivan and Ilic, Slobodan},
  year = {2019},
  month = apr,
  abstract = {One of the most important prerequisites for creating and evaluating 6D object pose detectors are datasets with labeled 6D poses. In the advent of deep learning methods, demand for such datasets is consinuously arising. Despite the fact that some of those exist, they are scarce and typically have restricted setups, e.g. a single object per sequence, or focus on specific object types, such as textureless industrial parts. Besides, two significant components are often ignored: training only from available 3D models instead of real data and scalability, i.e. training one method to detect all objects rather than training one detector per object. Other challenges, such as occlusions, changing light conditions and object appearance changes, as well as precisely defined benchmarks are either not present or scattered among different datasets. In this paper we present dataset for 6D pose estimation that covers the above-mentioned challenges, mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, light and object appearance changes. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. Moreover, we present a set benchmarks with the purpose of testing various desired properties of the detectors, particularly focusing on scalability with respect to the number of objects, resistance to changing light conditions, occlusions and clutter. We also set a baseline for the presented benchmarks using a publicly available state of the art detector. Considering difficulties in making such datasets, we plan to release the code allowing other researchers to extend this dataset or make their own datasets in the future.},
  archivePrefix = {arXiv},
  eprint = {1904.03167},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/HomebrewedDB-Kaskman et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AEXYTNWE/1904.html},
  journal = {arXiv:1904.03167 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{katharopoulosTransformersAreRNNs2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = aug,
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \$\textbackslash mathcal\{O\}\textbackslash left(N\^2\textbackslash right)\$ to \$\textbackslash mathcal\{O\}\textbackslash left(N\textbackslash right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archivePrefix = {arXiv},
  eprint = {2006.16236},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/FPWMGQMY/Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;/Users/sunjiaming/Zotero/storage/J7C3GL9N/2006.html},
  journal = {arXiv:2006.16236 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{katoNeural3DMesh2017,
  title = {Neural {{3D Mesh Renderer}}},
  author = {Kato, Hiroharu and Ushiku, Yoshitaka and Harada, Tatsuya},
  year = {2017},
  month = nov,
  abstract = {For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.},
  archivePrefix = {arXiv},
  eprint = {1711.07566},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural 3D Mesh Renderer-Kato et al-2017.pdf;/Users/sunjiaming/Zotero/storage/SMTXKVJZ/1711.html},
  journal = {arXiv:1711.07566 [cs]},
  primaryClass = {cs}
}

@article{katoSelfsupervisedLearning3D2019,
  title = {Self-Supervised {{Learning}} of {{3D Objects}} from {{Natural Images}}},
  author = {Kato, Hiroharu and Harada, Tatsuya},
  year = {2019},
  month = nov,
  abstract = {We present a method to learn single-view reconstruction of the 3D shape, pose, and texture of objects from categorized natural images in a self-supervised manner. Since this is a severely ill-posed problem, carefully designing a training method and introducing constraints are essential. To avoid the difficulty of training all elements at the same time, we propose training category-specific base shapes with fixed pose distribution and simple textures first, and subsequently training poses and textures using the obtained shapes. Another difficulty is that shapes and backgrounds sometimes become excessively complicated to mistakenly reconstruct textures on object surfaces. To suppress it, we propose using strong regularization and constraints on object surfaces and background images. With these two techniques, we demonstrate that we can use natural image collections such as CIFAR-10 and PASCAL objects for training, which indicates the possibility to realize 3D object reconstruction on diverse object categories beyond synthetic datasets.},
  archivePrefix = {arXiv},
  eprint = {1911.08850},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-supervised Learning of 3D Objects from Natural Images-Kato_Harada-2019.pdf;/Users/sunjiaming/Zotero/storage/J93X7QC5/1911.html},
  journal = {arXiv:1911.08850 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{kazhdanPoissonSurfaceReconstruction,
  title = {Poisson Surface Reconstruction},
  author = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
  pages = {10},
  abstract = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Poisson surface reconstruction-Kazhdan et al-.pdf},
  language = {en}
}

@article{kazhdanScreenedPoissonSurface2013,
  title = {Screened Poisson Surface Reconstruction},
  author = {Kazhdan, Michael and Hoppe, Hugues},
  year = {2013},
  month = jun,
  volume = {32},
  pages = {1--13},
  issn = {07300301},
  doi = {10.1145/2487228.2487237},
  file = {/Users/sunjiaming/Zotero/storage/J7VIZ5RD/Kazhdan and Hoppe - 2013 - Screened poisson surface reconstruction.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {neufu_paper},
  language = {en},
  number = {3}
}

@article{keGSNetJointVehicle2020,
  title = {{{GSNet}}: {{Joint Vehicle Pose}} and {{Shape Reconstruction}} with {{Geometrical}} and {{Scene}}-Aware {{Supervision}}},
  shorttitle = {{{GSNet}}},
  author = {Ke, Lei and Li, Shichao and Sun, Yanan and Tai, Yu-Wing and Tang, Chi-Keung},
  year = {2020},
  month = jul,
  abstract = {We present a novel end-to-end framework named as GSNet (Geometric and Scene-aware Network), which jointly estimates 6DoF poses and reconstructs detailed 3D car shapes from single urban street view. GSNet utilizes a unique four-way feature extraction and fusion scheme and directly regresses 6DoF poses and shapes in a single forward pass. Extensive experiments show that our diverse feature extraction and fusion scheme can greatly improve model performance. Based on a divide-and-conquer 3D shape representation strategy, GSNet reconstructs 3D vehicle shape with great detail (1352 vertices and 2700 faces). This dense mesh representation further leads us to consider geometrical consistency and scene context, and inspires a new multi-objective loss function to regularize network training, which in turn improves the accuracy of 6D pose estimation and validates the merit of jointly performing both tasks. We evaluate GSNet on the largest multi-task ApolloCar3D benchmark and achieve state-of-the-art performance both quantitatively and qualitatively. Project page is available at https://lkeab.github.io/gsnet/.},
  archivePrefix = {arXiv},
  eprint = {2007.13124},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GSNet-Ke et al-2020.pdf;/Users/sunjiaming/Zotero/storage/J3FZQDQY/2007.html},
  journal = {arXiv:2007.13124 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{kehlDeepLearningLocal2016,
  title = {Deep {{Learning}} of {{Local RGB}}-{{D Patches}} for {{3D Object Detection}} and {{6D Pose Estimation}}},
  author = {Kehl, Wadim and Milletari, Fausto and Tombari, Federico and Ilic, Slobodan and Navab, Nassir},
  year = {2016},
  month = jul,
  abstract = {We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.},
  archivePrefix = {arXiv},
  eprint = {1607.06038},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose-Kehl et al-2016.pdf;/Users/sunjiaming/Zotero/storage/6PSCUZQ9/1607.html},
  journal = {arXiv:1607.06038 [cs]},
  primaryClass = {cs}
}

@article{kendallEndtoEndLearningGeometry2017,
  title = {End-to-{{End Learning}} of {{Geometry}} and {{Context}} for {{Deep Stereo Regression}}},
  author = {Kendall, Alex and Martirosyan, Hayk and Dasgupta, Saumitro and Henry, Peter and Kennedy, Ryan and Bachrach, Abraham and Bry, Adam},
  year = {2017},
  month = mar,
  abstract = {We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new stateof-the-art benchmark, while being significantly faster than competing approaches.},
  archivePrefix = {arXiv},
  eprint = {1703.04309},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/End-to-End Learning of Geometry and Context for Deep Stereo Regression-Kendall et al-2017.pdf},
  journal = {arXiv:1703.04309 [cs]},
  keywords = {disparity},
  language = {en},
  primaryClass = {cs}
}

@article{keshavarziSceneGenGenerativeContextual2020,
  title = {{{SceneGen}}: {{Generative Contextual Scene Augmentation}} Using {{Scene Graph Priors}}},
  shorttitle = {{{SceneGen}}},
  author = {Keshavarzi, Mohammad and Parikh, Aakash and Zhai, Xiyu and Mao, Melody and Caldas, Luisa and Yang, Allen},
  year = {2020},
  month = sep,
  abstract = {Spatial computing experiences are constrained by the real-world surroundings of the user. In such experiences, augmenting virtual objects to existing scenes require a contextual approach, where geometrical conflicts are avoided, and functional and plausible relationships to other objects are maintained in the target environment. Yet, due to the complexity and diversity of user environments, automatically calculating ideal positions of virtual content that is adaptive to the context of the scene is considered a challenging task. Motivated by this problem, in this paper we introduce SceneGen, a generative contextual augmentation framework that predicts virtual object positions and orientations within existing scenes. SceneGen takes a semantically segmented scene as input, and outputs positional and orientational probability maps for placing virtual content. We formulate a novel spatial Scene Graph representation, which encapsulates explicit topological properties between objects, object groups, and the room. We believe providing explicit and intuitive features plays an important role in informative content creation and user interaction of spatial computing settings, a quality that is not captured in implicit models. We use kernel density estimation (KDE) to build a multivariate conditional knowledge model trained using prior spatial Scene Graphs extracted from real-world 3D scanned data. To further capture orientational properties, we develop a fast pose annotation tool to extend current real-world datasets with orientational labels. Finally, to demonstrate our system in action, we develop an Augmented Reality application, in which objects can be contextually augmented in real-time.},
  archivePrefix = {arXiv},
  eprint = {2009.12395},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/PF36MJNQ/Keshavarzi et al. - 2020 - SceneGen Generative Contextual Scene Augmentation.pdf;/Users/sunjiaming/Zotero/storage/XVMNLKDW/2009.html},
  journal = {arXiv:2009.12395 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{khotLearningUnsupervisedMultiView,
  title = {Learning {{Unsupervised Multi}}-{{View Stereopsis}} via {{Robust Photometric Consistency}}},
  author = {Khot, Tejas and Agrawal, Shubham and Tulsiani, Shubham and Mertz, Christoph and Lucey, Simon and Hebert, Martial},
  pages = {16},
  abstract = {We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning.},
  file = {/Users/sunjiaming/Zotero/storage/859L7JZW/Khot et al. - Learning Unsupervised Multi-View Stereopsis via Ro.pdf},
  language = {en}
}

@inproceedings{khouryLearningCompactGeometric2017,
  title = {Learning {{Compact Geometric Features}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Khoury, Marc and Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2017},
  month = oct,
  pages = {153--161},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.26},
  abstract = {We present an approach to learning features that represent the local geometry around a point in an unstructured point cloud. Such features play a central role in geometric registration, which supports diverse applications in robotics and 3D vision. Current state-of-the-art local features for unstructured point clouds have been manually crafted and none combines the desirable properties of precision, compactness, and robustness. We show that features with these properties can be learned from data, by optimizing deep networks that map high-dimensional histograms into low-dimensional Euclidean spaces. The presented approach yields a family of features, parameterized by dimension, that are both more compact and more accurate than existing descriptors.},
  file = {/Users/sunjiaming/Zotero/storage/MX9654H8/Khoury et al. - 2017 - Learning Compact Geometric Features.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@article{kimDeepMetricLearning2019,
  title = {Deep {{Metric Learning Beyond Binary Supervision}}},
  author = {Kim, Sungyeon and Seo, Minkyo and Laptev, Ivan and Cho, Minsu and Kwak, Suha},
  year = {2019},
  month = apr,
  abstract = {Metric Learning for visual similarity has mostly adopted binary supervision indicating whether a pair of images are of the same class or not. Such a binary indicator covers only a limited subset of image relations, and is not sufficient to represent semantic similarity between images described by continuous and/or structured labels such as object poses, image captions, and scene graphs. Motivated by this, we present a novel method for deep metric learning using continuous labels. First, we propose a new triplet loss that allows distance ratios in the label space to be preserved in the learned metric space. The proposed loss thus enables our model to learn the degree of similarity rather than just the order. Furthermore, we design a triplet mining strategy adapted to metric learning with continuous labels. We address three different image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions, and demonstrate the superior performance of our approach compared to previous methods.},
  archivePrefix = {arXiv},
  eprint = {1904.09626},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Metric Learning Beyond Binary Supervision-Kim et al-2019.pdf;/Users/sunjiaming/Zotero/storage/DVB84VNE/1904.html},
  journal = {arXiv:1904.09626 [cs]},
  primaryClass = {cs}
}

@article{kimPedXBenchmarkDataset2018,
  title = {{{PedX}}: {{Benchmark Dataset}} for {{Metric 3D Pose Estimation}} of {{Pedestrians}} in {{Complex Urban Intersections}}},
  shorttitle = {{{PedX}}},
  author = {Kim, Wonhui and Ramanagopal, Manikandasriram Srinivasan and Barto, Charles and Yu, Ming-Yuan and Rosaen, Karl and Goumas, Nick and Vasudevan, Ram and {Johnson-Roberson}, Matthew},
  year = {2018},
  month = sep,
  abstract = {This paper presents a novel dataset titled PedX, a large-scale multimodal collection of pedestrians at complex urban intersections. PedX consists of more than 5,000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians. We also present a novel 3D model fitting algorithm for automatic 3D labeling harnessing constraints across different modalities and novel shape and temporal priors. All annotated 3D pedestrians are localized into the real-world metric space, and the generated 3D models are validated using a mocap system configured in a controlled outdoor environment to simulate pedestrians in urban intersections. We also show that the manual 2D labels can be replaced by state-of-the-art automated labeling approaches, thereby facilitating automatic generation of large scale datasets.},
  archivePrefix = {arXiv},
  eprint = {1809.03605},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PedX-Kim et al-2018.pdf;/Users/sunjiaming/Zotero/storage/TQ4CTG6F/1809.html},
  journal = {arXiv:1809.03605 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{kimRecurrentTransformerNetworks2018,
  title = {Recurrent {{Transformer Networks}} for {{Semantic Correspondence}}},
  author = {Kim, Seungryong and Lin, Stephen and Jeon, Sangryul and Min, Dongbo and Sohn, Kwanghoon},
  year = {2018},
  month = oct,
  abstract = {We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.},
  archivePrefix = {arXiv},
  eprint = {1810.12155},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Recurrent Transformer Networks for Semantic Correspondence-Kim et al-2018.pdf},
  journal = {arXiv:1810.12155 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{kimVideoPanopticSegmentation2020,
  title = {Video {{Panoptic Segmentation}}},
  author = {Kim, Dahun and Woo, Sanghyun and Lee, Joon-Young and Kweon, In So},
  year = {2020},
  month = jun,
  abstract = {Panoptic segmentation has become a new standard of visual recognition task by unifying previous semantic segmentation and instance segmentation tasks in concert. In this paper, we propose and explore a new video extension of this task, called video panoptic segmentation. The task requires generating consistent panoptic segmentation as well as an association of instance ids across video frames. To invigorate research on this new task, we present two types of video panoptic datasets. The first is a re-organization of the synthetic VIPER dataset into the video panoptic format to exploit its large-scale pixel annotations. The second is a temporal extension on the Cityscapes val. set, by providing new video panoptic annotations (Cityscapes-VPS). Moreover, we propose a novel video panoptic segmentation network (VPSNet) which jointly predicts object classes, bounding boxes, masks, instance id tracking, and semantic segmentation in video frames. To provide appropriate metrics for this task, we propose a video panoptic quality (VPQ) metric and evaluate our method and several other baselines. Experimental results demonstrate the effectiveness of the presented two datasets. We achieve state-of-the-art results in image PQ on Cityscapes and also in VPQ on Cityscapes-VPS and VIPER datasets. The datasets and code are made publicly available.},
  archivePrefix = {arXiv},
  eprint = {2006.11339},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Video Panoptic Segmentation-Kim et al-2020.pdf;/Users/sunjiaming/Zotero/storage/7FZRSLN9/2006.html},
  journal = {arXiv:2006.11339 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{kirillovPanopticFeaturePyramid2019,
  title = {Panoptic {{Feature Pyramid Networks}}},
  author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2019},
  month = jan,
  abstract = {The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-ofthe-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, topperforming method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.},
  archivePrefix = {arXiv},
  eprint = {1901.02446},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Panoptic Feature Pyramid Networks-Kirillov et al-2019.pdf},
  journal = {arXiv:1901.02446 [cs]},
  keywords = {2d segmentation},
  language = {en},
  primaryClass = {cs}
}

@article{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  month = feb,
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archivePrefix = {arXiv},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reformer-Kitaev et al-2020.pdf;/Users/sunjiaming/Zotero/storage/IBENFD2B/2001.html},
  journal = {arXiv:2001.04451 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{klingensmithChiselRealTime2015,
  title = {Chisel: {{Real Time Large Scale 3D Reconstruction Onboard}} a {{Mobile Device}} Using {{Spatially Hashed Signed Distance Fields}}},
  shorttitle = {Chisel},
  booktitle = {Robotics: {{Science}} and {{Systems XI}}},
  author = {Klingensmith, Matthew and Dryanovski, Ivan and Srinivasa, Siddhartha and Xiao, Jizhong},
  year = {2015},
  month = jul,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2015.XI.040},
  abstract = {We describe CHISEL: a system for real-time housescale (300 square meter or more) dense 3D reconstruction onboard a Google Tango [1] mobile device by using a dynamic spatially-hashed truncated signed distance field[2] for mapping, and visual-inertial odometry for localization. By aggressively culling parts of the scene that do not contain surfaces, we avoid needless computation and wasted memory. Even under very noisy conditions, we produce high-quality reconstructions through the use of space carving. We are able to reconstruct and render very large scenes at a resolution of 2-3 cm in real time on a mobile device without the use of GPU computing. The user is able to view and interact with the reconstruction in real-time through an intuitive interface. We provide both qualitative and quantitative results on publicly available RGB-D datasets [3], and on datasets collected in real-time from two devices.},
  file = {/Users/sunjiaming/Zotero/storage/IUCT73F9/Klingensmith et al. - 2015 - Chisel Real Time Large Scale 3D Reconstruction On.pdf},
  isbn = {978-0-9923747-1-6},
  language = {en}
}

@article{klokovEscapeCellsDeep2017,
  title = {Escape from {{Cells}}: {{Deep Kd}}-{{Networks}} for the {{Recognition}} of {{3D Point Cloud Models}}},
  shorttitle = {Escape from {{Cells}}},
  author = {Klokov, Roman and Lempitsky, Victor},
  year = {2017},
  month = apr,
  abstract = {We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.},
  archivePrefix = {arXiv},
  eprint = {1704.01222},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Escape from Cells-Klokov_Lempitsky-2017.pdf},
  journal = {arXiv:1704.01222 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{klugerCONSACRobustMultiModel2020,
  title = {{{CONSAC}}: {{Robust Multi}}-{{Model Fitting}} by {{Conditional Sample Consensus}}},
  shorttitle = {{{CONSAC}}},
  author = {Kluger, Florian and Brachmann, Eric and Ackermann, Hanno and Rother, Carsten and Yang, Michael Ying and Rosenhahn, Bodo},
  year = {2020},
  month = jan,
  abstract = {We present a robust estimator for fitting multiple parametric models of the same form to noisy measurements. Applications include finding multiple vanishing points in man-made scenes, fitting planes to architectural imagery, or estimating multiple rigid motions within the same sequence. In contrast to previous works, which resorted to hand-crafted search strategies for multiple model detection, we learn the search strategy from data. A neural network conditioned on previously detected models guides a RANSAC estimator to different subsets of all measurements, thereby finding model instances one after another. We train our method supervised as well as self-supervised. For supervised training of the search strategy, we contribute a new dataset for vanishing point estimation. Leveraging this dataset, the proposed algorithm is superior with respect to other robust estimators as well as to designated vanishing point estimation algorithms. For self-supervised learning of the search, we evaluate the proposed algorithm on multi-homography estimation and demonstrate an accuracy that is superior to state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {2001.02643},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CONSAC-Kluger et al-2020.pdf;/Users/sunjiaming/Zotero/storage/X4R5NBUB/2001.html},
  journal = {arXiv:2001.02643 [cs]},
  primaryClass = {cs}
}

@article{knapitschTanksTemplesBenchmarking2017,
  title = {Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction},
  shorttitle = {Tanks and Temples},
  author = {Knapitsch, Arno and Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2017},
  month = jul,
  volume = {36},
  pages = {1--13},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3072959.3073599},
  abstract = {We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.},
  file = {/Users/sunjiaming/Zotero/storage/D2ZUYWEQ/Knapitsch et al. - 2017 - Tanks and temples benchmarking large-scale scene .pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {neufu_paper},
  language = {en},
  number = {4}
}

@article{kochABCBigCAD2018,
  title = {{{ABC}}: {{A Big CAD Model Dataset For Geometric Deep Learning}}},
  shorttitle = {{{ABC}}},
  author = {Koch, Sebastian and Matveev, Albert and Jiang, Zhongshi and Williams, Francis and Artemov, Alexey and Burnaev, Evgeny and Alexa, Marc and Zorin, Denis and Panozzo, Daniele},
  year = {2018},
  month = dec,
  abstract = {We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.},
  archivePrefix = {arXiv},
  eprint = {1812.06216},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ABC-Koch et al-2018.pdf;/Users/sunjiaming/Zotero/storage/U8NZI8QG/1812.html},
  journal = {arXiv:1812.06216 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{koestlerLearningMonocular3D2020,
  title = {Learning {{Monocular 3D Vehicle Detection}} without {{3D Bounding Box Labels}}},
  author = {Koestler, L. and Yang, N. and Wang, R. and Cremers, D.},
  year = {2020},
  month = oct,
  abstract = {The training of deep-learning-based 3D object detectors requires large datasets with 3D bounding box labels for supervision that have to be generated by hand-labeling. We propose a network architecture and training procedure for learning monocular 3D object detection without 3D bounding box labels. By representing the objects as triangular meshes and employing differentiable shape rendering, we define loss functions based on depth maps, segmentation masks, and ego- and object-motion, which are generated by pre-trained, off-the-shelf networks. We evaluate the proposed algorithm on the real-world KITTI dataset and achieve promising performance in comparison to state-of-the-art methods requiring 3D bounding box labels for training and superior performance to conventional baseline methods.},
  archivePrefix = {arXiv},
  eprint = {2010.03506},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/3RDJ6NUM/Koestler et al. - 2020 - Learning Monocular 3D Vehicle Detection without 3D.pdf;/Users/sunjiaming/Zotero/storage/NZFRLPAQ/2010.html},
  journal = {arXiv:2010.03506 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{kohliInferringSemanticInformation2020,
  title = {Inferring {{Semantic Information}} with {{3D Neural Scene Representations}}},
  author = {Kohli, Amit and Sitzmann, Vincent and Wetzstein, Gordon},
  year = {2020},
  month = mar,
  abstract = {Biological vision infers multi-modal 3D representations that support reasoning about scene properties such as materials, appearance, affordance, and semantics in 3D. These rich representations enable us humans, for example, to acquire new skills, such as the learning of a new semantic class, with extremely limited supervision. Motivated by this ability of biological vision, we demonstrate that 3D-structure-aware representation learning leads to multi-modal representations that enable 3D semantic segmentation with extremely limited, 2D-only supervision. Building on emerging neural scene representations, which have been developed for modeling the shape and appearance of 3D scenes supervised exclusively by posed 2D images, we are first to demonstrate a representation that jointly encodes shape, appearance, and semantics in a 3D-structure-aware manner. Surprisingly, we find that only a few tens of labeled 2D segmentation masks are required to achieve dense 3D semantic segmentation using a semi-supervised learning strategy. We explore two novel applications for our semantically aware neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.},
  archivePrefix = {arXiv},
  eprint = {2003.12673},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Inferring Semantic Information with 3D Neural Scene Representations-Kohli et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MA6IKNAK/2003.html},
  journal = {arXiv:2003.12673 [cs]},
  primaryClass = {cs}
}

@article{kolesnikovRevisitingSelfSupervisedVisual2019,
  title = {Revisiting {{Self}}-{{Supervised Visual Representation Learning}}},
  author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
  year = {2019},
  month = jan,
  abstract = {Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.},
  archivePrefix = {arXiv},
  eprint = {1901.09005},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Revisiting Self-Supervised Visual Representation Learning-Kolesnikov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/D9UKNB99/1901.html},
  journal = {arXiv:1901.09005 [cs]},
  keywords = {feature learning},
  primaryClass = {cs}
}

@inproceedings{kolevTurningMobilePhones2014,
  title = {Turning {{Mobile Phones}} into {{3D Scanners}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kolev, Kalin and Tanskanen, Petri and Speciale, Pablo and Pollefeys, Marc},
  year = {2014},
  month = jun,
  pages = {3946--3953},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.504},
  abstract = {In this paper, we propose an efficient and accurate scheme for the integration of multiple stereo-based depth measurements. For each provided depth map a confidencebased weight is assigned to each depth estimate by evaluating local geometry orientation, underlying camera setting and photometric evidence. Subsequently, all hypotheses are fused together into a compact and consistent 3D model. Thereby, visibility conflicts are identified and resolved, and fitting measurements are averaged with regard to their confidence scores. The individual stages of the proposed approach are validated by comparing it to two alternative techniques which rely on a conceptually different fusion scheme and a different confidence inference, respectively. Pursuing live 3D reconstruction on mobile devices as a primary goal, we demonstrate that the developed method can easily be integrated into a system for monocular interactive 3D modeling by substantially improving its accuracy while adding a negligible overhead to its performance and retaining its interactive potential.},
  file = {/Users/sunjiaming/Zotero/storage/HTK5UT2T/Kolev et al. - 2014 - Turning Mobile Phones into 3D Scanners.pdf},
  isbn = {978-1-4799-5118-5},
  keywords = {neufu_paper},
  language = {en}
}

@article{kolotourosLearningReconstruct3D2019,
  title = {Learning to {{Reconstruct 3D Human Pose}} and {{Shape}} via {{Model}}-Fitting in the {{Loop}}},
  author = {Kolotouros, Nikos and Pavlakos, Georgios and Black, Michael J. and Daniilidis, Kostas},
  year = {2019},
  month = sep,
  abstract = {Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/\textasciitilde nkolot/projects/spin.},
  archivePrefix = {arXiv},
  eprint = {1909.12828},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop-Kolotouros et al-2019.pdf;/Users/sunjiaming/Zotero/storage/IGZEFNRY/1909.html},
  journal = {arXiv:1909.12828 [cs]},
  primaryClass = {cs}
}

@article{kosiorekStackedCapsuleAutoencoders2019,
  ids = {kosiorekStackedCapsuleAutoencoders2019a},
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  year = {2019},
  month = jun,
  abstract = {An object can be seen as a geometrically organized set of interrelated parts. A system that makes explicit use of these geometric relationships to recognize objects should be naturally robust to changes in viewpoint, because the intrinsic geometric relationships are viewpoint-invariant. We describe an unsupervised version of capsule networks, in which a neural encoder, which looks at all of the parts, is used to infer the presence and poses of object capsules. The encoder is trained by backpropagating through a decoder, which predicts the pose of each already discovered part using a mixture of pose predictions. The parts are discovered directly from an image, in a similar manner, by using a neural encoder, which infers parts and their affine transformations. The corresponding decoder models each image pixel as a mixture of predictions made by affine-transformed parts. We learn object- and their part-capsules on unlabeled data, and then cluster the vectors of presences of object capsules. When told the names of these clusters, we achieve state-of-the-art results for unsupervised classification on SVHN (55\%) and near state-of-the-art on MNIST (98.5\%).},
  archivePrefix = {arXiv},
  eprint = {1906.06818},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Stacked Capsule Autoencoders-Kosiorek et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Stacked Capsule Autoencoders-Kosiorek et al-22.pdf;/Users/sunjiaming/Zotero/storage/962DT9BB/1906.html;/Users/sunjiaming/Zotero/storage/EQZZM5Y9/1906.html},
  journal = {arXiv:1906.06818 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{krahenbuhlEfficientInferenceFully,
  title = {Efficient {{Inference}} in {{Fully Connected CRFs}} with {{Gaussian Edge Potentials}}},
  author = {Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen},
  pages = {9},
  abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials-Krähenbühl_Koltun-.pdf},
  language = {en}
}

@inproceedings{krahenbuhlFreeSupervisionVideo2018a,
  title = {Free {{Supervision}} from {{Video Games}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Krahenbuhl, Philipp},
  year = {2018},
  month = jun,
  pages = {2955--2964},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00312},
  abstract = {Deep networks are extremely hungry for data. They devour hundreds of thousands of labeled images to learn robust and semantically meaningful feature representations. Current networks are so data hungry that collecting labeled data has become as important as designing the networks themselves. Unfortunately, manual data collection is both expensive and time consuming. We present an alternative, and show how ground truth labels for many vision tasks are easily extracted from video games in real time as we play them. We interface the popular Microsoft R DirectX R rendering API, and inject specialized rendering code into the game as it is running. This code produces ground truth labels for instance segmentation, semantic labeling, depth estimation, optical flow, intrinsic image decomposition, and instance tracking. Instead of labeling images, a researcher now simply plays video games all day long. Our method is general and works on a wide range of video games. We collected a dataset of 220k training images, and 60k test images across 3 video games, and evaluate state of the art optical flow, depth estimation and intrinsic image decomposition algorithms. Our video game data is visually closer to real world images, than other synthetic dataset.},
  file = {/Users/sunjiaming/Zotero/storage/SG4IDE6S/Krahenbuhl - 2018 - Free Supervision from Video Games.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{krishnamoorthiQuantizingDeepConvolutional2018,
  title = {Quantizing Deep Convolutional Networks for Efficient Inference: {{A}} Whitepaper},
  shorttitle = {Quantizing Deep Convolutional Networks for Efficient Inference},
  author = {Krishnamoorthi, Raghuraman},
  year = {2018},
  month = jun,
  abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
  archivePrefix = {arXiv},
  eprint = {1806.08342},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/J4ZIV7ZM/Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf;/Users/sunjiaming/Zotero/storage/I8KZNYFM/1806.html},
  journal = {arXiv:1806.08342 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{krishnamurthiProgrammingLanguagesApplication,
  title = {Programming {{Languages}}: {{Application}} and {{Interpretation}}},
  author = {Krishnamurthi, Shriram},
  pages = {207},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Programming Languages-Krishnamurthi-.pdf},
  language = {en}
}

@article{kuhnDeepCMVSDeepConfidence2019,
  title = {{{DeepC}}-{{MVS}}: {{Deep Confidence Prediction}} for {{Multi}}-{{View Stereo Reconstruction}}},
  shorttitle = {{{DeepC}}-{{MVS}}},
  author = {Kuhn, Andreas and Sormann, Christian and Rossi, Mattia and Erdler, Oliver and Fraundorfer, Friedrich},
  year = {2019},
  month = dec,
  abstract = {Deep Neural Networks (DNNs) have the potential to improve the quality of image-based 3D reconstructions. A challenge which still remains is to utilize the potential of DNNs to improve 3D reconstructions from high-resolution image datasets as available by the ETH3D benchmark. In this paper, we propose a way to employ DNNs in the image domain to gain a significant quality improvement of geometric image based 3D reconstruction. This is achieved by utilizing confidence prediction networks which have been adapted to the Multi-View Stereo (MVS) case and are trained on automatically generated ground truth established by geometric error propagation. In addition to a semi-dense real-world ground truth dataset for training the DNN, we present a synthetic dataset to enlarge the training dataset. We demonstrate the utility of the confidence predictions for two essential steps within a 3D reconstruction pipeline: Firstly, to be used for outlier clustering and filtering and secondly to be used within a depth refinement step. The presented 3D reconstruction pipeline DeepC-MVS makes use of deep learning for an essential part in MVS from high-resolution images and the experimental evaluation on popular benchmarks demonstrates the achieved state-of-the-art quality in 3D reconstruction.},
  archivePrefix = {arXiv},
  eprint = {1912.00439},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Kuhn et al_2019_DeepC-MVS.pdf;/Users/sunjiaming/Zotero/storage/3K4AM4MB/1912.html},
  journal = {arXiv:1912.00439 [cs]},
  primaryClass = {cs}
}

@article{kuhnHungarianMethodAssignment1955,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  year = {1955},
  month = mar,
  volume = {2},
  pages = {83--97},
  issn = {00281441, 19319193},
  doi = {10.1002/nav.3800020109},
  file = {/Users/sunjiaming/Zotero/storage/8LIJ8FHA/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf},
  journal = {Naval Research Logistics Quarterly},
  language = {en},
  number = {1-2}
}

@article{kuJoint3DProposal2017,
  title = {Joint {{3D Proposal Generation}} and {{Object Detection}} from {{View Aggregation}}},
  author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
  year = {2017},
  month = dec,
  abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
  archivePrefix = {arXiv},
  eprint = {1712.02294},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Joint 3D Proposal Generation and Object Detection from View Aggregation-Ku et al-2017.pdf},
  journal = {arXiv:1712.02294 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{kulkarniCanonicalSurfaceMapping2019,
  title = {Canonical {{Surface Mapping}} via {{Geometric Cycle Consistency}}},
  author = {Kulkarni, Nilesh and Gupta, Abhinav and Tulsiani, Shubham},
  year = {2019},
  month = jul,
  abstract = {We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.},
  archivePrefix = {arXiv},
  eprint = {1907.10043},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Canonical Surface Mapping via Geometric Cycle Consistency-Kulkarni et al-2019.pdf;/Users/sunjiaming/Zotero/storage/IJXAF5DJ/1907.html},
  journal = {arXiv:1907.10043 [cs]},
  primaryClass = {cs}
}

@article{kulkarniUnsupervisedLearningObject2019,
  title = {Unsupervised {{Learning}} of {{Object Keypoints}} for {{Perception}} and {{Control}}},
  author = {Kulkarni, Tejas and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
  year = {2019},
  month = jun,
  abstract = {The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains -- (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.},
  archivePrefix = {arXiv},
  eprint = {1906.11883},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Learning of Object Keypoints for Perception and Control-Kulkarni et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2965AFFT/1906.html},
  journal = {arXiv:1906.11883 [cs]},
  primaryClass = {cs}
}

@inproceedings{kummerleG2oGeneralFramework2011,
  title = {G{\textsuperscript{2}}o: {{A}} General Framework for Graph Optimization},
  shorttitle = {G{\textsuperscript{2}}o},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kummerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
  year = {2011},
  month = may,
  pages = {3607--3613},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/ICRA.2011.5979949},
  abstract = {Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of stateof-the-art approaches for the specific problems.},
  file = {/Users/sunjiaming/Zotero/storage/WLXQETLR/Kummerle et al. - 2011 - Gsup2supo A general framework for graph opti.pdf},
  isbn = {978-1-61284-386-5},
  language = {en}
}

@article{kuMonocular3DObject2019,
  title = {Monocular {{3D Object Detection Leveraging Accurate Proposals}} and {{Shape Reconstruction}}},
  author = {Ku, Jason and Pon, Alex D. and Waslander, Steven L.},
  year = {2019},
  month = apr,
  abstract = {We present MonoPSR, a monocular 3D object detection method that leverages proposals and shape reconstruction. First, using the fundamental relations of a pinhole camera model, detections from a mature 2D object detector are used to generate a 3D proposal per object in a scene. The 3D location of these proposals prove to be quite accurate, which greatly reduces the difficulty of regressing the final 3D bounding box detection. Simultaneously, a point cloud is predicted in an object centered coordinate system to learn local scale and shape information. However, the key challenge is how to exploit shape information to guide 3D localization. As such, we devise aggregate losses, including a novel projection alignment loss, to jointly optimize these tasks in the neural network to improve 3D localization accuracy. We validate our method on the KITTI benchmark where we set new state-of-the-art results among published monocular methods, including the harder pedestrian and cyclist classes, while maintaining efficient run-time.},
  archivePrefix = {arXiv},
  eprint = {1904.01690},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Monocular 3D Object Detection Leveraging Accurate Proposals and Shape-Ku et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YAJNEJIF/1904.html},
  journal = {arXiv:1904.01690 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@inproceedings{kundu3DRCNNInstanceLevel3D2018,
  title = {{{3D}}-{{RCNN}}: {{Instance}}-{{Level 3D Object Reconstruction}} via {{Render}}-and-{{Compare}}},
  shorttitle = {{{3D}}-{{RCNN}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kundu, Abhijit and Li, Yin and Rehg, James M.},
  year = {2018},
  month = jun,
  pages = {3559--3568},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00375},
  abstract = {We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D-RCNN-Kundu et al-2018.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{kunduSelfSupervised3DHuman2020,
  title = {Self-{{Supervised 3D Human Pose Estimation}} via {{Part Guided Novel Image Synthesis}}},
  author = {Kundu, Jogendra Nath and Seth, Siddharth and Jampani, Varun and Rakesh, Mugalodi and Babu, R. Venkatesh and Chakraborty, Anirban},
  year = {2020},
  month = apr,
  abstract = {Camera captured human pose is an outcome of several sources of variation. Performance of supervised 3D pose estimation approaches comes at the cost of dispensing with variations, such as shape and appearance, that may be useful for solving other related tasks. As a result, the learned model not only inculcates task-bias but also dataset-bias because of its strong reliance on the annotated samples, which also holds true for weakly-supervised models. Acknowledging this, we propose a self-supervised learning framework to disentangle such variations from unlabeled video frames. We leverage the prior knowledge on human skeleton and poses in the form of a single part-based 2D puppet model, human pose articulation constraints, and a set of unpaired 3D poses. Our differentiable formalization, bridging the representation gap between the 3D pose and spatial part maps, not only facilitates discovery of interpretable pose disentanglement but also allows us to operate on videos with diverse camera movements. Qualitative results on unseen in-the-wild datasets establish our superior generalization across multiple tasks beyond the primary tasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate state-of-the-art weakly-supervised 3D pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets.},
  archivePrefix = {arXiv},
  eprint = {2004.04400},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis-Kundu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9BAC7RR4/2004.html},
  journal = {arXiv:2004.04400 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{labatutRobustEfficientSurface2009,
  title = {Robust and {{Efficient Surface Reconstruction From Range Data}}},
  author = {Labatut, P. and Pons, J.-P. and Keriven, R.},
  year = {2009},
  month = dec,
  volume = {28},
  pages = {2275--2290},
  issn = {01677055, 14678659},
  doi = {10.1111/j.1467-8659.2009.01530.x},
  abstract = {We describe a robust but simple algorithm to reconstruct a surface from a set of merged range scans. Our key contribution is the formulation of the surface reconstruction problem as an energy minimisation problem that explicitly models the scanning process. The adaptivity of the Delaunay triangulation is exploited by restricting the energy to inside/outside labelings of Delaunay tetrahedra. Our energy measures both the output surface quality and how well the surface agrees with soft visibility constraints. Such energy is shown to perfectly fit into the minimum s-t cuts optimisation framework, allowing fast computation of a globally optimal tetrahedra labeling, while avoiding the ``shrinking bias'' that usually plagues graph cuts methods.},
  file = {/Users/sunjiaming/Zotero/storage/2AMH8E3W/Labatut et al. - 2009 - Robust and Efficient Surface Reconstruction From R.pdf},
  journal = {Computer Graphics Forum},
  keywords = {neufu_paper},
  language = {en},
  number = {8}
}

@article{labbeCosyPoseConsistentMultiview2020,
  title = {{{CosyPose}}: {{Consistent}} Multi-View Multi-Object {{6D}} Pose Estimation},
  shorttitle = {{{CosyPose}}},
  author = {Labb{\'e}, Yann and Carpentier, Justin and Aubry, Mathieu and Sivic, Josef},
  year = {2020},
  month = aug,
  abstract = {We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. First, we present a single-view single-object 6D pose estimation method, which we use to generate 6D object pose hypotheses. Second, we develop a robust method for matching individual 6D object pose hypotheses across different input images in order to jointly estimate camera viewpoints and 6D poses of all objects in a single consistent scene. Our approach explicitly handles object symmetries, does not require depth measurements, is robust to missing or incorrect object hypotheses, and automatically recovers the number of objects in the scene. Third, we develop a method for global scene refinement given multiple object hypotheses and their correspondences across views. This is achieved by solving an object-level bundle adjustment problem that refines the poses of cameras and objects to minimize the reprojection error in all views. We demonstrate that the proposed method, dubbed CosyPose, outperforms current state-of-the-art results for single-view and multi-view 6D object pose estimation by a large margin on two challenging benchmarks: the YCB-Video and T-LESS datasets. Code and pre-trained models are available on the project webpage https://www.di.ens.fr/willow/research/cosypose/.},
  archivePrefix = {arXiv},
  eprint = {2008.08465},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CosyPose-Labbé et al-2020.pdf;/Users/sunjiaming/Zotero/storage/JB3XN4KG/2008.html},
  journal = {arXiv:2008.08465 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{lagunaKeyNetKeypoint2019,
  title = {Key.{{Net}}: {{Keypoint Detection}} by {{Handcrafted}} and {{Learned CNN Filters}}},
  shorttitle = {Key.{{Net}}},
  author = {Laguna, Axel Barroso and Riba, Edgar and Ponsa, Daniel and Mikolajczyk, Krystian},
  year = {2019},
  month = apr,
  abstract = {We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Key-Laguna et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RNF4SWIL/1904.html},
  language = {en}
}

@article{laiBridgingStereoMatching2019,
  title = {Bridging {{Stereo Matching}} and {{Optical Flow}} via {{Spatiotemporal Correspondence}}},
  author = {Lai, Hsueh-Ying and Tsai, Yi-Hsuan and Chiu, Wei-Chen},
  year = {2019},
  month = may,
  abstract = {Stereo matching and flow estimation are two essential tasks for scene understanding, spatially in 3D and temporally in motion. Existing approaches have been focused on the unsupervised setting due to the limited resource to obtain the large-scale ground truth data. To construct a self-learnable objective, co-related tasks are often linked together to form a joint framework. However, the prior work usually utilizes independent networks for each task, thus not allowing to learn shared feature representations across models. In this paper, we propose a single and principled network to jointly learn spatiotemporal correspondence for stereo matching and flow estimation, with a newly designed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. We show that our method performs favorably against several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.},
  archivePrefix = {arXiv},
  eprint = {1905.09265},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence-Lai et al-2019.pdf;/Users/sunjiaming/Zotero/storage/UU657N8D/1905.html},
  journal = {arXiv:1905.09265 [cs]},
  primaryClass = {cs}
}

@inproceedings{laidlowDenseRGBDinertialSLAM2017,
  title = {Dense {{RGB}}-{{D}}-Inertial {{SLAM}} with Map Deformations},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Laidlow, Tristan and Bloesch, Michael and Li, Wenbin and Leutenegger, Stefan},
  year = {2017},
  month = sep,
  pages = {6741--6748},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  doi = {10.1109/IROS.2017.8206591},
  abstract = {While dense visual SLAM methods are capable of estimating dense reconstructions of the environment, they suffer from a lack of robustness in their tracking step, especially when the optimisation is poorly initialised. Sparse visual SLAM systems have attained high levels of accuracy and robustness through the inclusion of inertial measurements in a tightlycoupled fusion. Inspired by this performance, we propose the first tightly-coupled dense RGB-D-inertial SLAM system.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dense RGB-D-inertial SLAM with map deformations-Laidlow et al-2017.pdf},
  isbn = {978-1-5386-2682-5},
  language = {en}
}

@article{laidlowProbabilisticFusionLearned,
  title = {Towards the {{Probabilistic Fusion}} of {{Learned Priors}} into {{Standard Pipelines}} for {{3D Reconstruction}}},
  author = {Laidlow, Tristan and Czarnowski, Jan and Nicastro, Andrea and Clark, Ronald and Leutenegger, Stefan},
  pages = {7},
  abstract = {The best way to combine the results of deep learning with standard 3D reconstruction pipelines remains an open problem. While systems that pass the output of traditional multi-view stereo approaches to a network for regularisation or refinement currently seem to get the best results, it may be preferable to treat deep neural networks as separate components whose results can be probabilistically fused into geometrybased systems. Unfortunately, the error models required to do this type of fusion are not well understood, with many different approaches being put forward. Recently, a few systems have achieved good results by having their networks predict probability distributions rather than single values. We propose using this approach to fuse a learned single-view depth prior into a standard 3D reconstruction system.},
  file = {/Users/sunjiaming/Zotero/storage/MD2RWSDZ/Laidlow et al. - Towards the Probabilistic Fusion of Learned Priors.pdf},
  language = {en}
}

@article{laiLargeScaleHierarchicalMultiView,
  title = {A {{Large}}-{{Scale Hierarchical Multi}}-{{View RGB}}-{{D Object Dataset}}},
  author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  pages = {8},
  abstract = {Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinectstyle) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results.},
  file = {/Users/sunjiaming/Zotero/storage/C5SDYS8J/Lai et al. - A Large-Scale Hierarchical Multi-View RGB-D Object.pdf},
  language = {en}
}

@article{laineModularPrimitivesHighPerformance2020,
  title = {Modular {{Primitives}} for {{High}}-{{Performance Differentiable Rendering}}},
  author = {Laine, Samuli and Hellsten, Janne and Karras, Tero and Seol, Yeongho and Lehtinen, Jaakko and Aila, Timo},
  year = {2020},
  month = nov,
  abstract = {We present a modular differentiable renderer design that yields performance superior to previous methods by leveraging existing, highly optimized hardware graphics pipelines. Our design supports all crucial operations in a modern graphics pipeline: rasterizing large numbers of triangles, attribute interpolation, filtered texture lookups, as well as user-programmable shading and geometry processing, all in high resolutions. Our modular primitives allow custom, high-performance graphics pipelines to be built directly within automatic differentiation frameworks such as PyTorch or TensorFlow. As a motivating application, we formulate facial performance capture as an inverse rendering problem and show that it can be solved efficiently using our tools. Our results indicate that this simple and straightforward approach achieves excellent geometric correspondence between rendered results and reference imagery.},
  archivePrefix = {arXiv},
  eprint = {2011.03277},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Modular Primitives for High-Performance Differentiable Rendering-Laine et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RJSD7Z7F/2011.html},
  journal = {arXiv:2011.03277 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{lampleLargeMemoryLayers2019,
  title = {Large {{Memory Layers}} with {{Product Keys}}},
  author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  year = {2019},
  month = dec,
  abstract = {This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.},
  archivePrefix = {arXiv},
  eprint = {1907.05242},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large Memory Layers with Product Keys-Lample et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LHA6DF82/1907.html},
  journal = {arXiv:1907.05242 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{langloisSurfaceReconstruction3D2019,
  title = {Surface {{Reconstruction}} from {{3D Line Segments}}},
  author = {Langlois, Pierre-Alain and Boulch, Alexandre and Marlet, Renaud},
  year = {2019},
  month = sep,
  pages = {553--563},
  doi = {10.1109/3DV.2019.00067},
  abstract = {In man-made environments such as indoor scenes, when point-based 3D reconstruction fails due to the lack of texture, lines can still be detected and used to support surfaces. We present a novel method for watertight piecewise-planar surface reconstruction from 3D line segments with visibility information. First, planes are extracted by a novel RANSAC approach for line segments that allows multiple shape support. Then, each 3D cell of a plane arrangement is labeled full or empty based on line attachment to planes, visibility and regularization. Experiments show the robustness to sparse input data, noise and outliers.},
  archivePrefix = {arXiv},
  eprint = {1911.00451},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Surface Reconstruction from 3D Line Segments-Langlois et al-2019.pdf;/Users/sunjiaming/Zotero/storage/CD8FZ633/1911.html},
  journal = {2019 International Conference on 3D Vision (3DV)}
}

@article{langPointPillarsFastEncoders2018,
  title = {{{PointPillars}}: {{Fast Encoders}} for {{Object Detection}} from {{Point Clouds}}},
  shorttitle = {{{PointPillars}}},
  author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  year = {2018},
  month = dec,
  abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
  archivePrefix = {arXiv},
  eprint = {1812.05784},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/PointPillars-Lang et al-2018.pdf},
  journal = {arXiv:1812.05784 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{langSampleNetDifferentiablePoint2019,
  title = {{{SampleNet}}: {{Differentiable Point Cloud Sampling}}},
  shorttitle = {{{SampleNet}}},
  author = {Lang, Itai and Manor, Asaf and Avidan, Shai},
  year = {2019},
  month = dec,
  abstract = {There is a growing number of tasks that work directly on point clouds. As the size of the point cloud grows, so do the computational demands of these tasks. A possible solution is to sample the point cloud first. Classic sampling approaches, such as farthest point sampling (FPS), do not consider the downstream task. A recent work showed that learning a task-specific sampling can improve results significantly. However, the proposed technique did not deal with the non-differentiability of the sampling operation and offered a workaround instead. We introduce a novel differentiable relaxation for point cloud sampling. Our approach employs a soft projection operation that approximates sampled points as a mixture of points in the primary input cloud. The approximation is controlled by a temperature parameter and converges to regular sampling when the temperature goes to zero. During training, we use a projection loss that encourages the temperature to drop, thereby driving every sample point to be close to one of the input points. This approximation scheme leads to consistently good results on various applications such as classification, retrieval, and geometric reconstruction. We also show that the proposed sampling network can be used as a front to a point cloud registration network. This is a challenging task since sampling must be consistent across two different point clouds. In all cases, our method works better than existing non-learned and learned sampling alternatives. Our code is publicly available at https://github.com/itailang/SampleNet.},
  archivePrefix = {arXiv},
  eprint = {1912.03663},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SampleNet-Lang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TQ2RGZS6/1912.html},
  journal = {arXiv:1912.03663 [cs]},
  primaryClass = {cs}
}

@article{lanModelingLocalGeometric2018,
  title = {Modeling {{Local Geometric Structure}} of {{3D Point Clouds}} Using {{Geo}}-{{CNN}}},
  author = {Lan, Shiyi and Yu, Ruichi and Yu, Gang and Davis, Larry S.},
  year = {2018},
  month = nov,
  abstract = {Recent advances in deep convolutional neural networks (CNNs) have motivated researchers to adapt CNNs to directly model points in 3D point clouds. Modeling local structure has been proven to be important for the success of convolutional architectures, and researchers exploited the modeling of local point sets in the feature extraction hierarchy. However, limited attention has been paid to explicitly model the geometric structure amongst points in a local region. To address this problem, we propose Geo-CNN, which applies a generic convolution-like operation dubbed as GeoConv to each point and its local neighborhood. Local geometric relationships among points are captured when extracting edge features between the center and its neighboring points. We first decompose the edge feature extraction process onto three orthogonal bases, and then aggregate the extracted features based on the angles between the edge vector and the bases. This encourages the network to preserve the geometric structure in Euclidean space throughout the feature extraction hierarchy. GeoConv is a generic and efficient operation that can be easily integrated into 3D point cloud analysis pipelines for multiple applications. We evaluate Geo-CNN on ModelNet40 and KITTI and achieve state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {1811.07782},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN-Lan et al-2018.pdf;/Users/sunjiaming/Zotero/storage/ZR5ZELH7/1811.html},
  journal = {arXiv:1811.07782 [cs]},
  primaryClass = {cs}
}

@article{lasingerRobustMonocularDepth2019,
  title = {Towards {{Robust Monocular Depth Estimation}}: {{Mixing Datasets}} for {{Zero}}-{{Shot Cross}}-{{Dataset Transfer}}},
  shorttitle = {Towards {{Robust Monocular Depth Estimation}}},
  author = {Lasinger, Katrin and Ranftl, Ren{\'e} and Schindler, Konrad and Koltun, Vladlen},
  year = {2019},
  month = jul,
  abstract = {The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a training objective that is invariant to changes in depth range and scale. Armed with this objective, we explore an abundant source of training data: 3D films. We demonstrate that despite pervasive inaccuracies, 3D films constitute a useful source of data that is complementary to existing training sets. We evaluate the presented approach on diverse datasets, focusing on zero-shot cross-dataset transfer: testing the generality of the learned model by evaluating it on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources yields improved depth estimates, particularly on previously unseen datasets. Some results are shown in the supplementary video: https://youtu.be/ITI0YS6IrUQ},
  archivePrefix = {arXiv},
  eprint = {1907.01341},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Towards Robust Monocular Depth Estimation-Lasinger et al-2019.pdf;/Users/sunjiaming/Zotero/storage/7J8V87NS/1907.html},
  journal = {arXiv:1907.01341 [cs]},
  primaryClass = {cs}
}

@article{lassnerFastDifferentiableRaycasting2020,
  title = {Fast {{Differentiable Raycasting}} for {{Neural Rendering}} Using {{Sphere}}-Based {{Representations}}},
  author = {Lassner, Christoph},
  year = {2020},
  month = apr,
  abstract = {Differentiable rendering in combination with deep learning promises great advantages: deep learning models can produce realistic scenes rapidly, while differentiable rendering offers consistent scene representations and respective gradients. However, gradient based optimization of classical mesh representations is cumbersome because of the explicit topology encoding. Moreover, complex scenes may need detailed geometric representation, requiring many geometric primitives and a fast rendering operation. We propose to break up the rendering process into multiple parts: (1) the scene representation, (2) a differentiable geometry projection and (3) neural shading. While mature, off-the-shelf models for scene representation and neural shading are widely available, we propose pulsar as a general purpose differentiable geometry engine tightly integrated with PyTorch. By replacing mesh representations with sphere clouds for the scene representation, the operation is fast compared to existing differentiable renderers and avoids problems with surface topology. It provides gradients for the full scene parameterization, i.e., sphere positions, colors, radiuses, opacity and the camera parameters. pulsar can execute many times, up to orders of magnitudes faster than existing renderers and allows real-time rendering and optimization of scenes with millions of spheres. It can be used for 3D reconstruction, rendering and volumetric scene optimization.},
  archivePrefix = {arXiv},
  eprint = {2004.07484},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fast Differentiable Raycasting for Neural Rendering using Sphere-based-Lassner-2020.pdf;/Users/sunjiaming/Zotero/storage/MPLD95GM/2004.html},
  journal = {arXiv:2004.07484 [cs]},
  primaryClass = {cs}
}

@article{lawCornerNetDetectingObjects2018,
  title = {{{CornerNet}}: {{Detecting Objects}} as {{Paired Keypoints}}},
  shorttitle = {{{CornerNet}}},
  author = {Law, Hei and Deng, Jia},
  year = {2018},
  month = aug,
  abstract = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.},
  archivePrefix = {arXiv},
  eprint = {1808.01244},
  eprinttype = {arxiv},
  journal = {arXiv:1808.01244 [cs]},
  primaryClass = {cs}
}

@article{lebedaTMAGICModelFree3D2017,
  title = {{{TMAGIC}}: {{A Model}}-{{Free 3D Tracker}}},
  shorttitle = {{{TMAGIC}}},
  author = {Lebeda, Karel and Hadfield, Simon and Bowden, Richard},
  year = {2017},
  month = sep,
  volume = {26},
  pages = {4378--4388},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2017.2675343},
  abstract = {Significant effort has been devoted within the visual tracking community to rapid learning of object properties on the fly. However, state-of-the-art approaches still often fail in cases such as rapid out-of-plane rotation, when the appearance changes suddenly. One of the major contributions of this paper is a radical rethinking of the traditional wisdom of modeling 3D motion as appearance changes during tracking. Instead, 3D motion is modeled as 3D motion. This intuitive but previously unexplored approach provides new possibilities in visual tracking research. First, 3D tracking is more general, as large out-of-plane motion is often fatal for 2D trackers, but helps 3D trackers to build better models. Second, the tracker's internal model of the object can be used in many different applications and it could even become the main motivation, with tracking supporting reconstruction rather than vice versa. This effectively bridges the gap between visual tracking and structure from motion. A new benchmark data set of sequences with extreme out-of-plane rotation is presented and an online leader-board offered to stimulate new research in the relatively underdeveloped area of 3D tracking. The proposed method, provided as a baseline, is capable of successfully tracking these sequences, all of which pose a considerable challenge to 2D trackers (error reduced by 46 \%).},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TMAGIC-Lebeda et al-2017.pdf},
  journal = {IEEE Transactions on Image Processing},
  language = {en},
  number = {9}
}

@article{leebMotionNets6DTracking2019,
  title = {Motion-{{Nets}}: {{6D Tracking}} of {{Unknown Objects}} in {{Unseen Environments}} Using {{RGB}}},
  shorttitle = {Motion-{{Nets}}},
  author = {Leeb, Felix and Byravan, Arunkumar and Fox, Dieter},
  year = {2019},
  month = oct,
  abstract = {In this work, we bridge the gap between recent pose estimation and tracking work to develop a powerful method for robots to track objects in their surroundings. Motion-Nets use a segmentation model to segment the scene, and separate translation and rotation models to identify the relative 6D motion of an object between two consecutive frames. We train our method with generated data of floating objects, and then test on several prediction tasks, including one with a real PR2 robot, and a toy control task with a simulated PR2 robot never seen during training. Motion-Nets are able to track the pose of objects with some quantitative accuracy for about 30-60 frames including occlusions and distractors. Additionally, the single step prediction errors remain low even after 100 frames. We also investigate an iterative correction procedure to improve performance for control tasks.},
  archivePrefix = {arXiv},
  eprint = {1910.13942},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Motion-Nets-Leeb et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4C3UZ79H/1910.html},
  journal = {arXiv:1910.13942 [cs]},
  primaryClass = {cs}
}

@article{legendreDeepLightLearningIllumination2019,
  title = {{{DeepLight}}: {{Learning Illumination}} for {{Unconstrained Mobile Mixed Reality}}},
  shorttitle = {{{DeepLight}}},
  author = {LeGendre, Chloe and Ma, Wan-Chun and Fyffe, Graham and Flynn, John and Charbonnel, Laurent and Busch, Jay and Debevec, Paul},
  year = {2019},
  month = apr,
  abstract = {We present a learning-based method to infer plausible high dynamic range (HDR), omnidirectional illumination given an unconstrained, low dynamic range (LDR) image from a mobile phone camera with a limited field of view (FOV). For training data, we collect videos of various reflective spheres placed within the camera's FOV, leaving most of the background unoccluded, leveraging that materials with diverse reflectance functions reveal different lighting cues in a single exposure. We train a deep neural network to regress from the LDR background image to HDR lighting by matching the LDR ground truth sphere images to those rendered with the predicted illumination using image-based relighting, which is differentiable. Our inference runs at interactive frame rates on a mobile device, enabling realistic rendering of virtual objects into real scenes for mobile mixed reality. Training on automatically exposed and white-balanced videos, we improve the realism of rendered objects compared to the state-of-the art methods for both indoor and outdoor scenes.},
  archivePrefix = {arXiv},
  eprint = {1904.01175},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepLight-LeGendre et al-2019.pdf;/Users/sunjiaming/Zotero/storage/6WVBG36Z/1904.html},
  journal = {arXiv:1904.01175 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{lehnerPatchRefinementLocalized2019,
  title = {Patch {{Refinement}} -- {{Localized 3D Object Detection}}},
  author = {Lehner, Johannes and Mitterecker, Andreas and Adler, Thomas and Hofmarcher, Markus and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2019},
  month = oct,
  abstract = {We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN). We decompose the detection task into a preliminary Bird's Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets ("patches"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 \% of the available training data and only LiDAR information.},
  archivePrefix = {arXiv},
  eprint = {1910.04093},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Patch Refinement -- Localized 3D Object Detection-Lehner et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WR54FXFP/1910.html},
  journal = {arXiv:1910.04093 [cs]},
  primaryClass = {cs}
}

@inproceedings{leibeDynamic3DScene2007,
  title = {Dynamic {{3D Scene Analysis}} from a {{Moving Vehicle}}},
  booktitle = {2007 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Leibe, Bastian and Cornelis, Nico and Cornelis, Kurt and Van Gool, Luc},
  year = {2007},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Minneapolis, MN, USA}},
  doi = {10.1109/CVPR.2007.383146},
  abstract = {In this paper, we present a system that integrates fully automatic scene geometry estimation, 2D object detection, 3D localization, trajectory estimation, and tracking for dynamic scene interpretation from a moving vehicle. Our sole input are two video streams from a calibrated stereo rig on top of a car. From these streams, we estimate Structurefrom-Motion (SfM) and scene geometry in real-time. In parallel, we perform multi-view/multi-category object recognition to detect cars and pedestrians in both camera images. Using the SfM self-localization, 2D object detections are converted to 3D observations, which are accumulated in a world coordinate frame. A subsequent tracking module analyzes the resulting 3D observations to find physically plausible spacetime trajectories. Finally, a global optimization criterion takes object-object interactions into account to arrive at accurate 3D localization and trajectory estimates for both cars and pedestrians. We demonstrate the performance of our integrated system on challenging real-world data showing car passages through crowded city areas.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Dynamic 3D Scene Analysis from a Moving Vehicle-Leibe et al-2007.pdf},
  isbn = {978-1-4244-1179-5 978-1-4244-1180-1},
  language = {en}
}

@article{leibeRobustObjectDetection2008,
  title = {Robust {{Object Detection}} with {{Interleaved Categorization}} and {{Segmentation}}},
  author = {Leibe, Bastian and Leonardis, Ale{\v s} and Schiele, Bernt},
  year = {2008},
  month = may,
  volume = {77},
  pages = {259--289},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-007-0095-3},
  abstract = {This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance.},
  file = {/Users/sunjiaming/Zotero/storage/2UB8XJPU/Leibe et al. - 2008 - Robust Object Detection with Interleaved Categoriz.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {1-3}
}

@inproceedings{lenzFollowMeEfficientOnline2015,
  title = {{{FollowMe}}: {{Efficient Online Min}}-{{Cost Flow Tracking}} with {{Bounded Memory}} and {{Computation}}},
  shorttitle = {{{FollowMe}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Lenz, Philip and Geiger, Andreas and Urtasun, Raquel},
  year = {2015},
  month = dec,
  pages = {4364--4372},
  publisher = {{IEEE}},
  address = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.496},
  abstract = {One of the most popular approaches to multi-target tracking is tracking-by-detection. Current min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm which solves the data association problem optimally while reusing computation, resulting in faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrary length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FollowMe-Lenz et al-2015.pdf},
  isbn = {978-1-4673-8391-2},
  language = {en}
}

@article{lepetitRecentAdvances3D2020,
  title = {Recent {{Advances}} in {{3D Object}} and {{Hand Pose Estimation}}},
  author = {Lepetit, Vincent},
  year = {2020},
  month = jun,
  abstract = {3D object and hand pose estimation have huge potentials for Augmented Reality, to enable tangible interfaces, natural interfaces, and blurring the boundaries between the real and virtual worlds. In this chapter, we present the recent developments for 3D object and hand pose estimation using cameras, and discuss their abilities and limitations and the possible future development of the field.},
  archivePrefix = {arXiv},
  eprint = {2006.05927},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Recent Advances in 3D Object and Hand Pose Estimation-Lepetit-2020.pdf},
  journal = {arXiv:2006.05927 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{leuteneggerKeyframeBasedVisualInertialSLAM2013,
  title = {Keyframe-{{Based Visual}}-{{Inertial SLAM}} Using {{Nonlinear Optimization}}},
  booktitle = {Robotics: {{Science}} and {{Systems IX}}},
  author = {Leutenegger, Stefan and Furgale, Paul and Rabaud, Vincent and Chli, Margarita and Konolige, Kurt and Siegwart, Roland},
  year = {2013},
  month = jun,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2013.IX.037},
  abstract = {The fusion of visual and inertial cues has become popular in robotics due to the complementary nature of the two sensing modalities. While most fusion strategies to date rely on filtering schemes, the visual robotics community has recently turned to non-linear optimization approaches for tasks such as visual Simultaneous Localization And Mapping (SLAM), following the discovery that this comes with significant advantages in quality of performance and computational complexity. Following this trend, we present a novel approach to tightly integrate visual measurements with readings from an Inertial Measurement Unit (IMU) in SLAM. An IMU error term is integrated with the landmark reprojection error in a fully probabilistic manner, resulting to a joint non-linear cost function to be optimized. Employing the powerful concept of `keyframes' we partially marginalize old states to maintain a bounded-sized optimization window, ensuring real-time operation. Comparing against both vision-only and loosely-coupled visual-inertial algorithms, our experiments confirm the benefits of tight fusion in terms of accuracy and robustness.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Keyframe-Based Visual-Inertial SLAM using Nonlinear Optimization-Leutenegger et al-2013.pdf},
  isbn = {978-981-07-3937-9},
  language = {en}
}

@inproceedings{leventonStatisticalShapeInfluence2000,
  title = {Statistical Shape Influence in Geodesic Active Contours},
  booktitle = {In {{Proc}}. 2000 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}), {{Hilton Head}}, {{SC}}},
  author = {Leventon, Michael E. and Eric, W. and Grimson, L. and Faugeras, Olivier},
  year = {2000},
  pages = {316--323},
  abstract = {A novel method of incorporating shape information into the image segmentation process is presented. We introduce a representation for deformable shapes and define a probability distribution over the variances of a set of training shapes. The segmentation process embeds an initial curve as the zero level set of a higher dimensional surface, and evolves the surface such that the zero level set converges on the boundary of the object to be segmented. At each step of the surface evolution, we estimate the maximum a posteriori (MAP) position and shape of the object in the image, based on the prior shape information and the image information. We then evolve the surface globally, towards the MAP estimate, and locally, based on image gradients and curvature. Results are demonstrated on synthetic data and medical imagery, in 2D and 3D. 1},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Statistical shape influence in geodesic active contours-Leventon et al-2000.pdf;/Users/sunjiaming/Zotero/storage/56IF8TNY/summary.html}
}

@article{levyLeastSquaresConformal,
  title = {Least {{Squares Conformal Maps}} for {{Automatic Texture Atlas Generation}}},
  author = {L{\'e}vy, Bruno and Petitjean, Sylvain and Ray, Nicolas and Maillot, J{\'e}rome},
  pages = {10},
  abstract = {A Texture Atlas is an efficient color representation for 3D Paint Systems. The model to be textured is decomposed into charts homeomorphic to discs, each chart is parameterized, and the unfolded charts are packed in texture space. Existing texture atlas methods for triangulated surfaces suffer from several limitations, requiring them to generate a large number of small charts with simple borders. The discontinuities between the charts cause artifacts, and make it difficult to paint large areas with regular patterns.},
  file = {/Users/sunjiaming/Zotero/storage/MGTSCJ9W/Lévy et al. - Least Squares Conformal Maps for Automatic Texture.pdf},
  language = {en}
}

@article{liangDeepContinuousFusion,
  title = {Deep {{Continuous Fusion}} for {{Multi}}-{{Sensor 3D Object Detection}}},
  author = {Liang, Ming and Wang, Shenlong and Yang, Bin and Urtasun, Raquel},
  pages = {16},
  abstract = {In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Deep Continuous Fusion for Multi-Sensor 3D Object Detection-Liang et al-.pdf},
  language = {en}
}

@article{liangEnhancingReliabilityOutofdistribution2017,
  title = {Enhancing {{The Reliability}} of {{Out}}-of-Distribution {{Image Detection}} in {{Neural Networks}}},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  year = {2017},
  month = jun,
  abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
  archivePrefix = {arXiv},
  eprint = {1706.02690},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Enhancing The Reliability of Out-of-distribution Image Detection in Neural-Liang et al-2017.pdf;/Users/sunjiaming/Zotero/storage/3SLS3K4F/1706.html},
  journal = {arXiv:1706.02690 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{liangMultiTaskMultiSensorFusion,
  title = {Multi-{{Task Multi}}-{{Sensor Fusion}} for {{3D Object Detection}}},
  author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
  pages = {9},
  abstract = {In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and bird's eye view object detection, while being real-time.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-Task Multi-Sensor Fusion for 3D Object Detection-Liang et al-.pdf},
  language = {en}
}

@article{liangPeekingFuturePredicting2019,
  title = {Peeking into the {{Future}}: {{Predicting Future Person Activities}} and {{Locations}} in {{Videos}}},
  shorttitle = {Peeking into the {{Future}}},
  author = {Liang, Junwei and Jiang, Lu and Niebles, Juan Carlos and Hauptmann, Alexander and {Fei-Fei}, Li},
  year = {2019},
  month = feb,
  abstract = {Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about the human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with two auxiliary tasks of predicting future activities and the location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that a joint modeling of paths and activities benefits future path prediction.},
  archivePrefix = {arXiv},
  eprint = {1902.03748},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Peeking into the Future-Liang et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Peeking into the Future-Liang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/HFSI6PI9/1902.html;/Users/sunjiaming/Zotero/storage/SQZU2VN3/1902.html},
  journal = {arXiv:1902.03748 [cs]},
  keywords = {motion prediction},
  primaryClass = {cs}
}

@article{liangPnPNetEndtoEndPerception2020,
  title = {{{PnPNet}}: {{End}}-to-{{End Perception}} and {{Prediction}} with {{Tracking}} in the {{Loop}}},
  shorttitle = {{{PnPNet}}},
  author = {Liang, Ming and Yang, Bin and Zeng, Wenyuan and Chen, Yun and Hu, Rui and Casas, Sergio and Urtasun, Raquel},
  year = {2020},
  month = may,
  abstract = {We tackle the problem of joint perception and motion forecasting in the context of self-driving vehicles. Towards this goal we propose PnPNet, an end-to-end model that takes as input sequential sensor data, and outputs at each time step object tracks and their future trajectories. The key component is a novel tracking module that generates object tracks online from detections and exploits trajectory level features for motion forecasting. Specifically, the object tracks get updated at each time step by solving both the data association problem and the trajectory estimation problem. Importantly, the whole model is end-to-end trainable and benefits from joint optimization of all tasks. We validate PnPNet on two large-scale driving datasets, and show significant improvements over the state-of-the-art with better occlusion recovery and more accurate future prediction.},
  archivePrefix = {arXiv},
  eprint = {2005.14711},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PnPNet-Liang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/3SPGEGX2/2005.html},
  journal = {arXiv:2005.14711 [cs]},
  primaryClass = {cs}
}

@article{liangPolyTransformDeepPolygon2019,
  title = {{{PolyTransform}}: {{Deep Polygon Transformer}} for {{Instance Segmentation}}},
  shorttitle = {{{PolyTransform}}},
  author = {Liang, Justin and Homayounfar, Namdar and Ma, Wei-Chiu and Xiong, Yuwen and Hu, Rui and Urtasun, Raquel},
  year = {2019},
  month = dec,
  abstract = {In this paper, we propose PolyTransform, a novel instance segmentation algorithm that produces precise, geometry-preserving masks by combining the strengths of prevailing segmentation approaches and modern polygon-based methods. In particular, we first exploit a segmentation network to generate instance masks. We then convert the masks into a set of polygons that are then fed to a deforming network that transforms the polygons such that they better fit the object boundaries. Our experiments on the challenging Cityscapes dataset show that our PolyTransform significantly improves the performance of the backbone instance segmentation network and ranks 1st on the Cityscapes test-set leaderboard. We also show impressive gains in the interactive annotation setting.},
  archivePrefix = {arXiv},
  eprint = {1912.02801},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PolyTransform-Liang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9RWBFDL5/1912.html},
  journal = {arXiv:1912.02801 [cs]},
  primaryClass = {cs}
}

@article{liaoRevivingImprovingRecurrent2018,
  title = {Reviving and {{Improving Recurrent Back}}-{{Propagation}}},
  author = {Liao, Renjie and Xiong, Yuwen and Fetaya, Ethan and Zhang, Lisa and Yoon, KiJung and Pitkow, Xaq and Urtasun, Raquel and Zemel, Richard},
  year = {2018},
  month = mar,
  abstract = {In this paper, we revisit the recurrent back-propagation (RBP) algorithm, discuss the conditions under which it applies as well as how to satisfy them in deep neural networks. We show that RBP can be unstable and propose two variants based on conjugate gradient on the normal equations (CG-RBP) and Neumann series (Neumann-RBP). We further investigate the relationship between Neumann-RBP and back propagation through time (BPTT) and its truncated version (TBPTT). Our Neumann-RBP has the same time complexity as TBPTT but only requires constant memory, whereas TBPTT's memory cost scales linearly with the number of truncation steps. We examine all RBP variants along with BPTT and TBPTT in three different application domains: associative memory with continuous Hopfield networks, document classification in citation networks using graph neural networks and hyperparameter optimization for fully connected networks. All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are efficient and effective for optimizing convergent recurrent neural networks.},
  archivePrefix = {arXiv},
  eprint = {1803.06396},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reviving and Improving Recurrent Back-Propagation-Liao et al-2018.pdf;/Users/sunjiaming/Zotero/storage/XK4842F9/1803.html},
  journal = {arXiv:1803.06396 [cs, stat]},
  keywords = {optimization},
  primaryClass = {cs, stat}
}

@article{liaoUnsupervisedLearningGenerative2019,
  title = {Towards {{Unsupervised Learning}} of {{Generative Models}} for {{3D Controllable Image Synthesis}}},
  author = {Liao, Yiyi and Schwarz, Katja and Mescheder, Lars and Geiger, Andreas},
  year = {2019},
  month = dec,
  abstract = {In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.},
  archivePrefix = {arXiv},
  eprint = {1912.05237},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Towards Unsupervised Learning of Generative Models for 3D Controllable Image-Liao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2W37L8RK/1912.html},
  journal = {arXiv:1912.05237 [cs]},
  primaryClass = {cs}
}

@article{liConfidenceGuidedStereo2020,
  title = {Confidence {{Guided Stereo 3D Object Detection}} with {{Split Depth Estimation}}},
  author = {Li, Chengyao and Ku, Jason and Waslander, Steven L.},
  year = {2020},
  month = mar,
  abstract = {Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDAR-based methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.},
  archivePrefix = {arXiv},
  eprint = {2003.05505},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Confidence Guided Stereo 3D Object Detection with Split Depth Estimation-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/WZZUAAYK/2003.html},
  journal = {arXiv:2003.05505 [cs]},
  primaryClass = {cs}
}

@article{liCorrespondenceNetworksAdaptive2020,
  title = {Correspondence {{Networks}} with {{Adaptive Neighbourhood Consensus}}},
  author = {Li, Shuda and Han, Kai and Costain, Theo W. and {Howard-Jenkins}, Henry and Prisacariu, Victor},
  year = {2020},
  month = mar,
  abstract = {In this paper, we tackle the task of establishing dense visual correspondences between images containing objects of the same category. This is a challenging task due to large intra-class variations and a lack of dense pixel level annotations. We propose a convolutional neural network architecture, called adaptive neighbourhood consensus network (ANC-Net), that can be trained end-to-end with sparse key-point annotations, to handle this challenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching. We also introduce a simple and efficient multi-scale self-similarity module in ANC-Net to make the learned feature robust to intra-class variations. Furthermore, we propose a novel orthogonal loss that can enforce the one-to-one matching constraint. We thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {2003.12059},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Correspondence Networks with Adaptive Neighbourhood Consensus-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2NQ54QZQ/2003.html},
  journal = {arXiv:2003.12059 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{liDifferentiableMonteCarlo2018,
  ids = {liDifferentiableMonteCarlo2018a},
  title = {Differentiable {{Monte Carlo}} Ray Tracing through Edge Sampling},
  author = {Li, Tzu-Mao and Aittala, Miika and Durand, Fr{\'e}do and Lehtinen, Jaakko},
  year = {2018},
  month = dec,
  volume = {37},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/3272127.3275109},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Differentiable Monte Carlo ray tracing through edge sampling-Li et al-22.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Differentiable Monte Carlo ray tracing through edge sampling-Li et al-2018.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {graphics},
  language = {en},
  number = {6}
}

@article{liDifferentiableVisualComputing2019,
  title = {Differentiable {{Visual Computing}}},
  author = {Li, Tzu-Mao},
  year = {2019},
  month = apr,
  abstract = {Derivatives of computer graphics, image processing, and deep learning algorithms have tremendous use in guiding parameter space searches, or solving inverse problems. As the algorithms become more sophisticated, we no longer only need to differentiate simple mathematical functions, but have to deal with general programs which encode complex transformations of data. This dissertation introduces three tools for addressing the challenges that arise when obtaining and applying the derivatives for complex graphics algorithms. Traditionally, practitioners have been constrained to composing programs with a limited set of operators, or hand-deriving derivatives. We extend the image processing language Halide with reverse-mode automatic differentiation, and the ability to automatically optimize the gradient computations. This enables automatic generation of the gradients of arbitrary Halide programs, at high performance, with little programmer effort. In 3D rendering, the gradient is required with respect to variables such as camera parameters, geometry, and appearance. However, computing the gradient is challenging because the rendering integral includes visibility terms that are not differentiable. We introduce, to our knowledge, the first general-purpose differentiable ray tracer that solves the full rendering equation, while correctly taking the geometric discontinuities into account. Finally, we demonstrate that the derivatives of light path throughput can also be useful for guiding sampling in forward rendering. Simulating light transport in the presence of multi-bounce glossy effects and motion in 3D rendering is challenging due to the hard-to-sample high-contribution areas. We present a Markov Chain Monte Carlo rendering algorithm that extends Metropolis Light Transport by automatically and explicitly adapting to the local integrand, thereby increasing sampling efficiency.},
  archivePrefix = {arXiv},
  eprint = {1904.12228},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/J8RKRTJF/1904.html},
  journal = {arXiv:1904.12228 [cs]},
  primaryClass = {cs}
}

@article{liDualResolutionCorrespondenceNetworks2020,
  title = {Dual-{{Resolution Correspondence Networks}}},
  author = {Li, Xinghui and Han, Kai and Li, Shuda and Prisacariu, Victor Adrian},
  year = {2020},
  month = jun,
  abstract = {We tackle the problem of establishing dense pixel-wise correspondences between a pair of images. In this work, we introduce Dual-Resolution Correspondence Networks (DRC-Net), to obtain pixel-wise correspondences in a coarse-to-fine manner. DRC-Net extracts both coarse- and fine- resolution feature maps. The coarse maps are used to produce a full but coarse 4D correlation tensor, which is then refined by a learnable neighbourhood consensus module. The fine-resolution feature maps are used to obtain the final dense correspondences guided by the refined coarse 4D correlation tensor. The selected coarse-resolution matching scores allow the fine-resolution features to focus only on a limited number of possible matches with high confidence. In this way, DRC-Net dramatically increases matching reliability and localisation accuracy, while avoiding to apply the expensive 4D convolution kernels on fine-resolution feature maps. We comprehensively evaluate our method on large-scale public benchmarks including HPatches, InLoc, and Aachen Day-Night. It achieves the state-of-the-art results on all of them.},
  archivePrefix = {arXiv},
  eprint = {2006.08844},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dual-Resolution Correspondence Networks-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/LTAIGB2D/2006.html},
  journal = {arXiv:2006.08844 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{liFroDODetections3D2020,
  title = {{{FroDO}}: {{From Detections}} to {{3D Objects}}},
  shorttitle = {{{FroDO}}},
  author = {Li, Kejie and R{\"u}nz, Martin and Tang, Meng and Ma, Lingni and Kong, Chen and Schmidt, Tanner and Reid, Ian and Agapito, Lourdes and Straub, Julian and Lovegrove, Steven and Newcombe, Richard},
  year = {2020},
  month = may,
  abstract = {Object-oriented maps are important for scene understanding since they jointly capture geometry and semantics, allow individual instantiation and meaningful reasoning about objects. We introduce FroDO, a method for accurate 3D reconstruction of object instances from RGB video that infers object location, pose and shape in a coarse-to-fine manner. Key to FroDO is to embed object shapes in a novel learnt space that allows seamless switching between sparse point cloud and dense DeepSDF decoding. Given an input sequence of localized RGB frames, FroDO first aggregates 2D detections to instantiate a category-aware 3D bounding box per object. A shape code is regressed using an encoder network before optimizing shape and pose further under the learnt shape priors using sparse and dense shape representations. The optimization uses multi-view geometric, photometric and silhouette losses. We evaluate on real-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view, multi-view, and multi-object reconstruction.},
  archivePrefix = {arXiv},
  eprint = {2005.05125},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FroDO-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/N7X99E78/2005.html},
  journal = {arXiv:2005.05125 [cs]},
  primaryClass = {cs}
}

@incollection{liGridsLearningGraph2018,
  title = {Beyond {{Grids}}: {{Learning Graph Representations}} for {{Visual Recognition}}},
  shorttitle = {Beyond {{Grids}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Li, Yin and Gupta, Abhinav},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9245--9255},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Beyond Grids-Li_Gupta-2018.pdf;/Users/sunjiaming/Zotero/storage/EEWT8JI8/8135-beyond-grids-learning-graph-representations-for-visual-recognition.html},
  keywords = {graph convolution}
}

@article{liHighPerformanceVisual,
  title = {High {{Performance Visual Tracking With Siamese Region Proposal Network}}},
  author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  pages = {10},
  abstract = {Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/High Performance Visual Tracking With Siamese Region Proposal Network-Li et al-.pdf},
  language = {en}
}

@article{liInteriorNetMegascaleMultisensor2018,
  title = {{{InteriorNet}}: {{Mega}}-Scale {{Multi}}-Sensor {{Photo}}-Realistic {{Indoor Scenes Dataset}}},
  shorttitle = {{{InteriorNet}}},
  author = {Li, Wenbin and Saeedi, Sajad and McCormac, John and Clark, Ronald and Tzoumanikas, Dimos and Ye, Qing and Huang, Yuzhong and Tang, Rui and Leutenegger, Stefan},
  year = {2018},
  month = sep,
  abstract = {Datasets have gained an enormous amount of popularity in the computer vision community, from training and evaluation of Deep Learning-based methods to benchmarking Simultaneous Localization and Mapping (SLAM). Without a doubt, synthetic imagery bears a vast potential due to scalability in terms of amounts of data obtainable without tedious manual ground truth annotations or measurements. Here, we present a dataset with the aim of providing a higher degree of photo-realism, larger scale, more variability as well as serving a wider range of purposes compared to existing datasets. Our dataset leverages the availability of millions of professional interior designs and millions of production-level furniture and object assets -- all coming with fine geometric details and high-resolution texture. We render high-resolution and high frame-rate video sequences following realistic trajectories while supporting various camera types as well as providing inertial measurements. Together with the release of the dataset, we will make executable program of our interactive simulator software as well as our renderer available at https://interiornetdataset.github.io. To showcase the usability and uniqueness of our dataset, we show benchmarking results of both sparse and dense SLAM algorithms.},
  archivePrefix = {arXiv},
  eprint = {1809.00716},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/InteriorNet-Li et al-2018.pdf;/Users/sunjiaming/Zotero/storage/LX3JCYFC/Li et al. - 2018 - InteriorNet Mega-scale Multi-sensor Photo-realist.pdf;/Users/sunjiaming/Zotero/storage/A8LP7P2A/1809.html},
  journal = {arXiv:1809.00716 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{liInverseRenderingComplex,
  title = {Inverse {{Rendering}} for {{Complex Indoor Scenes}}: {{Shape}}, {{Spatially}}-{{Varying Lighting}} and {{SVBRDF From}} a {{Single Image}}},
  author = {Li, Zhengqin and Shafiei, Mohammad and Ramamoorthi, Ravi and Sunkavalli, Kalyan and Chandraker, Manmohan},
  pages = {10},
  abstract = {We propose a deep inverse rendering framework for indoor scenes. From a single RGB image of an arbitrary indoor scene, we obtain a complete scene reconstruction, estimating shape, spatially-varying lighting, and spatiallyvarying, non-Lambertian surface reflectance. Our novel inverse rendering network incorporates physical insights \textendash including a spatially-varying spherical Gaussian lighting representation, a differentiable rendering layer to model scene appearance, a cascade structure to iteratively refine the predictions and a bilateral solver for refinement \textendash{} allowing us to jointly reason about shape, lighting, and reflectance. Since no existing dataset provides ground truth high quality spatially-varying material and spatially-varying lighting, we propose novel methods to map complex materials to existing indoor scene datasets and a new physically-based GPU renderer to create a large-scale, photorealistic indoor dataset. Experiments show that our framework outperforms previous methods and enables various novel applications like photorealistic object insertion and material editing.},
  file = {/Users/sunjiaming/Zotero/storage/EWQ3UY3E/Li et al. - Inverse Rendering for Complex Indoor Scenes Shape.pdf},
  language = {en}
}

@article{liJointSpatialTemporalOptimization2020,
  title = {Joint {{Spatial}}-{{Temporal Optimization}} for {{Stereo 3D Object Tracking}}},
  author = {Li, Peiliang and Shi, Jieqi and Shen, Shaojie},
  year = {2020},
  month = apr,
  abstract = {Directly learning multiple 3D objects motion from sequential images is difficult, while the geometric bundle adjustment lacks the ability to localize the invisible object centroid. To benefit from both the powerful object understanding skill from deep neural network meanwhile tackle precise geometry modeling for consistent trajectory estimation, we propose a joint spatial-temporal optimization-based stereo 3D object tracking method. From the network, we detect corresponding 2D bounding boxes on adjacent images and regress an initial 3D bounding box. Dense object cues (local depth and local coordinates) that associating to the object centroid are then predicted using a region-based network. Considering both the instant localization accuracy and motion consistency, our optimization models the relations between the object centroid and observed cues into a joint spatial-temporal error function. All historic cues will be summarized to contribute to the current estimation by a per-frame marginalization strategy without repeated computation. Quantitative evaluation on the KITTI tracking dataset shows our approach outperforms previous image-based 3D tracking methods by significant margins. We also report extensive results on multiple categories and larger datasets (KITTI raw and Argoverse Tracking) for future benchmarking.},
  archivePrefix = {arXiv},
  eprint = {2004.09305},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2SIM4UF8/2004.html},
  journal = {arXiv:2004.09305 [cs]},
  primaryClass = {cs}
}

@article{liLearningDepthsMoving2019,
  title = {Learning the {{Depths}} of {{Moving People}} by {{Watching Frozen People}}},
  author = {Li, Zhengqi and Dekel, Tali and Cole, Forrester and Tucker, Richard and Snavely, Noah and Liu, Ce and Freeman, William T.},
  year = {2019},
  month = apr,
  abstract = {We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving. Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects' motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene. Because people are stationary, training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We demonstrate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and show various 3D effects produced using our predicted depth.},
  archivePrefix = {arXiv},
  eprint = {1904.11111},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning the Depths of Moving People by Watching Frozen People-Li et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LKZSFPHP/1904.html},
  journal = {arXiv:1904.11111 [cs]},
  primaryClass = {cs}
}

@article{liLearningDepthsMoving2019a,
  title = {Learning the {{Depths}} of {{Moving People}} by {{Watching Frozen People}}},
  author = {Li, Zhengqi and Dekel, Tali and Cole, Forrester and Tucker, Richard and Snavely, Noah and Liu, Ce and Freeman, William T.},
  year = {2019},
  month = apr,
  abstract = {We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving. Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects' motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene. Because people are stationary, training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We demonstrate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and show various 3D effects produced using our predicted depth.},
  archivePrefix = {arXiv},
  eprint = {1904.11111},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning the Depths of Moving People by Watching Frozen People-Li et al-22.pdf;/Users/sunjiaming/Zotero/storage/XPIFP654/1904.html},
  journal = {arXiv:1904.11111 [cs]},
  primaryClass = {cs}
}

@article{liLookingGlassNeural2020,
  title = {Through the {{Looking Glass}}: {{Neural 3D Reconstruction}} of {{Transparent Shapes}}},
  shorttitle = {Through the {{Looking Glass}}},
  author = {Li, Zhengqin and Yeh, Yu-Ying and Chandraker, Manmohan},
  year = {2020},
  month = apr,
  abstract = {Recovering the 3D shape of transparent objects using a small number of unconstrained natural images is an ill-posed problem. Complex light paths induced by refraction and reflection have prevented both traditional and deep multiview stereo from solving this challenge. We propose a physically-based network to recover 3D shape of transparent objects using a few images acquired with a mobile phone camera, under a known but arbitrary environment map. Our novel contributions include a normal representation that enables the network to model complex light transport through local computation, a rendering layer that models refractions and reflections, a cost volume specifically designed for normal refinement of transparent shapes and a feature mapping based on predicted normals for 3D point cloud reconstruction. We render a synthetic dataset to encourage the model to learn refractive light transport across different views. Our experiments show successful recovery of high-quality 3D geometry for complex transparent shapes using as few as 5-12 natural images. Code and data are publicly released.},
  archivePrefix = {arXiv},
  eprint = {2004.10904},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Through the Looking Glass-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/SVNSSLDV/2004.html},
  journal = {arXiv:2004.10904 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@inproceedings{liMegaDepthLearningSingleView2018,
  title = {{{MegaDepth}}: {{Learning Single}}-{{View Depth Prediction}} from {{Internet Photos}}},
  shorttitle = {{{MegaDepth}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Zhengqi and Snavely, Noah},
  year = {2018},
  month = jun,
  pages = {2041--2050},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00218},
  file = {/Users/sunjiaming/Zotero/storage/LG24STZ5/Li and Snavely - 2018 - MegaDepth Learning Single-View Depth Prediction f.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{liMonocularRealTimeVolumetric2020,
  title = {Monocular {{Real}}-{{Time Volumetric Performance Capture}}},
  author = {Li, Ruilong and Xiu, Yuliang and Saito, Shunsuke and Huang, Zeng and Olszewski, Kyle and Li, Hao},
  year = {2020},
  month = jul,
  abstract = {We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.},
  archivePrefix = {arXiv},
  eprint = {2007.13988},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Monocular Real-Time Volumetric Performance Capture-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FXEWUJ9B/2007.html},
  journal = {arXiv:2007.13988 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Performance},
  primaryClass = {cs}
}

@article{limSimpleApproachIntrinsic2018,
  title = {A {{Simple Approach}} to {{Intrinsic Correspondence Learning}} on {{Unstructured 3D Meshes}}},
  author = {Lim, Isaak and Dielen, Alexander and Campen, Marcel and Kobbelt, Leif},
  year = {2018},
  month = sep,
  abstract = {The question of representation of 3D geometry is of vital importance when it comes to leveraging the recent advances in the field of machine learning for geometry processing tasks. For common unstructured surface meshes state-of-the-art methods rely on patch-based or mapping-based techniques that introduce resampling operations in order to encode neighborhood information in a structured and regular manner. We investigate whether such resampling can be avoided, and propose a simple and direct encoding approach. It does not only increase processing efficiency due to its simplicity - its direct nature also avoids any loss in data fidelity. To evaluate the proposed method, we perform a number of experiments in the challenging domain of intrinsic, non-rigid shape correspondence estimation. In comparisons to current methods we observe that our approach is able to achieve highly competitive results.},
  archivePrefix = {arXiv},
  eprint = {1809.06664},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Simple Approach to Intrinsic Correspondence Learning on Unstructured 3D Meshes-Lim et al-2018.pdf;/Users/sunjiaming/Zotero/storage/EF55VQX6/1809.html},
  journal = {arXiv:1809.06664 [cs]},
  primaryClass = {cs}
}

@article{liMultiSensor3DObject2019,
  title = {Multi-{{Sensor 3D Object Box Refinement}} for {{Autonomous Driving}}},
  author = {Li, Peiliang and Liu, Siqi and Shen, Shaojie},
  year = {2019},
  month = sep,
  abstract = {We propose a 3D object detection system with multi-sensor refinement in the context of autonomous driving. In our framework, the monocular camera serves as the fundamental sensor for 2D object proposal and initial 3D bounding box prediction. While the stereo cameras and LiDAR are treated as adaptive plug-in sensors to refine the 3D box localization performance. For each observed element in the raw measurement domain (e.g., pixels for stereo, 3D points for LiDAR), we model the local geometry as an instance vector representation, which indicates the 3D coordinate of each element respecting to the object frame. Using this unified geometric representation, the 3D object location can be unified refined by the stereo photometric alignment or point cloud alignment. We demonstrate superior 3D detection and localization performance compared to state-of-the-art monocular, stereo methods and competitive performance compared with the baseline LiDAR method on the KITTI object benchmark.},
  archivePrefix = {arXiv},
  eprint = {1909.04942},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-Sensor 3D Object Box Refinement for Autonomous Driving-Li et al-2019.pdf;/Users/sunjiaming/Zotero/storage/W6WJ4ZZR/1909.html},
  journal = {arXiv:1909.04942 [cs]},
  primaryClass = {cs}
}

@article{liMultiViewPhotometricStereo2020,
  title = {Multi-{{View Photometric Stereo}}: {{A Robust Solution}} and {{Benchmark Dataset}} for {{Spatially Varying Isotropic Materials}}},
  shorttitle = {Multi-{{View Photometric Stereo}}},
  author = {Li, Min and Zhou, Zhenglong and Wu, Zhe and Shi, Boxin and Diao, Changyu and Tan, Ping},
  year = {2020},
  month = jan,
  abstract = {We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo (MVPS) technique that works for general isotropic materials. Our algorithm is suitable for perspective cameras and nearby point light sources. Our data capture setup is simple, which consists of only a digital camera, some LED lights, and an optional automatic turntable. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distribution function (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. In experiments, we demonstrate our algorithm with two different setups: a studio setup for highest precision and a desktop setup for best usability. According to our experiments, under the studio setting, the captured shapes are accurate to 0.5 millimeters and the captured reflectance has a relative root-mean-square error (RMSE) of 9\%. We also quantitatively evaluate state-of-the-art MVPS on a newly collected benchmark dataset, which is publicly available for inspiring future research.},
  archivePrefix = {arXiv},
  eprint = {2001.06659},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-View Photometric Stereo-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/5THDS5SA/2001.html},
  journal = {arXiv:2001.06659 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{linEfficiencySinkhornGreenkhorn2020,
  title = {On the {{Efficiency}} of the {{Sinkhorn}} and {{Greenkhorn Algorithms}} and {{Their Acceleration}} for {{Optimal Transport}}},
  author = {Lin, Tianyi and Ho, Nhat and Jordan, Michael I.},
  year = {2020},
  month = mar,
  abstract = {We present new complexity results for several algorithms that approximately solve the regularized optimal transport (OT) problem between two discrete probability measures with at most \$n\$ atoms. First, we show that a greedy variant of the classical Sinkhorn algorithm, known as the \textbackslash textit\{Greenkhorn\} algorithm, achieves the complexity bound of \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^2\textbackslash varepsilon\^\{-2\})\$, which improves the best known bound \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^2\textbackslash varepsilon\^\{-3\})\$. Notably, this matches the best known complexity bound of the Sinkhorn algorithm and explains the superior performance of the Greenkhorn algorithm in practice. Furthermore, we generalize an adaptive primal-dual accelerated gradient descent (APDAGD) algorithm with mirror mapping \$\textbackslash phi\$ and show that the resulting \textbackslash textit\{adaptive primal-dual accelerated mirror descent\} (APDAMD) algorithm achieves the complexity bound of \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^2\textbackslash sqrt\{\textbackslash delta\}\textbackslash varepsilon\^\{-1\})\$ where \$\textbackslash delta{$>$}0\$ depends on \$\textbackslash phi\$. We point out that an existing complexity bound for the APDAGD algorithm is not valid in general using a simple counterexample and then establish the complexity bound of \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^\{5/2\}\textbackslash varepsilon\^\{-1\})\$ by exploiting the connection between the APDAMD and APDAGD algorithms. Moreover, we introduce accelerated Sinkhorn and Greenkhorn algorithms that achieve the complexity bound of \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^\{7/3\}\textbackslash varepsilon\^\{-1\})\$, which improves on the complexity bounds \$\textbackslash widetilde\{\textbackslash mathcal\{O\}\}(n\^2\textbackslash varepsilon\^\{-2\})\$ of Sinkhorn and Greenkhorn algorithms in terms of \$\textbackslash varepsilon\$. Experimental results on synthetic and real datasets demonstrate the favorable performance of new algorithms in practice.},
  archivePrefix = {arXiv},
  eprint = {1906.01437},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On the Efficiency of the Sinkhorn and Greenkhorn Algorithms and Their-Lin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NB32QLF7/1906.html},
  journal = {arXiv:1906.01437 [cs, stat]},
  keywords = {Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = {2017},
  month = apr,
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archivePrefix = {arXiv},
  eprint = {1612.03144},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Feature Pyramid Networks for Object Detection-Lin et al-2017.pdf;/Users/sunjiaming/Zotero/storage/34Z9HUH4/1612.html},
  journal = {arXiv:1612.03144 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper},
  primaryClass = {cs}
}

@article{linFocalLossDense2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2018},
  month = feb,
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archivePrefix = {arXiv},
  eprint = {1708.02002},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Focal Loss for Dense Object Detection-Lin et al-2018.pdf;/Users/sunjiaming/Zotero/storage/TUBSZRP6/1708.html},
  journal = {arXiv:1708.02002 [cs]},
  primaryClass = {cs}
}

@article{linInverseCompositionalSpatial2016,
  title = {Inverse {{Compositional Spatial Transformer Networks}}},
  author = {Lin, Chen-Hsuan and Lucey, Simon},
  year = {2016},
  month = dec,
  abstract = {In this paper, we establish a theoretical connection between the classical Lucas \& Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.},
  archivePrefix = {arXiv},
  eprint = {1612.03897},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Inverse Compositional Spatial Transformer Networks-Lin_Lucey-2016.pdf;/Users/sunjiaming/Zotero/storage/IVSF43C8/1612.html},
  journal = {arXiv:1612.03897 [cs]},
  primaryClass = {cs}
}

@article{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2014},
  month = may,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archivePrefix = {arXiv},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Microsoft COCO-Lin et al-2014.pdf;/Users/sunjiaming/Zotero/storage/4SQIBCWJ/1405.html},
  journal = {arXiv:1405.0312 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{linPhotometricMeshOptimization2019,
  title = {Photometric {{Mesh Optimization}} for {{Video}}-{{Aligned 3D Object Reconstruction}}},
  author = {Lin, Chen-Hsuan and Wang, Oliver and Russell, Bryan C. and Shechtman, Eli and Kim, Vladimir G. and Fisher, Matthew and Lucey, Simon},
  year = {2019},
  month = mar,
  abstract = {In this paper, we address the problem of 3D object mesh reconstruction from RGB videos. Our approach combines the best of multi-view geometric and data-driven methods for 3D reconstruction by optimizing object meshes for multiview photometric consistency while constraining mesh deformations with a shape prior. We pose this as a piecewise image alignment problem for each mesh face projection. Our approach allows us to update shape parameters from the photometric error without any depth or mask information. Moreover, we show how to avoid a degeneracy of zero photometric gradients via rasterizing from a virtual viewpoint. We demonstrate 3D object mesh reconstruction results from both synthetic and real-world videos with our photometric mesh optimization, which is unachievable with either na\textasciidieresis\i ve mesh generation networks or traditional pipelines of surface reconstruction without heavy manual post-processing.},
  archivePrefix = {arXiv},
  eprint = {1903.08642},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/JGFS258T/Lin et al. - 2019 - Photometric Mesh Optimization for Video-Aligned 3D.pdf},
  journal = {arXiv:1903.08642 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liRevisitingStereoDepth2020,
  title = {Revisiting {{Stereo Depth Estimation From}} a {{Sequence}}-to-{{Sequence Perspective}} with {{Transformers}}},
  author = {Li, Zhaoshuo and Liu, Xingtong and Creighton, Francis X. and Taylor, Russell H. and Unberath, Mathias},
  year = {2020},
  month = nov,
  abstract = {Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right image to infer depth. Rather than matching individual pixels, in this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence of estimation, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes well across different domains, even without fine-tuning. Our code is publicly available at https://github.com/mli0603/stereo-transformer.},
  archivePrefix = {arXiv},
  eprint = {2011.02910},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/A5C6L8PG/2011.html},
  journal = {arXiv:2011.02910 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{liRobust3DSelfportraits2020,
  title = {Robust {{3D Self}}-Portraits in {{Seconds}}},
  author = {Li, Zhe and Yu, Tao and Pan, Chuanyu and Zheng, Zerong and Liu, Yebin},
  year = {2020},
  month = apr,
  abstract = {In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only "loop" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {2004.02460},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust 3D Self-portraits in Seconds-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FUEF8M4A/2004.html},
  journal = {arXiv:2004.02460 [cs]},
  primaryClass = {cs}
}

@article{liSelfsupervisedSingleview3D2020,
  title = {Self-Supervised {{Single}}-View {{3D Reconstruction}} via {{Semantic Consistency}}},
  author = {Li, Xueting and Liu, Sifei and Kim, Kihwan and De Mello, Shalini and Jampani, Varun and Yang, Ming-Hsuan and Kautz, Jan},
  year = {2020},
  month = mar,
  abstract = {We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging self-supervisedly learned part segmentation of a large collection of category-specific images, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision.},
  archivePrefix = {arXiv},
  eprint = {2003.06473},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-supervised Single-view 3D Reconstruction via Semantic Consistency-Li et al-2020.pdf;/Users/sunjiaming/Zotero/storage/LHF79I8J/2003.html},
  journal = {arXiv:2003.06473 [cs]},
  primaryClass = {cs}
}

@article{liSelfsupervisedSingleview3D2020a,
  title = {Self-Supervised {{Single}}-View {{3D Reconstruction}} via {{Semantic Consistency}}},
  author = {Li, Xueting and Liu, Sifei and Kim, Kihwan and De Mello, Shalini and Jampani, Varun and Yang, Ming-Hsuan and Kautz, Jan},
  year = {2020},
  month = mar,
  abstract = {We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging self-supervisedly learned part segmentation of a large collection of category-specific images, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision.},
  archivePrefix = {arXiv},
  eprint = {2003.06473},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-supervised Single-view 3D Reconstruction via Semantic Consistency-Li et al-22.pdf;/Users/sunjiaming/Zotero/storage/6TY7N4W8/2003.html},
  journal = {arXiv:2003.06473 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{liSiamRPNEvolutionSiamese2018,
  title = {{{SiamRPN}}++: {{Evolution}} of {{Siamese Visual Tracking}} with {{Very Deep Networks}}},
  shorttitle = {{{SiamRPN}}++},
  author = {Li, Bo and Wu, Wei and Wang, Qiang and Zhang, Fangyi and Xing, Junliang and Yan, Junjie},
  year = {2018},
  month = dec,
  abstract = {Siamese network based trackers formulate tracking as convolutional feature cross-correlation between a target template and a search region. However, Siamese trackers still have an accuracy gap compared with state-of-theart algorithms and they cannot take advantage of features from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform layer-wise and depthwise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on five large tracking benchmarks, including OTB2015, VOT2018, UAV123, LaSOT, and TrackingNet. Our model will be released to facilitate further researches.},
  archivePrefix = {arXiv},
  eprint = {1812.11703},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SiamRPN++-Li et al-2018.pdf},
  journal = {arXiv:1812.11703 [cs]},
  keywords = {2d tracking},
  language = {en},
  primaryClass = {cs}
}

@article{liStereoRCNNBased2019,
  title = {Stereo {{R}}-{{CNN}} Based {{3D Object Detection}} for {{Autonomous Driving}}},
  author = {Li, Peiliang and Chen, Xiaozhi and Shen, Shaojie},
  year = {2019},
  month = feb,
  abstract = {We propose a 3D object detection method for autonomous driving by fully exploiting the sparse and dense, semantic and geometry information in stereo imagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo inputs to simultaneously detect and associate object in left and right images. We add extra branches after stereo Region Proposal Network (RPN) to predict sparse keypoints, viewpoints, and object dimensions, which are combined with 2D left-right boxes to calculate a coarse 3D object bounding box. We then recover the accurate 3D bounding box by a region-based photometric alignment using left and right RoIs. Our method does not require depth input and 3D position supervision, however, outperforms all existing fully supervised image-based methods. Experiments on the challenging KITTI dataset show that our method outperforms the state-of-the-art stereo-based method by around 30\% AP on both 3D detection and 3D localization tasks. Code will be made publicly available.},
  archivePrefix = {arXiv},
  eprint = {1902.09738},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Stereo R-CNN based 3D Object Detection for Autonomous Driving-Li et al-2019.pdf;/Users/sunjiaming/Zotero/storage/D47S7NTJ/1902.html},
  journal = {arXiv:1902.09738 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@article{liStereoVisionbasedSemantic2018,
  title = {Stereo {{Vision}}-Based {{Semantic 3D Object}} and {{Ego}}-Motion {{Tracking}} for {{Autonomous Driving}}},
  author = {Li, Peiliang and Qin, Tong and Shen, Shaojie},
  year = {2018},
  month = jul,
  abstract = {We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-theart solutions.},
  archivePrefix = {arXiv},
  eprint = {1807.02062},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous-Li et al-2018.pdf},
  journal = {arXiv:1807.02062 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liTextSLAMVisualSLAM2019,
  title = {{{TextSLAM}}: {{Visual SLAM}} with {{Planar Text Features}}},
  shorttitle = {{{TextSLAM}}},
  author = {Li, Boying and Zou, Danping and Sartori, Daniele and Pei, Ling and Yu, Wenxian},
  year = {2019},
  month = nov,
  abstract = {We propose to integrate text objects in man-made scenes tightly into the visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to treat each detected text as a planar feature which is rich of textures and semantic meanings. The text feature is compactly represented by three parameters and integrated into visual SLAM by adopting the illumination-invariant photometric error. We also describe important details involved in implementing a full pipeline of text-based visual SLAM. To our best knowledge, this is the first visual SLAM method tightly coupled with the text features. We tested our method in both indoor and outdoor environments. The results show that with text features, the visual SLAM system becomes more robust and produces much more accurate 3D text maps that could be useful for navigation and scene understanding in robotic or augmented reality applications.},
  archivePrefix = {arXiv},
  eprint = {1912.05002},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TextSLAM-Li et al-2019.pdf},
  journal = {arXiv:1912.05002 [cs]},
  primaryClass = {cs}
}

@article{liuAutoDeepLabHierarchicalNeural2019,
  title = {Auto-{{DeepLab}}: {{Hierarchical Neural Architecture Search}} for {{Semantic Image Segmentation}}},
  shorttitle = {Auto-{{DeepLab}}},
  author = {Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan and {Fei-Fei}, Li},
  year = {2019},
  month = jan,
  abstract = {Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification problems. In this paper, we study NAS for semantic image segmentation, an important computer vision task that assigns a semantic label to every pixel in an image. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Without any ImageNet pretraining, our architecture searched specifically for semantic image segmentation attains state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {1901.02985},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Auto-DeepLab-Liu et al-2019.pdf},
  journal = {arXiv:1901.02985 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liuDeepFittingDegree2019,
  ids = {liuDeepFittingDegree2019a},
  title = {Deep {{Fitting Degree Scoring Network}} for {{Monocular 3D Object Detection}}},
  author = {Liu, Lijie and Lu, Jiwen and Xu, Chunjing and Tian, Qi and Zhou, Jie},
  year = {2019},
  month = apr,
  abstract = {In this paper, we propose to learn a deep fitting degree scoring network for monocular 3D object detection, which aims to score fitting degree between proposals and object conclusively. Different from most existing monocular frameworks which use tight constraint to get 3D location, our approach achieves high-precision localization through measuring the visual fitting degree between the projected 3D proposals and the object. We first regress the dimension and orientation of the object using an anchor-based method so that a suitable 3D proposal can be constructed. We propose FQNet, which can infer the 3D IoU between the 3D proposals and the object solely based on 2D cues. Therefore, during the detection process, we sample a large number of candidates in the 3D space and project these 3D bounding boxes on 2D image individually. The best candidate can be picked out by simply exploring the spatial overlap between proposals and the object, in the form of the output 3D IoU score of FQNet. Experiments on the KITTI dataset demonstrate the effectiveness of our framework.},
  archivePrefix = {arXiv},
  eprint = {1904.12681},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Fitting Degree Scoring Network for Monocular 3D Object Detection-Liu et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Fitting Degree Scoring Network for Monocular 3D Object Detection-Liu et al-22.pdf;/Users/sunjiaming/Zotero/storage/C3XSBVAA/1904.html;/Users/sunjiaming/Zotero/storage/LZF68CJH/1904.html},
  journal = {arXiv:1904.12681 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@article{liuDensePointLearningDensely2019,
  title = {{{DensePoint}}: {{Learning Densely Contextual Representation}} for {{Efficient Point Cloud Processing}}},
  shorttitle = {{{DensePoint}}},
  author = {Liu, Yongcheng and Fan, Bin and Meng, Gaofeng and Lu, Jiwen and Xiang, Shiming and Pan, Chunhong},
  year = {2019},
  month = sep,
  abstract = {Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts.},
  archivePrefix = {arXiv},
  eprint = {1909.03669},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DensePoint-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YEFVQYFL/1909.html},
  journal = {arXiv:1909.03669 [cs]},
  primaryClass = {cs}
}

@article{liuDISTRenderingDeep2019,
  title = {{{DIST}}: {{Rendering Deep Implicit Signed Distance Function}} with {{Differentiable Sphere Tracing}}},
  shorttitle = {{{DIST}}},
  author = {Liu, Shaohui and Zhang, Yinda and Peng, Songyou and Shi, Boxin and Pollefeys, Marc and Cui, Zhaopeng},
  year = {2019},
  month = nov,
  abstract = {We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward pass of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backward to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noise.},
  archivePrefix = {arXiv},
  eprint = {1911.13225},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DIST-Liu et al-22.pdf;/Users/sunjiaming/Zotero/storage/2SSHCABP/1911.html},
  journal = {arXiv:1911.13225 [cs]},
  primaryClass = {cs}
}

@article{liuFlowNet3DLearningScene2018,
  title = {{{FlowNet3D}}: {{Learning Scene Flow}} in {{3D Point Clouds}}},
  shorttitle = {{{FlowNet3D}}},
  author = {Liu, Xingyu and Qi, Charles R. and Guibas, Leonidas J.},
  year = {2018},
  month = jun,
  abstract = {Many applications in robotics and human-computer interaction can benefit from understanding 3D motion of points in a dynamic environment, widely noted as scene flow. While most previous methods focus on stereo and RGB-D images as input, few try to estimate scene flow directly from point clouds. In this work, we propose a novel deep neural network named \$FlowNet3D\$ that learns scene flow from point clouds in an end-to-end fashion. Our network simultaneously learns deep hierarchical features of point clouds and flow embeddings that represent point motions, supported by two newly proposed learning layers for point sets. We evaluate the network on both challenging synthetic data from FlyingThings3D and real Lidar scans from KITTI. Trained on synthetic data only, our network successfully generalizes to real scans, outperforming various baselines and showing competitive results to the prior art. We also demonstrate two applications of our scene flow output (scan registration and motion segmentation) to show its potential wide use cases.},
  archivePrefix = {arXiv},
  eprint = {1806.01411},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FlowNet3D-Liu et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FlowNet3D-Liu et al-3.pdf},
  journal = {arXiv:1806.01411 [cs]},
  primaryClass = {cs}
}

@article{liuFurnishingYourRoom2019,
  title = {Furnishing {{Your Room}} by {{What You See}}: {{An End}}-to-{{End Furniture Set Retrieval Framework}} with {{Rich Annotated Benchmark Dataset}}},
  shorttitle = {Furnishing {{Your Room}} by {{What You See}}},
  author = {Liu, Bingyuan and Zhang, Jiantao and Zhang, Xiaoting and Zhang, Wei and Yu, Chuanhui and Zhou, Yuan},
  year = {2019},
  month = nov,
  abstract = {Understanding interior scenes has attracted enormous interest in computer vision community. However, few works focus on the understanding of furniture within the scenes and a large-scale dataset is also lacked to advance the field. In this paper, we first fill the gap by presenting DeepFurniture, a richly annotated large indoor scene dataset, including 24k indoor images, 170k furniture instances and 20k unique furniture identities. On the dataset, we introduce a new benchmark, named furniture set retrieval. Given an indoor photo as input, the task requires to detect all the furniture instances and search a matched set of furniture identities. To address this challenging task, we propose a feature and context embedding based framework. It contains 3 major contributions: (1) An improved Mask-RCNN model with an additional mask-based classifier is introduced for better utilizing the mask information to relieve the occlusion problems in furniture detection context. (2) A multi-task style Siamese network is proposed to train the feature embedding model for retrieval, which is composed of a classification subnet supervised by self-clustered pseudo attributes and a verification subnet to estimate whether the input pair is matched. (3) In order to model the relationship of the furniture entities in an interior design, a context embedding model is employed to re-rank the retrieval results. Extensive experiments demonstrate the effectiveness of each module and the overall system.},
  archivePrefix = {arXiv},
  eprint = {1911.09299},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Furnishing Your Room by What You See-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/NGX2JGKB/1911.html},
  journal = {arXiv:1911.09299 [cs]},
  primaryClass = {cs}
}

@article{liuGIFTLearningTransformationInvariant2019,
  title = {{{GIFT}}: {{Learning Transformation}}-{{Invariant Dense Visual Descriptors}} via {{Group CNNs}}},
  shorttitle = {{{GIFT}}},
  author = {Liu, Yuan and Shen, Zehong and Lin, Zhixuan and Peng, Sida and Bao, Hujun and Zhou, Xiaowei},
  year = {2019},
  month = nov,
  abstract = {Finding local correspondences between images with different viewpoints requires local descriptors that are robust against geometric transformations. An approach for transformation invariance is to integrate out the transformations by pooling the features extracted from transformed versions of an image. However, the feature pooling may sacrifice the distinctiveness of the resulting descriptors. In this paper, we introduce a novel visual descriptor named Group Invariant Feature Transform (GIFT), which is both discriminative and robust to geometric transformations. The key idea is that the features extracted from the transformed versions of an image can be viewed as a function defined on the group of the transformations. Instead of feature pooling, we use group convolutions to exploit underlying structures of the extracted features on the group, resulting in descriptors that are both discriminative and provably invariant to the group of transformations. Extensive experiments show that GIFT outperforms state-of-the-art methods on several benchmark datasets and practically improves the performance of relative pose estimation.},
  archivePrefix = {arXiv},
  eprint = {1911.05932},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GIFT-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/A6KQCARN/1911.html},
  journal = {arXiv:1911.05932 [cs]},
  primaryClass = {cs}
}

@inproceedings{liuHigherOrderCRFStructural2015,
  title = {Higher-{{Order CRF Structural Segmentation}} of {{3D Reconstructed Surfaces}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Jingbo and Wang, Jinglu and Fang, Tian and Tai, Chiew-Lan and Quan, Long},
  year = {2015},
  month = dec,
  pages = {2093--2101},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.242},
  abstract = {In this paper, we propose a structural segmentation algorithm to partition multi-view stereo reconstructed surfaces of large-scale urban environments into structural segments. Each segment corresponds to a structural component describable by a surface primitive of up to the second order. This segmentation is for use in subsequent urban object modeling, vectorization, and recognition. To overcome the high geometrical and topological noise levels in the 3D reconstructed urban surfaces, we formulate the structural segmentation as a higher-order Conditional Random Field (CRF) labeling problem. It not only incorporates classical lower-order 2D and 3D local cues, but also encodes contextual geometric regularities to disambiguate the noisy local cues. A general higher-order CRF is difficult to solve. We develop a bottom-up progressive approach through a patch-based surface representation, which iteratively evolves from the initial mesh triangles to the final segmentation. Each iteration alternates between performing a prior discovery step, which finds the contextual regularities of the patch-based representation, and an inference step that leverages the regularities as higher-order priors to construct a more stable and regular segmentation. The efficiency and robustness of the proposed method is extensively demonstrated on real reconstruction models, yielding significantly better performance than classical mesh segmentation methods.},
  file = {/Users/sunjiaming/Zotero/storage/5YCUDV6D/7410599.html}
}

@article{liuIntriguingFailingConvolutional2018,
  title = {An {{Intriguing Failing}} of {{Convolutional Neural Networks}} and the {{CoordConv Solution}}},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  year = {2018},
  month = jul,
  abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24\% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
  archivePrefix = {arXiv},
  eprint = {1807.03247},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/An Intriguing Failing of Convolutional Neural Networks and the CoordConv-Liu et al-2018.pdf;/Users/sunjiaming/Zotero/storage/Y7MWD6JA/1807.html},
  journal = {arXiv:1807.03247 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{liuKeyPoseMultiview3D2019,
  title = {{{KeyPose}}: {{Multi}}-View {{3D Labeling}} and {{Keypoint Estimation}} for {{Transparent Objects}}},
  shorttitle = {{{KeyPose}}},
  author = {Liu, Xingyu and Jonschkowski, Rico and Angelova, Anelia and Konolige, Kurt},
  year = {2019},
  month = dec,
  abstract = {Estimating the 3D pose of desktop objects is crucial for applications such as robotic manipulation. Finding the depth of the object is an important part of this task, both for training and prediction, and is usually accomplished with a depth sensor or markers in a motion-capture system. For transparent or highly reflective objects, such methods are not feasible without impinging on the resultant image of the object. Hence, many existing methods restrict themselves to opaque, lambertian objects that give good returns from RGBD sensors. In this paper we address two problems: first, establish an easy method for capturing and labeling 3D keypoints on desktop objects with a stereo sensor (no special depth sensor required); and second, develop a deep method, called \$KeyPose\$, that learns to accurately predict 3D keypoints on objects, including challenging ones such as transparent objects. To showcase the performance of the method, we create and employ a dataset of 15 clear objects in 5 classes, with 48k 3D-keypoint labeled images. We train both instance and category models, and show generalization to new textures, poses, and objects. KeyPose surpasses state-of-the-art performance in 3D pose estimation on this dataset, sometimes by a wide margin, and even in cases where the competing method is provided with registered depth. We will release a public version of the data capture and labeling pipeline, the transparent object database, and the KeyPose training and evaluation code.},
  archivePrefix = {arXiv},
  eprint = {1912.02805},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/KeyPose-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RSAWSE9A/1912.html},
  journal = {arXiv:1912.02805 [cs]},
  primaryClass = {cs}
}

@article{liuLearningAnalogyReliable2020,
  title = {Learning by {{Analogy}}: {{Reliable Supervision}} from {{Transformations}} for {{Unsupervised Optical Flow Estimation}}},
  shorttitle = {Learning by {{Analogy}}},
  author = {Liu, Liang and Zhang, Jiangning and He, Ruifei and Liu, Yong and Wang, Yabiao and Tai, Ying and Luo, Donghao and Wang, Chengjie and Li, Jilin and Huang, Feiyue},
  year = {2020},
  month = mar,
  abstract = {Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.},
  archivePrefix = {arXiv},
  eprint = {2003.13045},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning by Analogy-Liu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/HV957NGI/2003.html},
  journal = {arXiv:2003.13045 [cs]},
  primaryClass = {cs}
}

@article{liuLearningInferImplicit2019,
  title = {Learning to {{Infer Implicit Surfaces}} without {{3D Supervision}}},
  author = {Liu, Shichen and Saito, Shunsuke and Chen, Weikai and Li, Hao},
  year = {2019},
  month = nov,
  abstract = {Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.},
  archivePrefix = {arXiv},
  eprint = {1911.00767},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Infer Implicit Surfaces without 3D Supervision-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/ETEIYJJE/1911.html},
  journal = {arXiv:1911.00767 [cs]},
  primaryClass = {cs}
}

@article{liuLearningSceneFlow2018,
  title = {Learning {{Scene Flow}} in {{3D Point Clouds}}},
  author = {Liu, Xingyu and Qi, Charles R. and Guibas, Leonidas J.},
  year = {2018},
  month = jun,
  abstract = {Many applications in robotics and human-computer interaction can benefit from an understanding of 3D motion of points in a dynamic environment, widely noted as scene flow. While most previous methods focus on solving the problem with stereo and RGB-D images, few try to estimate scene flow directly from point clouds. In this work, we propose a novel deep neural network named FlowNet3D that learns scene flow from point clouds in an end-to-end fashion. Our network simultaneously learns deep hierarchical point cloud features, flow embeddings as well as how to smooth the output. We evaluate the network on both challenging synthetic data and real LiDAR scans from KITTI. Trained on synthetic data only, our network is able to generalize well to real scans. Benefited from learning directly in point clouds, our model achieved significantly more accurate scene flow results compared with various baselines on stereo images and RGB-D images.},
  archivePrefix = {arXiv},
  eprint = {1806.01411},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Scene Flow in 3D Point Clouds-Liu et al-2018.pdf},
  journal = {arXiv:1806.01411 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liuLearningVideoRepresentations2019a,
  title = {Learning {{Video Representations}} from {{Correspondence Proposals}}},
  author = {Liu, Xingyu and Lee, Joon-Young and Jin, Hailin},
  year = {2019},
  month = may,
  abstract = {Correspondences between frames encode rich information about dynamic content in videos. However, it is challenging to effectively capture and learn those due to their irregular structure and complex dynamics. In this paper, we propose a novel neural network that learns video representations by aggregating information from potential correspondences. This network, named CPNet, can learn evolving 2D fields with temporal consistency. In particular, it can effectively learn representations for videos by mixing appearance and long-range motion with an RGB-only input. We provide extensive ablation experiments to validate our model. CPNet shows stronger performance than existing methods on Kinetics and achieves the state-of-the-art performance on Something-Something and Jester. We provide analysis towards the behavior of our model and show its robustness to errors in proposals.},
  archivePrefix = {arXiv},
  eprint = {1905.07853},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/DHXDUGQR/Liu et al. - 2019 - Learning Video Representations from Correspondence.pdf},
  journal = {arXiv:1905.07853 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liuLookingFastSlow2019,
  title = {Looking {{Fast}} and {{Slow}}: {{Memory}}-{{Guided Mobile Video Object Detection}}},
  shorttitle = {Looking {{Fast}} and {{Slow}}},
  author = {Liu, Mason and Zhu, Menglong and White, Marie and Li, Yinxiao and Kalenichenko, Dmitry},
  year = {2019},
  month = mar,
  abstract = {With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the "gist" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.},
  archivePrefix = {arXiv},
  eprint = {1903.10172},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Looking Fast and Slow-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/F3C55N39/1903.html},
  journal = {arXiv:1903.10172 [cs]},
  primaryClass = {cs}
}

@article{liuMeteorNetDeepLearning2019,
  title = {{{MeteorNet}}: {{Deep Learning}} on {{Dynamic 3D Point Cloud Sequences}}},
  shorttitle = {{{MeteorNet}}},
  author = {Liu, Xingyu and Yan, Mengyuan and Bohg, Jeannette},
  year = {2019},
  month = oct,
  abstract = {Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called \$MeteorNet\$ for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.},
  archivePrefix = {arXiv},
  eprint = {1910.09165},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MeteorNet-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QYNU9GHA/1910.html},
  journal = {arXiv:1910.09165 [cs]},
  primaryClass = {cs}
}

@article{liuNeuralRGBSensing2019,
  title = {Neural {{RGB}}-{$>$}{{D Sensing}}: {{Depth}} and {{Uncertainty}} from a {{Video Camera}}},
  shorttitle = {Neural {{RGB}}-{$>$}{{D Sensing}}},
  author = {Liu, Chao and Gu, Jinwei and Kim, Kihwan and Narasimhan, Srinivasa and Kautz, Jan},
  year = {2019},
  month = jan,
  abstract = {Depth sensing is crucial for 3D reconstruction and scene understanding. Active depth sensors provide dense metric measurements, but often suffer from limitations such as restricted operating ranges, low spatial resolution, sensor interference, and high power consumption. In this paper, we propose a deep learning (DL) method to estimate per-pixel depth and its uncertainty continuously from a monocular video stream, with the goal of effectively turning an RGB camera into an RGB-D camera. Unlike prior DL-based methods, we estimate a depth probability distribution for each pixel rather than a single depth value, leading to an estimate of a 3D depth probability volume for each input frame. These depth probability volumes are accumulated over time under a Bayesian filtering framework as more incoming frames are processed sequentially, which effectively reduces depth uncertainty and improves accuracy, robustness, and temporal stability. Compared to prior work, the proposed approach achieves more accurate and stable results, and generalizes better to new datasets. Experimental results also show the output of our approach can be directly fed into classical RGB-D based 3D scanning methods for 3D scene reconstruction.},
  archivePrefix = {arXiv},
  eprint = {1901.02571},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural RGB-D Sensing-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QY6X9RIB/1901.html},
  journal = {arXiv:1901.02571 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{liuNeuralSparseVoxel2020,
  title = {Neural {{Sparse Voxel Fields}}},
  author = {Liu, Lingjie and Gu, Jiatao and Lin, Kyaw Zaw and Chua, Tat-Seng and Theobalt, Christian},
  year = {2020},
  month = jul,
  abstract = {Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is over 10 times faster than the state-of-the-art (namely, NeRF) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering.},
  archivePrefix = {arXiv},
  eprint = {2007.11571},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Sparse Voxel Fields-Liu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/7XMYPZ6N/2007.html},
  journal = {arXiv:2007.11571 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{liuPlaneRCNN3DPlane2018,
  title = {{{PlaneRCNN}}: {{3D Plane Detection}} and {{Reconstruction}} from a {{Single Image}}},
  shorttitle = {{{PlaneRCNN}}},
  author = {Liu, Chen and Kim, Kihwan and Gu, Jinwei and Furukawa, Yasutaka and Kautz, Jan},
  year = {2018},
  month = dec,
  abstract = {This paper proposes a deep neural architecture, PlaneRCNN, that detects and reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN employs a variant of Mask R-CNN to detect planes with their plane parameters and segmentation masks. PlaneRCNN then jointly refines all the segmentation masks with a novel loss enforcing the consistency with a nearby view during training. The paper also presents a new benchmark with more finegrained plane segmentations in the ground-truth, in which, PlaneRCNN outperforms existing state-of-the-art methods with significant margins in the plane detection, segmentation, and reconstruction metrics. PlaneRCNN makes an important step towards robust plane extraction, which would have an immediate impact on a wide range of applications including Robotics, Augmented Reality, and Virtual Reality.},
  archivePrefix = {arXiv},
  eprint = {1812.04072},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/PlaneRCNN-Liu et al-2018.pdf},
  journal = {arXiv:1812.04072 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{liuPointVoxelCNNEfficient2019,
  title = {Point-{{Voxel CNN}} for {{Efficient 3D Deep Learning}}},
  author = {Liu, Zhijian and Tang, Haotian and Lin, Yujun and Han, Song},
  year = {2019},
  month = jul,
  abstract = {We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80\% of the time is wasted on structuring the irregular data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to largely reduce the irregular data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of our PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4\% mAP on average with 1.5x measured speedup and GPU memory reduction.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Point-Voxel CNN for Efficient 3D Deep Learning-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/3TSI8I6L/1907.html},
  language = {en}
}

@article{liuSelFlowSelfSupervisedLearning2019,
  title = {{{SelFlow}}: {{Self}}-{{Supervised Learning}} of {{Optical Flow}}},
  shorttitle = {{{SelFlow}}},
  author = {Liu, Pengpeng and Lyu, Michael and King, Irwin and Xu, Jia},
  year = {2019},
  month = apr,
  abstract = {We present a self-supervised learning approach for optical flow. Our method distills reliable flow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical flow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better flow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical flow learning on the challenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. Our fine-tuned models achieve state-of-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.},
  archivePrefix = {arXiv},
  eprint = {1904.09117},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SelFlow-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MC6DAAKK/1904.html},
  journal = {arXiv:1904.09117 [cs]},
  primaryClass = {cs}
}

@article{liuSemanticCorrespondenceOptimal,
  title = {Semantic {{Correspondence}} as an {{Optimal Transport Problem}}},
  author = {Liu, Yanbin and Zhu, Linchao and Yamada, Makoto and Yang, Yi},
  pages = {10},
  abstract = {Establishing dense correspondences across semantically similar images is a challenging task. Due to the large intra-class variation and background clutter, two common issues occur in current approaches. First, many pixels in a source image are assigned to one target pixel, i.e., many to one matching. Second, some object pixels are assigned to the background pixels, i.e., background matching. We solve the first issue by global feature matching, which maximizes the total matching correlations between images to obtain a global optimal matching matrix. The row sum and column sum constraints are enforced on the matching matrix to induce a balanced solution, thus suppressing many to one matching. We solve the second issue by applying a staircase function on the class activation maps to re-weight the importance of pixels into four levels from foreground to background. The whole procedure is combined into a unified optimal transport algorithm by converting the maximization problem to the optimal transport formulation and incorporating the staircase weights into optimal transport algorithm to act as empirical distributions. The proposed algorithm achieves state-of-the-art performance on four benchmark datasets. Notably, a 26\% relative improvement is achieved on the large-scale SPair-71k dataset.},
  file = {/Users/sunjiaming/Zotero/storage/45CW8375/Liu et al. - Semantic Correspondence as an Optimal Transport Pr.pdf},
  language = {en}
}

@article{liuShapeAdaptorLearnable2020,
  title = {Shape {{Adaptor}}: {{A Learnable Resizing Module}}},
  shorttitle = {Shape {{Adaptor}}},
  author = {Liu, Shikun and Lin, Zhe and Wang, Yilin and Zhang, Jianming and Perazzi, Federico and Johns, Edward},
  year = {2020},
  month = aug,
  abstract = {We present a novel resizing module for neural networks: shape adaptor, a drop-in enhancement built on top of traditional resizing layers, such as pooling, bilinear sampling, and strided convolution. Whilst traditional resizing layers have fixed and deterministic reshaping factors, our module allows for a learnable reshaping factor. Our implementation enables shape adaptors to be trained end-to-end without any additional supervision, through which network architectures can be optimised for each individual task, in a fully automated way. We performed experiments across seven image classification datasets, and results show that by simply using a set of our shape adaptors instead of the original resizing layers, performance increases consistently over human-designed networks, across all datasets. Additionally, we show the effectiveness of shape adaptors on two other applications: network compression and transfer learning. The source code is available at: https://github.com/lorenmt/shape-adaptor.},
  archivePrefix = {arXiv},
  eprint = {2008.00892},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Shape Adaptor-Liu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2ELP2WF3/2008.html},
  journal = {arXiv:2008.00892 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{liuSIFTFlowDense,
  title = {{{SIFT Flow}}: {{Dense Correspondence}} across {{Scenes}} and Its {{Applications}}},
  author = {Liu, Ce and Yuen, Jenny},
  pages = {17},
  abstract = {While image alignment has been studied in different areas of computer vision for decades, aligning images depicting different scenes remains a challenging problem. Analogous to optical flow where an image is aligned to its temporally adjacent frame, we propose SIFT flow, a method to align an image to its nearest neighbors in a large image corpus containing a variety of scenes. The SIFT flow algorithm consists of matching densely sampled, pixel-wise SIFT features between two images, while preserving spatial discontinuities. The SIFT features allow robust matching across different scene/object appearances, whereas the discontinuitypreserving spatial model allows matching of objects located at different parts of the scene. Experiments show that the proposed approach robustly aligns complex scene pairs containing significant spatial differences. Based on SIFT flow, we propose an alignmentbased large database framework for image analysis and synthesis, where image information is transferred from the nearest neighbors to a query image according to the dense scene correspondence. This framework is demonstrated through concrete applications, such as motion field prediction from a single image, motion synthesis via object transfer, satellite image registration and face recognition.},
  file = {/Users/sunjiaming/Zotero/storage/GJCYKJPS/Liu and Yuen - SIFT Flow Dense Correspondence across Scenes and .pdf},
  language = {en}
}

@article{liUSIPUnsupervisedStable2019,
  title = {{{USIP}}: {{Unsupervised Stable Interest Point Detection}} from {{3D Point Clouds}}},
  shorttitle = {{{USIP}}},
  author = {Li, Jiaxin and Lee, Gim Hee},
  year = {2019},
  month = mar,
  abstract = {In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis of our USIP detector and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP},
  archivePrefix = {arXiv},
  eprint = {1904.00229},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/USIP-Li_Lee-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/USIP-Li_Lee-22.pdf;/Users/sunjiaming/Zotero/storage/JJYRI73K/1904.html;/Users/sunjiaming/Zotero/storage/SDJRZ7Z4/1904.html},
  journal = {arXiv:1904.00229 [cs]},
  keywords = {3d keypoint},
  primaryClass = {cs}
}

@article{liuSoftRasterizerDifferentiable2019,
  title = {Soft {{Rasterizer}}: {{Differentiable Rendering}} for {{Unsupervised Single}}-{{View Mesh Reconstruction}}},
  shorttitle = {Soft {{Rasterizer}}},
  author = {Liu, Shichen and Chen, Weikai and Li, Tianye and Li, Hao},
  year = {2019},
  month = jan,
  abstract = {Rendering is the process of generating 2D images from 3D assets, simulated in a virtual environment, typically with a graphics pipeline. By inverting such renderer, one can think of a learning approach to predict a 3D shape from an input image. However, standard rendering pipelines involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence suitable for learning. We present the first non-parametric and truly differentiable rasterizer based on silhouettes. Our method enables unsupervised learning for high-quality 3D mesh reconstruction from a single image. We call our framework `soft rasterizer' as it provides an accurate soft approximation of the standard rasterizer. The key idea is to fuse the probabilistic contributions of all mesh triangles with respect to the rendered pixels. When combined with a mesh generator in a deep neural network, our soft rasterizer is able to generate an approximated silhouette of the generated polygon mesh in the forward pass. The rendering loss is back-propagated to supervise the mesh generation without the need of 3D training data. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art unsupervised techniques, both quantitatively and qualitatively. We also show that our soft rasterizer can achieve comparable results to the cutting-edge supervised learning method and in various cases even better ones, especially for real-world data.},
  archivePrefix = {arXiv},
  eprint = {1901.05567},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Soft Rasterizer-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XHVAW9U4/1901.html},
  journal = {arXiv:1901.05567 [cs]},
  primaryClass = {cs}
}

@article{liuSoftRasterizerDifferentiable2019a,
  title = {Soft {{Rasterizer}}: {{A Differentiable Renderer}} for {{Image}}-Based {{3D Reasoning}}},
  shorttitle = {Soft {{Rasterizer}}},
  author = {Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao},
  year = {2019},
  month = apr,
  abstract = {Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers.},
  archivePrefix = {arXiv},
  eprint = {1904.01786},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Soft Rasterizer-Liu et al-22.pdf;/Users/sunjiaming/Zotero/storage/BQJ8KCZG/1904.html},
  journal = {arXiv:1904.01786 [cs]},
  primaryClass = {cs}
}

@article{liuTransposerUniversalTexture2020,
  title = {Transposer: {{Universal Texture Synthesis Using Feature Maps}} as {{Transposed Convolution Filter}}},
  shorttitle = {Transposer},
  author = {Liu, Guilin and Taori, Rohan and Wang, Ting-Chun and Yu, Zhiding and Liu, Shiqiu and Reda, Fitsum A. and Sapra, Karan and Tao, Andrew and Catanzaro, Bryan},
  year = {2020},
  month = jul,
  abstract = {Conventional CNNs for texture synthesis consist of a sequence of (de)-convolution and up/down-sampling layers, where each layer operates locally and lacks the ability to capture the long-term structural dependency required by texture synthesis. Thus, they often simply enlarge the input texture, rather than perform reasonable synthesis. As a compromise, many recent methods sacrifice generalizability by training and testing on the same single (or fixed set of) texture image(s), resulting in huge re-training time costs for unseen images. In this work, based on the discovery that the assembling/stitching operation in traditional texture synthesis is analogous to a transposed convolution operation, we propose a novel way of using transposed convolution operation. Specifically, we directly treat the whole encoded feature map of the input texture as transposed convolution filters and the features' self-similarity map, which captures the auto-correlation information, as input to the transposed convolution. Such a design allows our framework, once trained, to be generalizable to perform synthesis of unseen textures with a single forward pass in nearly real-time. Our method achieves state-of-the-art texture synthesis quality based on various metrics. While self-similarity helps preserve the input textures' regular structural patterns, our framework can also take random noise maps for irregular input textures instead of self-similarity maps as transposed convolution inputs. It allows to get more diverse results as well as generate arbitrarily large texture outputs by directly sampling large noise maps in a single pass as well.},
  archivePrefix = {arXiv},
  eprint = {2007.07243},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Transposer-Liu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RED3KC87/2007.html},
  journal = {arXiv:2007.07243 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{liuVarianceAdaptiveLearning2019,
  title = {On the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  year = {2019},
  month = aug,
  abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
  archivePrefix = {arXiv},
  eprint = {1908.03265},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On the Variance of the Adaptive Learning Rate and Beyond-Liu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/W6M92Z7V/1908.html},
  journal = {arXiv:1908.03265 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{lizhangGlobalDataAssociation2008,
  ids = {lizhangGlobalDataAssociation2008a},
  title = {Global Data Association for Multi-Object Tracking Using Network Flows},
  booktitle = {2008 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Li Zhang} and {Yuan Li} and Nevatia, Ramakant},
  year = {2008},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/CVPR.2008.4587584},
  abstract = {We propose a network flow based optimization method for data association needed for multiple object tracking. The maximum-a-posteriori (MAP) data association problem is mapped into a cost-flow network with a non-overlap constraint on trajectories. The optimal data association is found by a min-cost flow algorithm in the network. The network is augmented to include an Explicit Occlusion Model(EOM) to track with long-term inter-object occlusions. A solution to the EOM-based network is found by an iterative approach built upon the original algorithm. Initialization and termination of trajectories and potential false observations are modeled by the formulation intrinsically. The method is efficient and does not require hypotheses pruning. Performance is compared with previous results on two public pedestrian datasets to show its improvement.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Global data association for multi-object tracking using network flows-Li Zhang et al-2008.pdf;/Users/sunjiaming/Zotero/storage/B84RGSP3/Li Zhang et al. - 2008 - Global data association for multi-object tracking .pdf},
  isbn = {978-1-4244-2242-5},
  language = {en}
}

@article{locatelloObjectCentricLearningSlot2020,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  month = oct,
  abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
  archivePrefix = {arXiv},
  eprint = {2006.15055},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object-Centric Learning with Slot Attention-Locatello et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FIZX8IV3/2006.html},
  journal = {arXiv:2006.15055 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{lombardiNeuralVolumesLearning2019,
  title = {Neural {{Volumes}}: {{Learning Dynamic Renderable Volumes}} from {{Images}}},
  shorttitle = {Neural {{Volumes}}},
  author = {Lombardi, Stephen and Simon, Tomas and Saragih, Jason and Schwartz, Gabriel and Lehrmann, Andreas and Sheikh, Yaser},
  year = {2019},
  month = jul,
  volume = {38},
  pages = {1--14},
  issn = {07300301},
  doi = {10.1145/3306346.3323020},
  abstract = {Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.},
  archivePrefix = {arXiv},
  eprint = {1906.07751},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Volumes-Lombardi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QGQY2TYN/1906.html},
  journal = {ACM Transactions on Graphics},
  number = {4}
}

@article{longOcclusionAwareDepthEstimation2020,
  title = {Occlusion-{{Aware Depth Estimation}} with {{Adaptive Normal Constraints}}},
  author = {Long, Xiaoxiao and Liu, Lingjie and Theobalt, Christian and Wang, Wenping},
  year = {2020},
  month = jul,
  abstract = {We present a new learning-based method for multi-frame depth estimation from a color video, which is a fundamental problem in scene understanding, robot navigation or handheld 3D reconstruction. While recent learning-based methods estimate depth at high accuracy, 3D point clouds exported from their depth maps often fail to preserve important geometric feature (e.g., corners, edges, planes) of man-made scenes. Widely-used pixel-wise depth errors do not specifically penalize inconsistency on these features. These inaccuracies are particularly severe when subsequent depth reconstructions are accumulated in an attempt to scan a full environment with man-made objects with this kind of features. Our depth estimation algorithm therefore introduces a Combined Normal Map (CNM) constraint, which is designed to better preserve high-curvature features and global planar regions. In order to further improve the depth estimation accuracy, we introduce a new occlusion-aware strategy that aggregates initial depth predictions from multiple adjacent views into one final depth map and one occlusion probability map for the current reference view. Our method outperforms the state-of-the-art in terms of depth estimation accuracy, and preserves essential geometric features of man-made indoor scenes much better than other algorithms.},
  archivePrefix = {arXiv},
  eprint = {2004.00845},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Occlusion-Aware Depth Estimation with Adaptive Normal Constraints-Long et al-2020.pdf;/Users/sunjiaming/Zotero/storage/WMJB4FYT/2004.html},
  journal = {arXiv:2004.00845 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper},
  primaryClass = {cs}
}

@article{lorensenMarchingCubesHigh1987,
  title = {Marching Cubes: {{A}} High Resolution {{3D}} Surface Construction Algorithm},
  shorttitle = {Marching Cubes},
  author = {Lorensen, William E. and Cline, Harvey E.},
  year = {1987},
  month = aug,
  volume = {21},
  pages = {163--169},
  issn = {0097-8930},
  doi = {10.1145/37402.37422},
  abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
  journal = {ACM SIGGRAPH Computer Graphics},
  keywords = {neufu_paper},
  number = {4}
}

@article{lorenzUnsupervisedPartBasedDisentangling2019,
  title = {Unsupervised {{Part}}-{{Based Disentangling}} of {{Object Shape}} and {{Appearance}}},
  author = {Lorenz, Dominik and Bereska, Leonard and Milbich, Timo and Ommer, Bj{\"o}rn},
  year = {2019},
  month = mar,
  abstract = {Large intra-class variation is the result of changes in multiple object characteristics. Images, however, only show the superposition of different variable factors such as appearance or shape. Therefore, learning to disentangle and represent these different characteristics poses a great challenge, especially in the unsupervised case. Moreover, large object articulation calls for a flexible part-based model. We present an unsupervised approach for disentangling appearance and shape by learning parts consistently over all instances of a category. Our model for learning an object representation is trained by simultaneously exploiting invariance and equivariance constraints between synthetically transformed images. Since no part annotation or prior information on an object class is required, the approach is applicable to arbitrary classes. We evaluate our approach on a wide range of object categories and diverse tasks including pose prediction, disentangled image synthesis, and video-to-video translation. The approach outperforms the state-of-the-art on unsupervised keypoint prediction and compares favorably even against supervised approaches on the task of shape and appearance transfer.},
  archivePrefix = {arXiv},
  eprint = {1903.06946},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Lorenz et al_2019_Unsupervised Part-Based Disentangling of Object Shape and Appearance.pdf;/Users/sunjiaming/Zotero/storage/H9XXHC4V/1903.html},
  journal = {arXiv:1903.06946 [cs]},
  primaryClass = {cs}
}

@article{loubetReparameterizingDiscontinuousIntegrands2019,
  title = {Reparameterizing Discontinuous Integrands for Differentiable Rendering},
  author = {Loubet, Guillaume and Holzschuch, Nicolas and Jakob, Wenzel},
  year = {2019},
  month = nov,
  volume = {38},
  pages = {1--14},
  issn = {07300301},
  doi = {10.1145/3355089.3356510},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reparameterizing discontinuous integrands for differentiable rendering-Loubet et al-2019.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{luDeepICPEndtoEndDeep2019,
  title = {{{DeepICP}}: {{An End}}-to-{{End Deep Neural Network}} for {{3D Point Cloud Registration}}},
  shorttitle = {{{DeepICP}}},
  author = {Lu, Weixin and Wan, Guowei and Zhou, Yao and Fu, Xiangyu and Yuan, Pengfei and Song, Shiyu},
  year = {2019},
  month = may,
  abstract = {We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task.},
  archivePrefix = {arXiv},
  eprint = {1905.04153},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepICP-Lu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/GUWRR89M/1905.html},
  journal = {arXiv:1905.04153 [cs]},
  primaryClass = {cs}
}

@article{luGridRCNN2018,
  title = {Grid {{R}}-{{CNN}}},
  author = {Lu, Xin and Li, Buyu and Yue, Yuxin and Li, Quanquan and Yan, Junjie},
  year = {2018},
  month = nov,
  abstract = {This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1\% AP gain at IoU=0.8 and a 10.0\% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.},
  archivePrefix = {arXiv},
  eprint = {1811.12030},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Grid R-CNN-Lu et al-2018.pdf},
  journal = {arXiv:1811.12030 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{luitenHOTAHigherOrder2020,
  title = {{{HOTA}}: {{A Higher Order Metric}} for {{Evaluating Multi}}-{{Object Tracking}}},
  shorttitle = {{{HOTA}}},
  author = {Luiten, Jonathon and Osep, Aljosa and Dendorfer, Patrick and Torr, Philip and Geiger, Andreas and {Leal-Taixe}, Laura and Leibe, Bastian},
  year = {2020},
  month = oct,
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01375-2},
  abstract = {Multi-Object Tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, HOTA (Higher Order Tracking Accuracy), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.},
  archivePrefix = {arXiv},
  eprint = {2009.07736},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/7KT4L7BC/Luiten et al. - 2020 - HOTA A Higher Order Metric for Evaluating Multi-O.pdf;/Users/sunjiaming/Zotero/storage/I4U2J6K3/2009.html},
  journal = {International Journal of Computer Vision},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{luitenTrackReconstructReconstruct2019,
  title = {Track to {{Reconstruct}} and {{Reconstruct}} to {{Track}}},
  author = {Luiten, Jonathon and Fischer, Tobias and Leibe, Bastian},
  year = {2019},
  month = sep,
  abstract = {Object tracking and reconstruction are often performed together, with tracking used as input for 3D reconstrution. However, the obtained 3D reconstructions also provide useful information that can be exploited to improve tracking. In this paper, we propose a novel method that closes this loop, tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. For this, we build up short tracklets using the 2D motion consistency of segmentation masks under optical flow warping. These tracklets are then fused into dynamic 3D object reconstructions which define the precise 3D object motion. This 3D motion is used to merge tracklets into long-term tracks, even when objects are completely occluded for up to 20 frames, and to locate objects when detections are missing. On the KITTI dataset, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50\%. This large improvement in long-term tracking ability results in MOTSFusion outperforming previous approaches in both bounding box and segmentation mask tracking accuracy.},
  archivePrefix = {arXiv},
  eprint = {1910.00130},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Track to Reconstruct and Reconstruct to Track-Luiten et al-2019.pdf;/Users/sunjiaming/Zotero/storage/W8IXV6JA/1910.html},
  journal = {arXiv:1910.00130 [cs]},
  primaryClass = {cs}
}

@article{luL3NetLearningBased,
  title = {L3-{{Net}}: {{Towards Learning}} Based {{LiDAR Localization}} for {{Autonomous Driving}}},
  author = {Lu, Weixin and Zhou, Yao and Wan, Guowei and Hou, Shenhua and Song, Shiyu},
  pages = {11},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/L3-Net-Lu et al-.pdf},
  language = {en}
}

@article{luoASLFeatLearningLocal2020,
  title = {{{ASLFeat}}: {{Learning Local Features}} of {{Accurate Shape}} and {{Localization}}},
  shorttitle = {{{ASLFeat}}},
  author = {Luo, Zixin and Zhou, Lei and Bai, Xuyang and Chen, Hongkai and Zhang, Jiahui and Yao, Yao and Li, Shiwei and Fang, Tian and Quan, Long},
  year = {2020},
  month = mar,
  abstract = {This work focuses on mitigating two limitations in the joint learning of local feature detectors and descriptors. First, the ability to estimate the local shape (scale, orientation, etc.) of feature points is often neglected during dense feature extraction, while the shape-awareness is crucial to acquire stronger geometric invariance. Second, the localization accuracy of detected keypoints is not sufficient to reliably recover camera geometry, which has become the bottleneck in tasks such as 3D reconstruction. In this paper, we present ASLFeat, with three light-weight yet effective modifications to mitigate above issues. First, we resort to deformable convolutional networks to densely estimate and apply local transformation. Second, we take advantage of the inherent feature hierarchy to restore spatial resolution and low-level details for accurate keypoint localization. Finally, we use a peakiness measurement to relate feature responses and derive more indicative detection scores. The effect of each modification is thoroughly studied, and the evaluation is extensively conducted across a variety of practical scenarios. State-of-the-art results are reported that demonstrate the superiority of our methods.},
  archivePrefix = {arXiv},
  eprint = {2003.10071},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ASLFeat-Luo et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FHWSZ2VX/2003.html},
  journal = {arXiv:2003.10071 [cs]},
  primaryClass = {cs}
}

@article{luoConsistentVideoDepth2020,
  title = {Consistent {{Video Depth Estimation}}},
  author = {Luo, Xuan and Huang, Jia-Bin and Szeliski, Richard and Matzen, Kevin and Kopf, Johannes},
  year = {2020},
  month = apr,
  abstract = {We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.},
  archivePrefix = {arXiv},
  eprint = {2004.15021},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Consistent Video Depth Estimation-Luo et al-2020.pdf;/Users/sunjiaming/Zotero/storage/6SE8KC6P/2004.html},
  journal = {arXiv:2004.15021 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{luoEveryPixelCounts2019,
  title = {Every {{Pixel Counts}} ++: {{Joint Learning}} of {{Geometry}} and {{Motion}} with {{3D Holistic Understanding}}},
  shorttitle = {Every {{Pixel Counts}} ++},
  author = {Luo, Chenxu and Yang, Zhenheng and Wang, Peng and Wang, Yang and Xu, Wei and Nevatia, Ram and Yuille, Alan},
  year = {2019},
  month = jul,
  abstract = {Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as "Every Pixel Counts++" or "EPC++". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (OptFlowNet) respectively. The three types of information are fed into a holistic 3D motion parser (HMP), and per-pixel 3D motion of both rigid background and moving objects are disentangled and recovered. Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset). Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods. Code will be available at: https://github.com/chenxuluo/EPC.},
  archivePrefix = {arXiv},
  eprint = {1810.06125},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Every Pixel Counts ++-Luo et al-2019.pdf;/Users/sunjiaming/Zotero/storage/N4A5NLSQ/1810.html},
  journal = {arXiv:1810.06125 [cs]},
  primaryClass = {cs}
}

@inproceedings{luoFastFuriousReal2018,
  title = {Fast and {{Furious}}: {{Real Time End}}-to-{{End 3D Detection}}, {{Tracking}} and {{Motion Forecasting}} with a {{Single Convolutional Net}}},
  shorttitle = {Fast and {{Furious}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Luo, Wenjie and Yang, Bin and Urtasun, Raquel},
  year = {2018},
  month = jun,
  pages = {3569--3577},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00376},
  abstract = {In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fast and Furious-Luo et al-22.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{luPerMOPerceivingMore2020,
  title = {{{PerMO}}: {{Perceiving More}} at {{Once}} from a {{Single Image}} for {{Autonomous Driving}}},
  shorttitle = {{{PerMO}}},
  author = {Lu, Feixiang and Liu, Zongdai and Song, Xibin and Zhou, Dingfu and Li, Wei and Miao, Hui and Liao, Miao and Zhang, Liangjun and Zhou, Bin and Yang, Ruigang and Manocha, Dinesh},
  year = {2020},
  month = jul,
  abstract = {We present a novel approach to detect, segment, and reconstruct complete textured 3D models of vehicles from a single image for autonomous driving. Our approach combines the strengths of deep learning and the elegance of traditional techniques from part-based deformable model representation to produce high-quality 3D models in the presence of severe occlusions. We present a new part-based deformable vehicle model that is used for instance segmentation and automatically generate a dataset that contains dense correspondences between 2D images and 3D models. We also present a novel end-to-end deep neural network to predict dense 2D/3D mapping and highlight its benefits. Based on the dense mapping, we are able to compute precise 6-DoF poses and 3D reconstruction results at almost interactive rates on a commodity GPU. We have integrated these algorithms with an autonomous driving system. In practice, our method outperforms the state-of-the-art methods for all major vehicle parsing tasks: 2D instance segmentation by 4.4 points (mAP), 6-DoF pose estimation by 9.11 points, and 3D detection by 1.37. Moreover, we have released all of the source code, dataset, and the trained model on Github.},
  archivePrefix = {arXiv},
  eprint = {2007.08116},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PerMO-Lu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/DHYJAT39/2007.html},
  journal = {arXiv:2007.08116 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{luRetinaTrackOnlineSingle2020,
  title = {{{RetinaTrack}}: {{Online Single Stage Joint Detection}} and {{Tracking}}},
  shorttitle = {{{RetinaTrack}}},
  author = {Lu, Zhichao and Rathod, Vivek and Votel, Ronny and Huang, Jonathan},
  year = {2020},
  month = mar,
  abstract = {Traditionally multi-object tracking and object detection are performed using separate systems with most prior works focusing exclusively on one of these aspects over the other. Tracking systems clearly benefit from having access to accurate detections, however and there is ample evidence in literature that detectors can benefit from tracking which, for example, can help to smooth predictions over time. In this paper we focus on the tracking-by-detection paradigm for autonomous driving where both tasks are mission critical. We propose a conceptually simple and efficient joint model of detection and tracking, called RetinaTrack, which modifies the popular single stage RetinaNet approach such that it is amenable to instance-level embedding training. We show, via evaluations on the Waymo Open Dataset, that we outperform a recent state of the art tracking algorithm while requiring significantly less computation. We believe that our simple yet effective approach can serve as a strong baseline for future work in this area.},
  archivePrefix = {arXiv},
  eprint = {2003.13870},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RetinaTrack-Lu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EGMK6N7C/2003.html},
  journal = {arXiv:2003.13870 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{lvTakingDeeperLook2018,
  title = {Taking a {{Deeper Look}} at the {{Inverse Compositional Algorithm}}},
  author = {Lv, Zhaoyang and Dellaert, Frank and Rehg, James M. and Geiger, Andreas},
  year = {2018},
  month = dec,
  abstract = {In this paper, we provide a modern synthesis of the classic inverse compositional algorithm for dense image alignment. We first discuss the assumptions made by this wellestablished technique, and subsequently propose to relax these assumptions by incorporating data-driven priors into this model. More specifically, we unroll a robust version of the inverse compositional algorithm and replace multiple components of this algorithm using more expressive models whose parameters we train in an end-to-end fashion from data. Our experiments on several challenging 3D rigid motion estimation tasks demonstrate the advantages of combining optimization with learning-based techniques, outperforming the classic inverse compositional algorithm as well as data-driven image-to-pose regression approaches.},
  archivePrefix = {arXiv},
  eprint = {1812.06861},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Taking a Deeper Look at the Inverse Compositional Algorithm-Lv et al-22.pdf},
  journal = {arXiv:1812.06861 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lynenLargescaleRealtimeVisualinertial2019,
  title = {Large-Scale, Real-Time Visual-Inertial Localization Revisited},
  author = {Lynen, Simon and Zeisl, Bernhard and Aiger, Dror and Bosse, Michael and Hesch, Joel and Pollefeys, Marc and Siegwart, Roland and Sattler, Torsten},
  year = {2019},
  month = jun,
  abstract = {The overarching goals in image-based localization are scale, robustness and speed. In recent years, approaches based on local features and sparse 3D point-cloud models have both dominated the benchmarks and seen successful realworld deployment. They enable applications ranging from robot navigation, autonomous driving, virtual and augmented reality to device geo-localization. Recently end-to-end learned localization approaches have been proposed which show promising results on small scale datasets. However the positioning accuracy, scalability, latency and compute \& storage requirements of these approaches remain open challenges. We aim to deploy localization at global-scale where one thus relies on methods using local features and sparse 3D models. Our approach spans from offline model building to real-time client-side pose fusion. The system compresses appearance and geometry of the scene for efficient model storage and lookup leading to scalability beyond what what has been previously demonstrated. It allows for low-latency localization queries and efficient fusion run in real-time on mobile platforms by combining server-side localization with real-time visual-inertial-based camera pose tracking. In order to further improve efficiency we leverage a combination of priors, nearest neighbor search, geometric match culling and a cascaded pose candidate refinement step. This combination outperforms previous approaches when working with large scale models and allows deployment at unprecedented scale. We demonstrate the effectiveness of our approach on a proof-of-concept system localizing 2.5 million images against models from four cities in different regions on the world achieving query latencies in the 200ms range.},
  archivePrefix = {arXiv},
  eprint = {1907.00338},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large-scale, real-time visual-inertial localization revisited-Lynen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/3J9RR2BS/1907.html},
  journal = {arXiv:1907.00338 [cs]},
  primaryClass = {cs}
}

@article{maAccurateMonocular3D2019,
  title = {Accurate {{Monocular 3D Object Detection}} via {{Color}}-{{Embedded 3D Reconstruction}} for {{Autonomous Driving}}},
  author = {Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Ouyang, Wanli and Zhang, Pengbo},
  year = {2019},
  month = mar,
  abstract = {In this paper, we propose a monocular 3D object detection framework in the domain of autonomous driving. Unlike previous image-based methods which focus on RGB feature extracted from 2D images, our method solves this problem in the reconstructed 3D space in order to exploit 3D contexts explicitly. To this end, we first leverage a stand-alone module to transform the input data from 2D image plane to 3D point clouds space for a better input representation, then we perform the 3D detection using PointNet backbone net to obtain objects 3D locations, dimensions and orientations. To enhance the discriminative capability of point clouds, we propose a multi-modal feature fusion module to embed the complementary RGB cue into the generated point clouds representation. We argue that it is more effective to infer the 3D bounding boxes from the generated 3D scene space (i.e., X,Y, Z space) compared to the image plane (i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows that our approach boosts the performance of state-of-the-art monocular approach by a large margin, i.e., around 15\% absolute AP on both 3D localization and detection tasks for Car category at 0.7 IoU threshold.},
  archivePrefix = {arXiv},
  eprint = {1903.11444},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for-Ma et al-2019.pdf;/Users/sunjiaming/Zotero/storage/M7QMTMU8/1903.html},
  journal = {arXiv:1903.11444 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@misc{MachineLearningPhD2018,
  title = {Machine {{Learning PhD Applications}} \textemdash{} {{Everything You Need}} to {{Know}}},
  year = {2018},
  month = nov,
  abstract = {This blog post explains how to proceed in your PhD applications from A to Z and how to get admitted to top school in deep learning and machine learning.},
  file = {/Users/sunjiaming/Zotero/storage/KJPM7PBY/phd-applications.html},
  journal = {Tim Dettmers},
  language = {en-US}
}

@article{maDeepRigidInstance2019,
  title = {Deep {{Rigid Instance Scene Flow}}},
  author = {Ma, Wei-Chiu and Wang, Shenlong and Hu, Rui and Xiong, Yuwen and Urtasun, Raquel},
  year = {2019},
  month = apr,
  abstract = {In this paper we tackle the problem of scene flow estimation in the context of self-driving. We leverage deep learning techniques as well as strong priors as in our application domain the motion of the scene can be composed by the motion of the robot and the 3D motion of the actors in the scene. We formulate the problem as energy minimization in a deep structured model, which can be solved efficiently in the GPU by unrolling a Gaussian-Newton solver. Our experiments in the challenging KITTI scene flow dataset show that we outperform the state-of-the-art by a very large margin, while being 800 times faster.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Rigid Instance Scene Flow-Ma et al-2019.pdf},
  language = {en}
}

@inproceedings{maierIntrinsic3DHighQuality3D2017,
  title = {{{Intrinsic3D}}: {{High}}-{{Quality 3D Reconstruction}} by {{Joint Appearance}} and {{Geometry Optimization}} with {{Spatially}}-{{Varying Lighting}}},
  shorttitle = {{{Intrinsic3D}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Maier, Robert and Kim, Kihwan and Cremers, Daniel and Kautz, Jan and NieBner, Matthias},
  year = {2017},
  month = oct,
  pages = {3133--3141},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.338},
  abstract = {We introduce a novel method to obtain high-quality 3D reconstructions from consumer RGB-D sensors. Our core idea is to simultaneously optimize for geometry encoded in a signed distance field (SDF), textures from automaticallyselected keyframes, and their camera poses along with material and scene lighting. To this end, we propose a joint surface reconstruction approach that is based on Shapefrom-Shading (SfS) techniques and utilizes the estimation of spatially-varying spherical harmonics (SVSH) from subvolumes of the reconstructed scene. Through extensive examples and evaluations, we demonstrate that our method dramatically increases the level of detail in the reconstructed scene geometry and contributes highly to consistent surface texture recovery.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Intrinsic3D-Maier et al-2017.pdf},
  isbn = {978-1-5386-1032-9},
  keywords = {reconstruction},
  language = {en}
}

@article{maImageMatchingHandcrafted2020,
  title = {Image {{Matching}} from {{Handcrafted}} to {{Deep Features}}: {{A Survey}}},
  shorttitle = {Image {{Matching}} from {{Handcrafted}} to {{Deep Features}}},
  author = {Ma, Jiayi and Jiang, Xingyu and Fan, Aoxiang and Jiang, Junjun and Yan, Junchi},
  year = {2020},
  month = aug,
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01359-2},
  abstract = {As a fundamental and critical task in various visual applications, image matching can identify then correspond the same or similar structure/content from two or more images. Over the past decades, growing amount and diversity of methods have been proposed for image matching, particularly with the development of deep learning techniques over the recent years. However, it may leave several open questions about which method would be a suitable choice for specific applications with respect to different scenarios and task requirements and how to design better image matching methods with superior performance in accuracy, robustness and efficiency. This encourages us to conduct a comprehensive and systematic review and analysis for those classical and latest techniques. Following the feature-based image matching pipeline, we first introduce feature detection, description, and matching techniques from handcrafted methods to trainable ones and provide an analysis of the development of these methods in theory and practice. Secondly, we briefly introduce several typical image matching-based applications for a comprehensive understanding of the significance of image matching. In addition, we also provide a comprehensive and objective comparison of these classical and latest techniques through extensive experiments on representative datasets. Finally, we conclude with the current status of image matching technologies and deliver insightful discussions and prospects for future works. This survey can serve as a reference for (but not limited to) researchers and engineers in image matching and related fields.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Image Matching from Handcrafted to Deep Features-Ma et al-2020.pdf},
  journal = {International Journal of Computer Vision},
  language = {en}
}

@article{maImageMatchingHandcrafted2020a,
  title = {Image {{Matching}} from {{Handcrafted}} to {{Deep Features}}: {{A Survey}}},
  shorttitle = {Image {{Matching}} from {{Handcrafted}} to {{Deep Features}}},
  author = {Ma, Jiayi and Jiang, Xingyu and Fan, Aoxiang and Jiang, Junjun and Yan, Junchi},
  year = {2020},
  month = aug,
  issn = {1573-1405},
  doi = {10.1007/s11263-020-01359-2},
  abstract = {As a fundamental and critical task in various visual applications, image matching can identify then correspond the same or similar structure/content from two or more images. Over the past decades, growing amount and diversity of methods have been proposed for image matching, particularly with the development of deep learning techniques over the recent years. However, it may leave several open questions about which method would be a suitable choice for specific applications with respect to different scenarios and task requirements and how to design better image matching methods with superior performance in accuracy, robustness and efficiency. This encourages us to conduct a comprehensive and systematic review and analysis for those classical and latest techniques. Following the feature-based image matching pipeline, we first introduce feature detection, description, and matching techniques from handcrafted methods to trainable ones and provide an analysis of the development of these methods in theory and practice. Secondly, we briefly introduce several typical image matching-based applications for a comprehensive understanding of the significance of image matching. In addition, we also provide a comprehensive and objective comparison of these classical and latest techniques through extensive experiments on representative datasets. Finally, we conclude with the current status of image matching technologies and deliver insightful discussions and prospects for future works. This survey can serve as a reference for (but not limited to) researchers and engineers in image matching and related fields.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Image Matching from Handcrafted to Deep Features-Ma et al-22.pdf},
  journal = {International Journal of Computer Vision},
  language = {en}
}

@article{manGroundNetSegmentationAwareMonocular2018,
  title = {{{GroundNet}}: {{Segmentation}}-{{Aware Monocular Ground Plane Estimation}} with {{Geometric Consistency}}},
  shorttitle = {{{GroundNet}}},
  author = {Man, Yunze and Weng, Xinshuo and Kitani, Kris},
  year = {2018},
  month = nov,
  abstract = {We focus on the problem of estimating the orientation of the ground plane with respect to a mobile monocular camera platform (e.g., ground robot, wearable camera, assistive robotic platform). To address this problem, we formulate the ground plane estimation problem as an intermingled multi-task prediction problem by jointly optimizing for point-wise surface normal direction, 2D ground segmentation, and depth estimates. Our proposed model \textendash{} GroundNet \textendash{} estimates the ground normal in two streams separately and then a consistency loss is applied on top of the two streams to enforce geometric consistency. A semantic segmentation stream is used to isolate the ground regions and are used to selectively back-propagate parameter updates only through the ground regions in the image. Our experiments on KITTI and ApolloScape datasets verify that the GroundNet is able to predict consistent depth and normal within the ground region. It also achieves top performance on ground plane normal estimation and horizon line detection.},
  archivePrefix = {arXiv},
  eprint = {1811.07222},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/GroundNet-Man et al-2018.pdf},
  journal = {arXiv:1811.07222 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{manhardtCPSClasslevel6D2020,
  title = {{{CPS}}: {{Class}}-Level {{6D Pose}} and {{Shape Estimation From Monocular Images}}},
  shorttitle = {{{CPS}}},
  author = {Manhardt, Fabian and Nickel, Manuel and Meier, Sven and Minciullo, Luca and Navab, Nassir},
  year = {2020},
  month = mar,
  abstract = {Contemporary monocular 6D pose estimation methods can only cope with a handful of object instances. This naturally limits possible applications as, for instance, robots need to work with hundreds of different objects in a real environment. In this paper, we propose the first deep learning approach for class-wise monocular 6D pose estimation, coupled with metric shape retrieval. We propose a new loss formulation which directly optimizes over all parameters, i.e. 3D orientation, translation, scale and shape at the same time. Instead of decoupling each parameter, we transform the regressed shape, in the form of a point cloud, to 3D and directly measure its metric misalignment. We experimentally demonstrate that we can retrieve precise metric point clouds from a single image, which can also be further processed for e.g. subsequent rendering. Moreover, we show that our new 3D point cloud loss outperforms all baselines and gives overall good results despite the inherent ambiguity due to monocular data.},
  archivePrefix = {arXiv},
  eprint = {2003.05848},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CPS-Manhardt et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RQU7PPPE/2003.html},
  journal = {arXiv:2003.05848 [cs]},
  primaryClass = {cs}
}

@article{manhardtROI10DMonocularLifting2018,
  title = {{{ROI}}-{{10D}}: {{Monocular Lifting}} of {{2D Detection}} to {{6D Pose}} and {{Metric Shape}}},
  shorttitle = {{{ROI}}-{{10D}}},
  author = {Manhardt, Fabian and Kehl, Wadim and Gaidon, Adrien},
  year = {2018},
  month = dec,
  abstract = {We present a deep learning method for end-to-end monocular 3D object detection and metric shape retrieval. We propose a novel loss formulation by lifting 2D detection, orientation, and scale estimation into 3D space. Instead of optimizing these quantities separately, the 3D instantiation allows to properly measure the metric misalignment of boxes. We experimentally show that our 10D lifting of sparse 2D Regions of Interests (RoIs) achieves great results both for 6D pose and recovery of the textured metric geometry of instances. This further enables 3D synthetic data augmentation via inpainting recovered meshes directly onto the 2D scenes. We evaluate on KITTI3D against other strong monocular methods and demonstrate that our approach doubles the AP on the 3D pose metrics on the official test set, defining the new state of the art.},
  archivePrefix = {arXiv},
  eprint = {1812.02781},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ROI-10D-Manhardt et al-2018.pdf;/Users/sunjiaming/Zotero/storage/343VDDEU/1812.html},
  journal = {arXiv:1812.02781 [cs]},
  primaryClass = {cs}
}

@article{manivasagamLiDARsimRealisticLiDAR2020,
  title = {{{LiDARsim}}: {{Realistic LiDAR Simulation}} by {{Leveraging}} the {{Real World}}},
  shorttitle = {{{LiDARsim}}},
  author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
  year = {2020},
  month = jun,
  abstract = {We tackle the problem of producing realistic simulations of LiDAR point clouds, the sensor of preference for most self-driving vehicles. We argue that, by leveraging real data, we can simulate the complex world more realistically compared to employing virtual worlds built from CAD/procedural models. Towards this goal, we first build a large catalog of 3D static maps and 3D dynamic objects by driving around several cities with our self-driving fleet. We can then generate scenarios by selecting a scene from our catalog and "virtually" placing the self-driving vehicle (SDV) and a set of dynamic objects from the catalog in plausible locations in the scene. To produce realistic simulations, we develop a novel simulator that captures both the power of physics-based and learning-based simulation. We first utilize ray casting over the 3D scene and then use a deep neural network to produce deviations from the physics-based simulation, producing realistic LiDAR point clouds. We showcase LiDARsim's usefulness for perception algorithms-testing on long-tail events and end-to-end closed-loop evaluation on safety-critical scenarios.},
  archivePrefix = {arXiv},
  eprint = {2006.09348},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LiDARsim-Manivasagam et al-2020.pdf;/Users/sunjiaming/Zotero/storage/Y5853QNU/2006.html},
  journal = {arXiv:2006.09348 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{maoDelayMetricVideo2019,
  title = {A {{Delay Metric}} for {{Video Object Detection}}: {{What Average Precision Fails}} to {{Tell}}},
  shorttitle = {A {{Delay Metric}} for {{Video Object Detection}}},
  author = {Mao, Huizi and Yang, Xiaodong and Dally, William J.},
  year = {2019},
  month = aug,
  abstract = {Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze object detection from videos and point out that AP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, average delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve AP well. In other words, AP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be additionally evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception.},
  archivePrefix = {arXiv},
  eprint = {1908.06368},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Delay Metric for Video Object Detection-Mao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AFYNMWQT/1908.html},
  journal = {arXiv:1908.06368 [cs]},
  primaryClass = {cs}
}

@article{maoInterpolatedConvolutionalNetworks2019,
  title = {Interpolated {{Convolutional Networks}} for {{3D Point Cloud Understanding}}},
  author = {Mao, Jiageng and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = aug,
  abstract = {Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.},
  archivePrefix = {arXiv},
  eprint = {1908.04512},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Interpolated Convolutional Networks for 3D Point Cloud Understanding-Mao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/5AEYD58G/1908.html},
  journal = {arXiv:1908.04512 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{maRapidAcquisitionSpecular,
  title = {Rapid {{Acquisition}} of {{Specular}} and {{Diffuse Normal Maps}} from {{Polarized Spherical Gradient Illumination}}},
  author = {Ma, Wan-Chun and Hawkins, Tim and Peers, Pieter and Chabert, Charles-Felix and Weiss, Malte and Debevec, Paul},
  pages = {12},
  abstract = {We estimate surface normal maps of an object from either its diffuse or specular reflectance using four spherical gradient illumination patterns. In contrast to traditional photometric stereo, the spherical patterns allow normals to be estimated simultaneously from any number of viewpoints. We present two polarized lighting techniques that allow the diffuse and specular normal maps of an object to be measured independently. For scattering materials, we show that the specular normal maps yield the best record of detailed surface shape while the diffuse normals deviate from the true surface normal due to subsurface scattering, and that this effect is dependent on wavelength. We show several applications of this acquisition technique. First, we capture normal maps of a facial performance simultaneously from several viewing positions using time-multiplexed illumination. Second, we show that highresolution normal maps based on the specular component can be used with structured light 3D scanning to quickly acquire high-resolution facial surface geometry using off-the-shelf digital still cameras. Finally, we present a realtime shading model that uses independently estimated normal maps for the specular and diffuse color channels to reproduce some of the perceptually important effects of subsurface scattering.},
  file = {/Users/sunjiaming/Zotero/storage/Z66UK749/Ma et al. - Rapid Acquisition of Specular and Diffuse Normal M.pdf},
  language = {en}
}

@article{maRethinkingPseudoLiDARRepresentation2020,
  title = {Rethinking {{Pseudo}}-{{LiDAR Representation}}},
  author = {Ma, Xinzhu and Liu, Shinan and Xia, Zhiyi and Zhang, Hongwen and Zeng, Xingyu and Ouyang, Wanli},
  year = {2020},
  month = aug,
  abstract = {The recently proposed pseudo-LiDAR based 3D detectors greatly improve the benchmark of monocular/stereo 3D detection task. However, the underlying mechanism remains obscure to the research community. In this paper, we perform an in-depth investigation and observe that the efficacy of pseudo-LiDAR representation comes from the coordinate transformation, instead of data representation itself. Based on this observation, we design an image based CNN detector named Patch-Net, which is more generalized and can be instantiated as pseudo-LiDAR based 3D detectors. Moreover, the pseudo-LiDAR data in our PatchNet is organized as the image representation, which means existing 2D CNN designs can be easily utilized for extracting deep features from input data and boosting 3D detection performance. We conduct extensive experiments on the challenging KITTI dataset, where the proposed PatchNet outperforms all existing pseudo-LiDAR based counterparts. Code has been made available at: https://github.com/xinzhuma/patchnet.},
  archivePrefix = {arXiv},
  eprint = {2008.04582},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Rethinking Pseudo-LiDAR Representation-Ma et al-2020.pdf;/Users/sunjiaming/Zotero/storage/KCR2R3H7/2008.html},
  journal = {arXiv:2008.04582 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{marionLabelFusionPipelineGenerating2017,
  title = {{{LabelFusion}}: {{A Pipeline}} for {{Generating Ground Truth Labels}} for {{Real RGBD Data}} of {{Cluttered Scenes}}},
  shorttitle = {{{LabelFusion}}},
  author = {Marion, Pat and Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
  year = {2017},
  month = jul,
  abstract = {Deep neural network (DNN) architectures have been shown to outperform traditional pipelines for object segmentation and pose estimation using RGBD data, but the performance of these DNN pipelines is directly tied to how representative the training data is of the true data. Hence a key requirement for employing these methods in practice is to have a large set of labeled data for your specific robotic manipulation task, a requirement that is not generally satisfied by existing datasets. In this paper we develop a pipeline to rapidly generate high quality RGBD data with pixelwise labels and object poses. We use an RGBD camera to collect video of a scene from multiple viewpoints and leverage existing reconstruction techniques to produce a 3D dense reconstruction. We label the 3D reconstruction using a human assisted ICP-fitting of object meshes. By reprojecting the results of labeling the 3D scene we can produce labels for each RGBD image of the scene. This pipeline enabled us to collect over 1,000,000 labeled object instances in just a few days. We use this dataset to answer questions related to how much training data is required, and of what quality the data must be, to achieve high performance from a DNN architecture.},
  archivePrefix = {arXiv},
  eprint = {1707.04796},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LabelFusion-Marion et al-2017.pdf;/Users/sunjiaming/Zotero/storage/3GLP6S78/1707.html},
  journal = {arXiv:1707.04796 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{maRobustLineSegments2020,
  title = {Robust {{Line Segments Matching}} via {{Graph Convolution Networks}}},
  author = {Ma, QuanMeng and Jiang, Guang and Lai, DianZhi},
  year = {2020},
  month = apr,
  abstract = {Line matching plays an essential role in structure from motion (SFM) and simultaneous localization and mapping (SLAM), especially in low-textured and repetitive scenes. In this paper, we present a new method of using a graph convolution network to match line segments in a pair of images, and we design a graph-based strategy of matching line segments with relaxing to an optimal transport problem. In contrast to hand-crafted line matching algorithms, our approach learns local line segment descriptor and the matching simultaneously through end-to-end training. The results show our method outperforms the state-of-the-art techniques, and especially, the recall is improved from 45.28\% to 70.47\% under a similar presicion. The code of our work is available at https://github.com/mameng1/GraphLineMatching.},
  archivePrefix = {arXiv},
  eprint = {2004.04993},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust Line Segments Matching via Graph Convolution Networks-Ma et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BMV3N658/2004.html},
  journal = {arXiv:2004.04993 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@book{marrVisionComputationalInvestigation2010,
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  shorttitle = {Vision},
  author = {Marr, David},
  year = {2010},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: ocn472791457},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Vision-Marr-2010.pdf},
  isbn = {978-0-262-51462-0},
  language = {en},
  lccn = {QP475 .M27 2010}
}

@article{martensDeepLearningHessianfree,
  title = {Deep Learning via {{Hessian}}-Free Optimization},
  author = {Martens, James},
  pages = {8},
  abstract = {We develop a 2nd-order optimization method based on the ``Hessian-free'' approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to autoencoders, or any specific model class. We also discuss the issue of ``pathological curvature'' as a possible explanation for the difficulty of deeplearning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
  file = {/Users/sunjiaming/Zotero/storage/3MPIDTRR/Martens - Deep learning via Hessian-free optimization.pdf},
  language = {en}
}

@article{martin-bruallaNeRFWildNeural2020,
  title = {{{NeRF}} in the {{Wild}}: {{Neural Radiance Fields}} for {{Unconstrained Photo Collections}}},
  shorttitle = {{{NeRF}} in the {{Wild}}},
  author = {{Martin-Brualla}, Ricardo and Radwan, Noha and Sajjadi, Mehdi S. M. and Barron, Jonathan T. and Dosovitskiy, Alexey and Duckworth, Daniel},
  year = {2020},
  month = aug,
  abstract = {We present a learning-based method for synthesizing novel views of complex outdoor scenes using only unstructured collections of in-the-wild photographs. We build on neural radiance fields (NeRF), which uses the weights of a multilayer perceptron to implicitly model the volumetric density and color of a scene. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. In this work, we introduce a series of extensions to NeRF to address these issues, thereby allowing for accurate reconstructions from unstructured image collections taken from the internet. We apply our system, which we dub NeRF-W, to internet photo collections of famous landmarks, thereby producing photorealistic, spatially consistent scene representations despite unknown and confounding factors, resulting in significant improvement over the state of the art.},
  archivePrefix = {arXiv},
  eprint = {2008.02268},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/NeRF in the Wild-Martin-Brualla et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FBY4BCAR/2008.html},
  journal = {arXiv:2008.02268 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{mastersRevisitingSmallBatch2018,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  year = {2018},
  month = apr,
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput.},
  archivePrefix = {arXiv},
  eprint = {1804.07612},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Revisiting Small Batch Training for Deep Neural Networks-Masters_Luschi-2018.pdf},
  journal = {arXiv:1804.07612 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{maTrafficPredictTrajectoryPrediction2018,
  title = {{{TrafficPredict}}: {{Trajectory Prediction}} for {{Heterogeneous Traffic}}-{{Agents}}},
  shorttitle = {{{TrafficPredict}}},
  author = {Ma, Yuexin and Zhu, Xinge and Zhang, Sibo and Yang, Ruigang and Wang, Wenping and Manocha, Dinesh},
  year = {2018},
  month = nov,
  abstract = {To safely and efficiently navigate in complex urban traffic, autonomous vehicles must make responsible predictions in relation to surrounding traffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and critical task is to explore the movement patterns of different traffic-agents and predict their future trajectories accurately to help the autonomous vehicle make reasonable navigation decision. To solve this problem, we propose a long short-term memory-based (LSTM-based) realtime traffic prediction algorithm, TrafficPredict. Our approach uses an instance layer to learn instances' movements and interactions and has a category layer to learn the similarities of instances belonging to the same type to refine the prediction. In order to evaluate its performance, we collected trajectory datasets in a large city consisting of varying conditions and traffic densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another. We evaluate the performance of TrafficPredict on our new dataset and highlight its higher accuracy for trajectory prediction by comparing with prior prediction methods.},
  archivePrefix = {arXiv},
  eprint = {1811.02146},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TrafficPredict-Ma et al-2018.pdf;/Users/sunjiaming/Zotero/storage/3ZANNAA4/1811.html},
  journal = {arXiv:1811.02146 [cs]},
  primaryClass = {cs}
}

@inproceedings{maUnsupervisedDenseObject2014,
  title = {Unsupervised {{Dense Object Discovery}}, {{Detection}}, {{Tracking}} and {{Reconstruction}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Ma, Lu and Sibley, Gabe},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {80--95},
  publisher = {{Springer International Publishing}},
  abstract = {In this paper, we present an unsupervised framework for discovering, detecting, tracking, and reconstructing dense objects from a video sequence. The system simultaneously localizes a moving camera, and discovers a set of shape and appearance models for multiple objects, including the scene background. Each object model is represented by both a 2D and 3D level-set. This representation is used to improve detection, 2D-tracking, 3D-registration and importantly subsequent updates to the level-set itself. This single framework performs dense simultaneous localization and mapping as well as unsupervised object discovery. At each iteration portions of the scene that fail to track, such as bulk outliers on moving rigid bodies, are used to either seed models for new objects or to update models of known objects. For the latter, once an object is successfully tracked in 2D with aid from a 2D level-set segmentation, the level-set is updated and then used to aid registration and evolution of a 3D level-set that captures shape information. For a known object either learned by our system or introduced from a third-party library, our framework can detect similar appearances and geometries in the scene. The system is tested using single and multiple object data sets. Results demonstrate an improved method for discovering and reconstructing 2D and 3D object models, which aid tracking even under significant occlusion or rapid motion.},
  isbn = {978-3-319-10605-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{mccormacFusionVolumetricObjectLevel2018,
  title = {Fusion++: {{Volumetric Object}}-{{Level SLAM}}},
  shorttitle = {Fusion++},
  booktitle = {2018 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Mccormac, J. and Clark, R. and Bloesch, M. and Davison, A. and Leutenegger, S.},
  year = {2018},
  month = sep,
  pages = {32--41},
  doi = {10.1109/3DV.2018.00015},
  abstract = {We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally refined via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is refined over time and an existence probability to account for spurious instance predictions. We demonstrate our approach on a hand-held RGB-D sequence from a cluttered office scene with a large number and variety of object instances, highlighting how the system closes loops and makes good use of existing objects on repeated loops. We quantitatively evaluate the trajectory error of our system against a baseline approach on the RGB-D SLAM benchmark, and qualitatively compare reconstruction quality of discovered objects on the YCB video dataset. Performance evaluation shows our approach is highly memory efficient and runs online at 4-8Hz (excluding relocalisation) despite not being optimised at the software level.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fusion++-Mccormac et al-2018.pdf;/Users/sunjiaming/Zotero/storage/33MFZ4PL/8490953.html},
  keywords = {reconstruction}
}

@article{mccormacSemanticFusionDense3D2016,
  ids = {mccormacSemanticFusionDense3D2016a},
  title = {{{SemanticFusion}}: {{Dense 3D Semantic Mapping}} with {{Convolutional Neural Networks}}},
  shorttitle = {{{SemanticFusion}}},
  author = {McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
  year = {2016},
  month = sep,
  abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.},
  archivePrefix = {arXiv},
  eprint = {1609.05130},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SemanticFusion-McCormac et al-22.pdf;/Users/sunjiaming/Zotero/storage/EWPK344C/1609.html},
  journal = {arXiv:1609.05130 [cs]},
  primaryClass = {cs}
}

@article{melekhovDGCNetDenseGeometric2018,
  title = {{{DGC}}-{{Net}}: {{Dense Geometric Correspondence Network}}},
  shorttitle = {{{DGC}}-{{Net}}},
  author = {Melekhov, Iaroslav and Tiulpin, Aleksei and Sattler, Torsten and Pollefeys, Marc and Rahtu, Esa and Kannala, Juho},
  year = {2018},
  month = oct,
  abstract = {This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.},
  archivePrefix = {arXiv},
  eprint = {1810.08393},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/RPQAGI6H/Melekhov et al. - 2018 - DGC-Net Dense Geometric Correspondence Network.pdf;/Users/sunjiaming/Zotero/storage/P4YEPFJN/1810.html},
  journal = {arXiv:1810.08393 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{mengSIGNetSemanticInstance2018,
  title = {{{SIGNet}}: {{Semantic Instance Aided Unsupervised 3D Geometry Perception}}},
  shorttitle = {{{SIGNet}}},
  author = {Meng, Yue and Lu, Yongxi and Raj, Aman and Sunarjo, Samuel and Guo, Rui and Javidi, Tara and Bansal, Gaurav and Bharadia, Dinesh},
  year = {2018},
  month = dec,
  abstract = {Unsupervised learning for geometric perception (depth, optical flow, etc.) is of great interest to autonomous systems. Recent works on unsupervised learning have made considerable progress on perceiving geometry; however, they usually ignore the coherence of objects and perform poorly under scenarios with dark and noisy environments. In contrast, supervised learning algorithms, which are robust, require large labeled geometric dataset. This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. Specifically, SIGNet integrates semantic information to make depth and flow predictions consistent with objects and robust to low lighting conditions. SIGNet is shown to improve upon the state-of-the-art unsupervised learning for depth prediction by 30\% (in squared relative error). In particular, SIGNet improves the dynamic object class performance by 39\% in depth prediction and 29\% in flow prediction. Our code will be made available at https://github.com/mengyuest/SIGNet},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SIGNet-Meng et al-2018.pdf;/Users/sunjiaming/Zotero/storage/CYDTBGXM/1812.html},
  language = {en}
}

@inproceedings{menzeObjectSceneFlow2015,
  title = {Object Scene Flow for Autonomous Vehicles},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Menze, Moritz and Geiger, Andreas},
  year = {2015},
  month = jun,
  pages = {3061--3070},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298925},
  abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently moving objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also reveal novel challenges which cannot be handled by existing methods.},
  file = {/Users/sunjiaming/Zotero/storage/KITB6D8U/Menze and Geiger - 2015 - Object scene flow for autonomous vehicles.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@article{merrillCALC2CombiningAppearance2019,
  title = {{{CALC2}}.0: {{Combining Appearance}}, {{Semantic}} and {{Geometric Information}} for {{Robust}} and {{Efficient Visual Loop Closure}}},
  shorttitle = {{{CALC2}}.0},
  author = {Merrill, Nathaniel and Huang, Guoquan},
  year = {2019},
  month = oct,
  abstract = {Traditional attempts for loop closure detection typically use hand-crafted features, relying on geometric and visual information only, whereas more modern approaches tend to use semantic, appearance or geometric features extracted from deep convolutional neural networks (CNNs). While these approaches are successful in many applications, they do not utilize all of the information that a monocular image provides, and many of them, particularly the deep-learning based methods, require user-chosen thresholding to actually close loops -- which may impact generality in practical applications. In this work, we address these issues by extracting all three modes of information from a custom deep CNN trained specifically for the task of place recognition. Our network is built upon a combination of a semantic segmentator, Variational Autoencoder (VAE) and triplet embedding network. The network is trained to construct a global feature space to describe both the visual appearance and semantic layout of an image. Then local keypoints are extracted from maximally-activated regions of low-level convolutional feature maps, and keypoint descriptors are extracted from these feature maps in a novel way that incorporates ideas from successful hand-crafted features. These keypoints are matched globally for loop closure candidates, and then used as a final geometric check to refute false positives. As a result, the proposed loop closure detection system requires no touchy thresholding, and is highly robust to false positives -- achieving better precision-recall curves than the state-of-the-art NetVLAD, and with real-time speeds.},
  archivePrefix = {arXiv},
  eprint = {1910.14103},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CALC2-Merrill_Huang-2019.pdf;/Users/sunjiaming/Zotero/storage/CDVUK5GN/1910.html},
  journal = {arXiv:1910.14103 [cs]},
  primaryClass = {cs}
}

@article{meschederOccupancyNetworksLearning2018,
  title = {Occupancy {{Networks}}: {{Learning 3D Reconstruction}} in {{Function Space}}},
  shorttitle = {Occupancy {{Networks}}},
  author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  year = {2018},
  month = dec,
  abstract = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
  archivePrefix = {arXiv},
  eprint = {1812.03828},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Occupancy Networks-Mescheder et al-4.pdf;/Users/sunjiaming/Zotero/storage/U2MRYXE3/1812.html},
  journal = {arXiv:1812.03828 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{meshryNeuralRerenderingWild2019,
  title = {Neural {{Rerendering}} in the {{Wild}}},
  author = {Meshry, Moustafa and Goldman, Dan B. and Khamis, Sameh and Hoppe, Hugues and Pandey, Rohit and Snavely, Noah and {Martin-Brualla}, Ricardo},
  year = {2019},
  month = apr,
  abstract = {We explore total scene capture \textemdash{} recording, modeling, and rerendering a scene under varying appearance such as season and time of day. Starting from internet photos of a tourist landmark, we apply traditional 3D reconstruction to register the photos and approximate the scene as a point cloud. For each photo, we render the scene points into a deep framebuffer, and train a neural network to learn the mapping of these initial renderings to the actual photos. This rerendering network also takes as input a latent appearance vector and a semantic mask indicating the location of transient objects like pedestrians. The model is evaluated on several datasets of publicly available images spanning a broad range of illumination conditions. We create short videos demonstrating realistic manipulation of the image viewpoint, appearance, and semantic labeling. We also compare results with prior work on scene reconstruction from internet photos.},
  archivePrefix = {arXiv},
  eprint = {1904.04290},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Rerendering in the Wild-Meshry et al-2019.pdf},
  journal = {arXiv:1904.04290 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{metzerSelfSamplingNeuralPoint2020,
  title = {Self-{{Sampling}} for {{Neural Point Cloud Consolidation}}},
  author = {Metzer, Gal and Hanocka, Rana and Giryes, Raja and {Cohen-Or}, Daniel},
  year = {2020},
  month = aug,
  abstract = {In this paper, we introduce a deep learning technique for consolidating and sharp feature generation of point clouds using only the input point cloud itself. Rather than explicitly define a prior that describes typical shape characteristics (i.e., piecewise-smoothness), or a heuristic policy for generating novel sharp points, we opt to learn both using a neural network with shared-weights. Instead of relying on a large collection of manually annotated data, we use the self-supervision present within a single shape, i.e., self-prior, to train the network, and learn the underlying distribution of sharp features specific to the given input point cloud. By learning to map a low-curvature subset of the input point cloud to a disjoint high-curvature subset, the network formalizes the shape-specific characteristics and infers how to generate sharp points. During test time, the network is repeatedly fed a random subset of points from the input and displaces them to generate an arbitrarily large set of novel sharp feature points. The local shared weights are optimized over the entire shape, learning non-local statistics and exploiting the recurrence of local-scale geometries. We demonstrate the ability to generate coherent sets of sharp feature points on a variety of shapes, while eliminating outliers and noise.},
  archivePrefix = {arXiv},
  eprint = {2008.06471},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-Sampling for Neural Point Cloud Consolidation-Metzer et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ABM9S7ZN/2008.html},
  journal = {arXiv:2008.06471 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{meyerLaserFlowEfficientProbabilistic2020,
  title = {{{LaserFlow}}: {{Efficient}} and {{Probabilistic Object Detection}} and {{Motion Forecasting}}},
  shorttitle = {{{LaserFlow}}},
  author = {Meyer, Gregory P. and Charland, Jake and Pandey, Shreyash and Laddha, Ankit and {Vallespi-Gonzalez}, Carlos and Wellington, Carl K.},
  year = {2020},
  month = mar,
  abstract = {In this work, we present LaserFlow, an efficient method for 3D object detection and motion forecasting from LiDAR. Unlike the previous work, our approach utilizes the native range view representation of the LiDAR, which enables our method to operate at the full range of the sensor in real-time without voxelization or compression of the data. We propose a new multi-sweep fusion architecture, which extracts and merges temporal features directly from the range images. Furthermore, we propose a novel technique for learning a probability distribution over future trajectories inspired by curriculum learning. We evaluate LaserFlow on two autonomous driving datasets and demonstrate competitive results when compared to the existing state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {2003.05982},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LaserFlow-Meyer et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2P2Q36AW/2003.html},
  journal = {arXiv:2003.05982 [cs]},
  primaryClass = {cs}
}

@article{meyerLaserNetEfficientProbabilistic2019,
  title = {{{LaserNet}}: {{An Efficient Probabilistic 3D Object Detector}} for {{Autonomous Driving}}},
  shorttitle = {{{LaserNet}}},
  author = {Meyer, Gregory P. and Laddha, Ankit and Kee, Eric and {Vallespi-Gonzalez}, Carlos and Wellington, Carl K.},
  year = {2019},
  month = mar,
  abstract = {In this paper, we present LaserNet, a computationally efficient method for 3D object detection from LiDAR data for autonomous driving. The efficiency results from processing LiDAR data in the native range view of the sensor, where the input data is naturally compact. Operating in the range view involves well known challenges for learning, including occlusion and scale variation, but it also provides contextual information based on how the sensor data was captured. Our approach uses a fully convolutional network to predict a multimodal distribution over 3D boxes for each point and then it efficiently fuses these distributions to generate a prediction for each object. Experiments show that modeling each detection as a distribution rather than a single deterministic box leads to better overall detection performance. Benchmark results show that this approach has significantly lower runtime than other recent detectors and that it achieves state-of-the-art performance when compared on a large dataset that has enough data to overcome the challenges of training on the range view.},
  archivePrefix = {arXiv},
  eprint = {1903.08701},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LaserNet-Meyer et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XNFCYSKC/1903.html},
  journal = {arXiv:1903.08701 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@book{meyersEffectiveModern422014,
  title = {Effective Modern {{C}}++: 42 Specific Ways to Improve Your Use of {{C}}++11 and {{C}}++14},
  shorttitle = {Effective Modern {{C}}++},
  author = {Meyers, Scott},
  year = {2014},
  edition = {First edition},
  publisher = {{O'Reilly Media}},
  address = {{Beijing ; Sebastopol, CA}},
  abstract = {"Coming to grips with C++11 and C++14 is more than a matter of familiarizing yourself with the features they introduce (e.g., auto type declarations, move semantics, lambda expressions, and concurrency support). The challenge is learning to use those features effectively -- so that your software is correct, efficient, maintainable, and portable. That's where this practical book comes in. It describes how to write truly great software using C++11 and C++14 -- i.e. using modern C++ ...Effective Modern C++ follows the proven guideline-based, example-driven format of Scott Meyers' earlier books, but covers entirely new material"--Publisher's website},
  annotation = {OCLC: ocn884480640},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Effective modern C++-Meyers-2014.pdf},
  isbn = {978-1-4919-0399-5},
  language = {en},
  lccn = {QA76.73.C153 M494 2015}
}

@article{meyerSensorFusionJoint2019,
  title = {Sensor {{Fusion}} for {{Joint 3D Object Detection}} and {{Semantic Segmentation}}},
  author = {Meyer, Gregory P. and Charland, Jake and Hegde, Darshan and Laddha, Ankit and {Vallespi-Gonzalez}, Carlos},
  year = {2019},
  month = apr,
  abstract = {In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime.},
  archivePrefix = {arXiv},
  eprint = {1904.11466},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation-Meyer et al-2019.pdf;/Users/sunjiaming/Zotero/storage/DW8MZ58E/1904.html},
  journal = {arXiv:1904.11466 [cs]},
  primaryClass = {cs}
}

@article{michalkiewiczDeepLevelSets2019,
  title = {Deep {{Level Sets}}: {{Implicit Surface Representations}} for {{3D Shape Inference}}},
  shorttitle = {Deep {{Level Sets}}},
  author = {Michalkiewicz, Mateusz and Pontes, Jhony K. and Jack, Dominic and Baktashmotlagh, Mahsa and Eriksson, Anders},
  year = {2019},
  month = jan,
  abstract = {Existing 3D surface representation approaches are unable to accurately classify pixels and their orientation lying on the boundary of an object. Thus resulting in coarse representations which usually require post-processing steps to extract 3D surface meshes. To overcome this limitation, we propose an end-to-end trainable model that directly predicts implicit surface representations of arbitrary topology by optimising a novel geometric loss function. Specifically, we propose to represent the output as an oriented level set of a continuous embedding function, and incorporate this in a deep end-to-end learning framework by introducing a variational shape inference formulation. We investigate the benefits of our approach on the task of 3D surface prediction and demonstrate its ability to produce a more accurate reconstruction compared to voxel-based representations. We further show that our model is flexible and can be applied to a variety of shape inference problems.},
  archivePrefix = {arXiv},
  eprint = {1901.06802},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Level Sets-Michalkiewicz et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YWCHTL6P/1901.html},
  journal = {arXiv:1901.06802 [cs]},
  primaryClass = {cs}
}

@article{miksikLiveReconstructionLargeScale2019,
  title = {Live {{Reconstruction}} of {{Large}}-{{Scale Dynamic Outdoor Worlds}}},
  author = {Miksik, Ondrej and Vineet, Vibhav},
  year = {2019},
  month = mar,
  abstract = {Standard 3D reconstruction pipelines assume stationary world, therefore suffer from `ghost artifacts' whenever dynamic objects are present in the scene. Recent approaches has started tackling this issue, however, they typically either only discard dynamic information, represent it using bounding boxes or per-frame depth or rely on approaches that are inherently slow and not suitable to online settings. We propose an end-to-end system for live reconstruction of large-scale outdoor dynamic environments. We leverage recent advances in computationally efficient data-driven approaches for 6-DoF object pose estimation to segment the scene into objects and stationary `background'. This allows us to represent the scene using a time-dependent (dynamic) map, in which each object is explicitly represented as a separate instance and reconstructed in its own volume. For each time step, our dynamic map maintains a relative pose of each volume with respect to the stationary background. Our system operates in incremental manner which is essential for on-line reconstruction, handles large-scale environments with objects at large distances and runs in (near) real-time. We demonstrate the efficacy of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense 3D reconstructions of a number of dynamic scenes.},
  archivePrefix = {arXiv},
  eprint = {1903.06708},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Live Reconstruction of Large-Scale Dynamic Outdoor Worlds-Miksik_Vineet-2019.pdf;/Users/sunjiaming/Zotero/storage/E2GY23RR/1903.html},
  journal = {arXiv:1903.06708 [cs]},
  primaryClass = {cs}
}

@article{milanoPrimalDualMeshConvolutional,
  title = {Primal-{{Dual Mesh Convolutional Neural Networks}}},
  author = {Milano, Francesco and Loquercio, Antonio and Rosinol, Antoni and Scaramuzza, Davide and Carlone, Luca},
  pages = {28},
  abstract = {Recent works in geometric deep learning have introduced neural networks that allow performing inference tasks on three-dimensional geometric data by defining convolution \textendash and sometimes pooling\textendash{} operations on triangle meshes. These methods, however, either consider the input mesh as a graph, and do not exploit specific geometric properties of meshes for feature aggregation and downsampling, or are specialized for meshes, but rely on a rigid definition of convolution that does not properly capture the local topology of the mesh. We propose a method that combines the advantages of both types of approaches, while addressing their limitations: we extend a primal-dual framework drawn from the graph-neural-network literature to triangle meshes, and define convolutions on two types of graphs constructed from an input mesh. Our method takes features for both edges and faces of a 3D mesh as input, and dynamically aggregates them using an attention mechanism. At the same time, we introduce a pooling operation with a precise geometric interpretation, that allows handling variations in the mesh connectivity by clustering mesh faces in a task-driven fashion. We provide theoretical insights of our approach using tools from the mesh-simplification literature. In addition, we validate experimentally our method in the tasks of shape classification and shape segmentation, where we obtain comparable or superior performance to the state of the art.},
  file = {/Users/sunjiaming/Zotero/storage/S6JB8U23/Milano et al. - Primal-Dual Mesh Convolutional Neural Networks.pdf},
  language = {en}
}

@article{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = mar,
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archivePrefix = {arXiv},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/NeRF-Mildenhall et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EE5QTSIZ/2003.html},
  journal = {arXiv:2003.08934 [cs]},
  primaryClass = {cs}
}

@article{mishkinArXivingSubmissionHelps2020,
  title = {{{ArXiving Before Submission Helps Everyone}}},
  author = {Mishkin, Dmytro and Tabb, Amy and Matas, Jiri},
  year = {2020},
  month = oct,
  abstract = {We claim, and present evidence, that allowing arXiv publication before a conference or journal submission benefits researchers, especially early career, as well as the whole scientific community. Specifically, arXiving helps professional identity building, protects against independent re-discovery, idea theft and gate-keeping; it facilitates open research result distribution and reduces inequality. The advantages dwarf the drawbacks -- mainly the relative increase in acceptance rate of papers of well-known authors -- which studies show to be marginal. Analyzing the pros and cons of arXiving papers, we conclude that requiring preprints be anonymous is nearly as detrimental as not allowing them. We see no reasons why anyone but the authors should decide whether to arXiv or not.},
  archivePrefix = {arXiv},
  eprint = {2010.05365},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ArXiving Before Submission Helps Everyone-Mishkin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9FAHEQ3R/2010.html},
  journal = {arXiv:2010.05365 [cs]},
  keywords = {Computer Science - Digital Libraries,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{mitraPartialApproximateSymmetrya,
  title = {Partial and {{Approximate Symmetry Detection}} for {{3D Geometry}}},
  author = {Mitra, Niloy J and Guibas, Leonidas J and Pauly, Mark},
  pages = {9},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Partial and Approximate Symmetry Detection for 3D Geometry-Mitra et al-2.pdf},
  language = {en}
}

@article{miyashitaMIDASProjectionMarkerless2019,
  title = {{{MIDAS}} Projection: Markerless and Modelless Dynamic Projection Mapping for Material Representation},
  shorttitle = {{{MIDAS}} Projection},
  author = {Miyashita, Leo and Watanabe, Yoshihiro and Ishikawa, Masatoshi},
  year = {2019},
  month = jan,
  volume = {37},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275045},
  file = {/Users/sunjiaming/Zotero/storage/MRFZXNIH/Miyashita et al. - 2019 - MIDAS projection markerless and modelless dynamic.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@article{miyatoSpectralNormalizationGenerative2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  year = {2018},
  month = feb,
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
  archivePrefix = {arXiv},
  eprint = {1802.05957},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Miyato et al_2018_Spectral Normalization for Generative Adversarial Networks.pdf;/Users/sunjiaming/Zotero/storage/LTB8Z3YT/1802.html},
  journal = {arXiv:1802.05957 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{molchanovPruningConvolutionalNeural2016,
  title = {Pruning {{Convolutional Neural Networks}} for {{Resource Efficient Inference}}},
  author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  year = {2016},
  month = nov,
  abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with finetuning by backpropagation\textemdash a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10\texttimes{} theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the largescale ImageNet dataset to emphasize the flexibility of our approach.},
  archivePrefix = {arXiv},
  eprint = {1611.06440},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Pruning Convolutional Neural Networks for Resource Efficient Inference-Molchanov et al-2016.pdf},
  journal = {arXiv:1611.06440 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@book{mollerRealtimeRendering2018,
  title = {Real-Time Rendering},
  author = {M{\"o}ller, Tomas},
  year = {2018},
  edition = {Fourth edition},
  publisher = {{Taylor \& Francis, CRC Press}},
  address = {{Boca Raton}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-time rendering-Möller-2018.pdf},
  isbn = {978-1-138-62700-0},
  language = {en},
  lccn = {T385 .M635 2018}
}

@article{monszpartIMapperInteractionguidedJoint2019,
  title = {{{iMapper}}: {{Interaction}}-Guided {{Joint Scene}} and {{Human Motion Mapping}} from {{Monocular Videos}}},
  shorttitle = {{{iMapper}}},
  author = {Monszpart, Aron and Guerrero, Paul and Ceylan, Duygu and Yumer, Ersin and Mitra, Niloy J.},
  year = {2019},
  month = jul,
  volume = {38},
  pages = {1--15},
  issn = {07300301},
  doi = {10.1145/3306346.3322961},
  abstract = {A long-standing challenge in scene analysis is the recovery of scene arrangements under moderate to heavy occlusion, directly from monocular video. While the problem remains a subject of active research, concurrent advances have been made in the context of human pose reconstruction from monocular video, including image-space feature point detection and 3D pose recovery. These methods, however, start to fail under moderate to heavy occlusion as the problem becomes severely under-constrained. We approach the problems differently. We observe that people interact similarly in similar scenes. Hence, we exploit the correlation between scene object arrangement and motions performed in that scene in both directions: first, typical motions performed when interacting with objects inform us about possible object arrangements; and second, object arrangements, in turn, constrain the possible motions. We present iMapper, a data-driven method that focuses on identifying human-object interactions, and jointly reasons about objects and human movement over space-time to recover both a plausible scene arrangement and consistent human interactions. We first introduce the notion of characteristic interactions as regions in space-time when an informative human-object interaction happens. This is followed by a novel occlusion-aware matching procedure that searches and aligns such characteristic snapshots from an interaction database to best explain the input monocular video. Through extensive evaluations, both quantitative and qualitative, we demonstrate that iMapper significantly improves performance over both dedicated state-of-the-art scene analysis and 3D human pose recovery approaches, especially under medium to heavy occlusion.},
  archivePrefix = {arXiv},
  eprint = {1806.07889},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/iMapper-Monszpart et al-2019.pdf;/Users/sunjiaming/Zotero/storage/7I3II5CL/1806.html},
  journal = {ACM Transactions on Graphics},
  number = {4}
}

@article{moPT2PCLearningGenerate2020,
  title = {{{PT2PC}}: {{Learning}} to {{Generate 3D Point Cloud Shapes}} from {{Part Tree Conditions}}},
  shorttitle = {{{PT2PC}}},
  author = {Mo, Kaichun and Wang, He and Yan, Xinchen and Guibas, Leonidas J.},
  year = {2020},
  month = mar,
  abstract = {3D generative shape modeling is a fundamental research area in computer vision and interactive computer graphics, with many real-world applications. This paper investigates the novel problem of generating 3D shape point cloud geometry from a symbolic part tree representation. In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN "part tree"-to-"point cloud" model (PT2PC) that disentangles the structural and geometric factors. The proposed model incorporates the part tree condition into the architecture design by passing messages top-down and bottom-up along the part tree hierarchy. Experimental results and user study demonstrate the strengths of our method in generating perceptually plausible and diverse 3D point clouds, given the part tree condition. We also propose a novel structural measure for evaluating if the generated shape point clouds satisfy the part tree conditions.},
  archivePrefix = {arXiv},
  eprint = {2003.08624},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PT2PC-Mo et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RS5RCASS/2003.html},
  journal = {arXiv:2003.08624 [cs]},
  primaryClass = {cs}
}

@article{moStructEditLearningStructural2019,
  title = {{{StructEdit}}: {{Learning Structural Shape Variations}}},
  shorttitle = {{{StructEdit}}},
  author = {Mo, Kaichun and Guerrero, Paul and Yi, Li and Su, Hao and Wonka, Peter and Mitra, Niloy and Guibas, Leonidas J.},
  year = {2019},
  month = nov,
  abstract = {Learning to encode differences in the geometry and (topological) structure of the shapes of ordinary objects is key to generating semantically plausible variations of a given shape, transferring edits from one shape to another, and many other applications in 3D content creation. The common approach of encoding shapes as points in a high-dimensional latent feature space suggests treating shape differences as vectors in that space. Instead, we treat shape differences as primary objects in their own right and propose to encode them in their own latent space. In a setting where the shapes themselves are encoded in terms of fine-grained part hierarchies, we demonstrate that a separate encoding of shape deltas or differences provides a principled way to deal with inhomogeneities in the shape space due to different combinatorial part structures, while also allowing for compactness in the representation, as well as edit abstraction and transfer. Our approach is based on a conditional variational autoencoder for encoding and decoding shape deltas, conditioned on a source shape. We demonstrate the effectiveness and robustness of our approach in multiple shape modification and generation tasks, and provide comparison and ablation studies on the PartNet dataset, one of the largest publicly available 3D datasets.},
  archivePrefix = {arXiv},
  eprint = {1911.11098},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/StructEdit-Mo et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LNKJWRMZ/1911.html},
  journal = {arXiv:1911.11098 [cs]},
  primaryClass = {cs}
}

@article{moStructureNetHierarchicalGraph2019,
  title = {{{StructureNet}}: {{Hierarchical Graph Networks}} for {{3D Shape Generation}}},
  shorttitle = {{{StructureNet}}},
  author = {Mo, Kaichun and Guerrero, Paul and Yi, Li and Su, Hao and Wonka, Peter and Mitra, Niloy and Guibas, Leonidas J.},
  year = {2019},
  month = aug,
  abstract = {The ability to generate novel, diverse, and realistic 3D shapes along with associated part semantics and structure is central to many applications requiring high-quality 3D assets or large volumes of realistic training data. A key challenge towards this goal is how to accommodate diverse shape variations, including both continuous deformations of parts as well as structural or discrete alterations which add to, remove from, or modify the shape constituents and compositional structure. Such object structure can typically be organized into a hierarchy of constituent object parts and relationships, represented as a hierarchy of n-ary graphs. We introduce StructureNet, a hierarchical graph network which (i) can directly encode shapes represented as such n-ary graphs; (ii) can be robustly trained on large and complex shape families; and (iii) can be used to generate a great diversity of realistic structured shape geometries. Technically, we accomplish this by drawing inspiration from recent advances in graph neural networks to propose an order-invariant encoding of n-ary graphs, considering jointly both part geometry and inter-part relations during network training. We extensively evaluate the quality of the learned latent spaces for various shape families and show significant advantages over baseline and competing methods. The learned latent spaces enable several structure-aware geometry processing applications, including shape generation and interpolation, shape editing, or shape structure discovery directly from un-annotated images, point clouds, or partial scans.},
  archivePrefix = {arXiv},
  eprint = {1908.00575},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/StructureNet-Mo et al-2019.pdf},
  journal = {arXiv:1908.00575 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{mousavian3DBoundingBox2016,
  title = {{{3D Bounding Box Estimation Using Deep Learning}} and {{Geometry}}},
  author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
  year = {2016},
  month = dec,
  abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
  archivePrefix = {arXiv},
  eprint = {1612.00496},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D Bounding Box Estimation Using Deep Learning and Geometry-Mousavian et al-2016.pdf},
  journal = {arXiv:1612.00496 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{mur-artalORBSLAM2OpenSourceSLAM2017,
  title = {{{ORB}}-{{SLAM2}}: {{An Open}}-{{Source SLAM System}} for {{Monocular}}, {{Stereo}}, and {{RGB}}-{{D Cameras}}},
  shorttitle = {{{ORB}}-{{SLAM2}}},
  author = {{Mur-Artal}, Raul and Tardos, Juan D.},
  year = {2017},
  month = oct,
  volume = {33},
  pages = {1255--1262},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2017.2705103},
  abstract = {We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/ORB-SLAM2-Mur-Artal_Tardos-2017.pdf},
  journal = {IEEE Transactions on Robotics},
  language = {en},
  number = {5}
}

@article{murezAtlasEndtoEnd3D2020,
  title = {Atlas: {{End}}-to-{{End 3D Scene Reconstruction}} from {{Posed Images}}},
  shorttitle = {Atlas},
  author = {Murez, Zak and {van As}, Tarrence and Bartolozzi, James and Sinha, Ayan and Badrinarayanan, Vijay and Rabinovich, Andrew},
  year = {2020},
  month = aug,
  abstract = {We present an end-to-end 3D reconstruction method for a scene by directly regressing a truncated signed distance function (TSDF) from a set of posed RGB images. Traditional approaches to 3D reconstruction rely on an intermediate representation of depth maps prior to estimating a full 3D model of a scene. We hypothesize that a direct regression to 3D is more effective. A 2D CNN extracts features from each image independently which are then back-projected and accumulated into a voxel volume using the camera intrinsics and extrinsics. After accumulation, a 3D CNN refines the accumulated features and predicts the TSDF values. Additionally, semantic segmentation of the 3D model is obtained without significant computation. This approach is evaluated on the Scannet dataset where we significantly outperform state-of-the-art baselines (deep multiview stereo followed by traditional TSDF fusion) both quantitatively and qualitatively. We compare our 3D semantic segmentation to prior methods that use a depth sensor since no previous work attempts the problem with only RGB input.},
  archivePrefix = {arXiv},
  eprint = {2003.10432},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/MHRSR2SH/Murez et al. - 2020 - Atlas End-to-End 3D Scene Reconstruction from Pos.pdf;/Users/sunjiaming/Zotero/storage/ULHGZMLG/2003.html},
  journal = {arXiv:2003.10432 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper},
  primaryClass = {cs}
}

@article{murthyReconstructingVechiclesSingle2016,
  ids = {murthyReconstructingVechiclesSingle2016a},
  title = {Reconstructing {{Vechicles}} from a {{Single Image}}: {{Shape Priors}} for {{Road Scene Understanding}}},
  shorttitle = {Reconstructing {{Vechicles}} from a {{Single Image}}},
  author = {Murthy, J. Krishna and Krishna, G. V. Sai and Chhaya, Falak and Krishna, K. Madhava},
  year = {2016},
  month = sep,
  abstract = {We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in \textbackslash emph\{shape priors\}, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle \textbackslash emph\{keypoints\} in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.},
  archivePrefix = {arXiv},
  eprint = {1609.09468},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reconstructing Vechicles from a Single Image-Murthy et al-2016.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Reconstructing Vechicles from a Single Image-Murthy et al-22.pdf;/Users/sunjiaming/Zotero/storage/5JPYPY5C/1609.html;/Users/sunjiaming/Zotero/storage/RJ4XBFUR/1609.html},
  journal = {arXiv:1609.09468 [cs]},
  primaryClass = {cs}
}

@inproceedings{murthyShapePriorsRealtime2017,
  title = {Shape Priors for Real-Time Monocular Object Localization in Dynamic Environments},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Murthy, J. Krishna and Sharma, Sarthak and Krishna, K. Madhava},
  year = {2017},
  month = sep,
  pages = {1768--1774},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  doi = {10.1109/IROS.2017.8205990},
  abstract = {Reconstruction of dynamic objects in a scene is a highly challenging problem in the context of SLAM. In this paper, we present a real-time monocular object localization system that estimates the shape and pose of dynamic objects in real-time, using video frames captured from a moving monocular camera. Although the problem seems to be ill-posed, we demonstrate that, by incorporating prior knowledge of the object category, we can obtain more detailed instance-level reconstructions. As opposed to earlier object model specifications, the proposed shape-prior model leads to the formulation of a Bundle Adjustment-like optimization problem for simultaneous shape and pose estimation.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Shape priors for real-time monocular object localization in dynamic environments-Murthy et al-2017.pdf},
  isbn = {978-1-5386-2682-5},
  language = {en}
}

@article{musethVDBHighresolutionSparse2013,
  title = {{{VDB}}: {{High}}-Resolution {{Sparse Volumes}} with {{Dynamic Topology}}},
  shorttitle = {{{VDB}}},
  author = {Museth, Ken},
  year = {2013},
  month = jul,
  volume = {32},
  pages = {27:1--27:22},
  issn = {0730-0301},
  doi = {10.1145/2487228.2487235},
  abstract = {We have developed a novel hierarchical data structure for the efficient representation of sparse, time-varying volumetric data discretized on a 3D grid. Our ``VDB'', so named because it is a Volumetric, Dynamic grid that shares several characteristics with B+trees, exploits spatial coherency of time-varying data to separately and compactly encode data values and grid topology. VDB models a virtually infinite 3D index space that allows for cache-coherent and fast data access into sparse volumes of high resolution. It imposes no topology restrictions on the sparsity of the volumetric data, and it supports fast (average O(1)) random access patterns when the data are inserted, retrieved, or deleted. This is in contrast to most existing sparse volumetric data structures, which assume either static or manifold topology and require specific data access patterns to compensate for slow random access. Since the VDB data structure is fundamentally hierarchical, it also facilitates adaptive grid sampling, and the inherent acceleration structure leads to fast algorithms that are well-suited for simulations. As such, VDB has proven useful for several applications that call for large, sparse, animated volumes, for example, level set dynamics and cloud modeling. In this article, we showcase some of these algorithms and compare VDB with existing, state-of-the-art data structures.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/VDB-Museth-2013.pdf;/Users/sunjiaming/Zotero/storage/S3IA9KTJ/openvdb_introduction_2015-1.0.0.pdf},
  journal = {ACM Trans. Graph.},
  number = {3}
}

@article{najibiDOPSLearningDetect2020,
  title = {{{DOPS}}: {{Learning}} to {{Detect 3D Objects}} and {{Predict}} Their {{3D Shapes}}},
  shorttitle = {{{DOPS}}},
  author = {Najibi, Mahyar and Lai, Guangda and Kundu, Abhijit and Lu, Zhichao and Rathod, Vivek and Funkhouser, Tom and Pantofaru, Caroline and Ross, David and Davis, Larry S. and Fathi, Alireza},
  year = {2020},
  month = apr,
  abstract = {We propose DOPS, a fast single-stage 3D object detection method for LIDAR data. Previous methods often make domain-specific design decisions, for example projecting points into a bird-eye view image in autonomous driving scenarios. In contrast, we propose a general-purpose method that works on both indoor and outdoor scenes. The core novelty of our method is a fast, single-pass architecture that both detects objects in 3D and estimates their shapes. 3D bounding box parameters are estimated in one pass for every point, aggregated through graph convolutions, and fed into a branch of the network that predicts latent codes representing the shape of each detected object. The latent shape space and shape decoder are learned on a synthetic dataset and then used as supervision for the end-to-end training of the 3D object detection pipeline. Thus our model is able to extract shapes without access to ground-truth shape information in the target dataset. During experiments, we find that our proposed method achieves state-of-the-art results by \textasciitilde 5\% on object detection in ScanNet scenes, and it gets top results by 3.4\% in the Waymo Open Dataset, while reproducing the shapes of detected cars.},
  archivePrefix = {arXiv},
  eprint = {2004.01170},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DOPS-Najibi et al-2020.pdf;/Users/sunjiaming/Zotero/storage/363LAI9L/2004.html},
  journal = {arXiv:2004.01170 [cs]},
  primaryClass = {cs}
}

@article{nakajimaIncrementalClassDiscovery2019,
  title = {Incremental {{Class Discovery}} for {{Semantic Segmentation}} with {{RGBD Sensing}}},
  author = {Nakajima, Yoshikatsu and Kang, Byeongkeun and Saito, Hideo and Kitani, Kris},
  year = {2019},
  month = jul,
  abstract = {This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at \{10.7\}Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.},
  archivePrefix = {arXiv},
  eprint = {1907.10008},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Incremental Class Discovery for Semantic Segmentation with RGBD Sensing-Nakajima et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4JTCXVM7/1907.html},
  journal = {arXiv:1907.10008 [cs]},
  primaryClass = {cs}
}

@article{namPracticalSVBRDFAcquisition2019,
  title = {Practical {{SVBRDF}} Acquisition of {{3D}} Objects with Unstructured Flash Photography},
  author = {Nam, Giljoo and Lee, Joo Ho and Gutierrez, Diego and Kim, Min H.},
  year = {2019},
  month = jan,
  volume = {37},
  pages = {1--12},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275017},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Practical SVBRDF acquisition of 3D objects with unstructured flash photography-Nam et al-2019.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@inproceedings{nanPolyFitPolygonalSurface2017,
  title = {{{PolyFit}}: {{Polygonal Surface Reconstruction}} from {{Point Clouds}}},
  shorttitle = {{{PolyFit}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Nan, Liangliang and Wonka, Peter},
  year = {2017},
  month = oct,
  pages = {2372--2380},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.258},
  abstract = {We propose a novel framework for reconstructing lightweight polygonal surfaces from point clouds1. Unlike traditional methods that focus on either extracting good geometric primitives or obtaining proper arrangements of primitives, the emphasis of this work lies in intersecting the primitives (planes only) and seeking for an appropriate combination of them to obtain a manifold polygonal surface model without boundary.},
  file = {/Users/sunjiaming/Zotero/storage/RJWY6A6M/Nan and Wonka - 2017 - PolyFit Polygonal Surface Reconstruction from Poi.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@article{naritaPanopticFusionOnlineVolumetric2019,
  title = {{{PanopticFusion}}: {{Online Volumetric Semantic Mapping}} at the {{Level}} of {{Stuff}} and {{Things}}},
  shorttitle = {{{PanopticFusion}}},
  author = {Narita, Gaku and Seno, Takashi and Ishikawa, Tomoya and Kaji, Yohsuke},
  year = {2019},
  month = mar,
  abstract = {We propose PanopticFusion, a novel online volumetric semantic mapping system at the level of stuff and things. In contrast to previous semantic mapping systems, PanopticFusion is able to densely predict class labels of a background region (stuff) and individually segment arbitrary foreground objects (things). In addition, our system has the capability to reconstruct a large-scale scene and extract a labeled mesh thanks to its use of a spatially hashed volumetric map representation. Our system first predicts pixel-wise panoptic labels (class labels for stuff regions and instance IDs for thing regions) for incoming RGB frames by fusing 2D semantic and instance segmentation outputs. The predicted panoptic labels are integrated into the volumetric map together with depth measurements while keeping the consistency of the instance IDs, which could vary frame to frame, by referring to the 3D map at that moment. In addition, we construct a fully connected conditional random field (CRF) model with respect to panoptic labels for map regularization. For online CRF inference, we propose a novel unary potential approximation and a map division strategy.   We evaluated the performance of our system on the ScanNet (v2) dataset. PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN methods in both semantic and instance segmentation benchmarks. Also, we demonstrate a promising augmented reality application using a 3D panoptic map generated by the proposed system.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PanopticFusion-Narita et al-2019.pdf;/Users/sunjiaming/Zotero/storage/SYRKS9KM/1903.html},
  language = {en}
}

@article{nashPolyGenAutoregressiveGenerative2020,
  title = {{{PolyGen}}: {{An Autoregressive Generative Model}} of {{3D Meshes}}},
  shorttitle = {{{PolyGen}}},
  author = {Nash, Charlie and Ganin, Yaroslav and Eslami, S. M. Ali and Battaglia, Peter W.},
  year = {2020},
  month = feb,
  abstract = {Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.},
  archivePrefix = {arXiv},
  eprint = {2002.10880},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PolyGen-Nash et al-2020.pdf;/Users/sunjiaming/Zotero/storage/L6UMF67F/2002.html},
  journal = {arXiv:2002.10880 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nevenInstanceSegmentationJointly2019,
  ids = {nevenInstanceSegmentationJointly2019a},
  title = {Instance {{Segmentation}} by {{Jointly Optimizing Spatial Embeddings}} and {{Clustering Bandwidth}}},
  author = {Neven, Davy and De Brabandere, Bert and Proesmans, Marc and Van Gool, Luc},
  year = {2019},
  month = jun,
  abstract = {Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5\textbackslash\% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code is available at https://github.com/davyneven/SpatialEmbeddings},
  archivePrefix = {arXiv},
  eprint = {1906.11109},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering-Neven et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering-Neven et al-22.pdf;/Users/sunjiaming/Zotero/storage/QCCR68MC/1906.html;/Users/sunjiaming/Zotero/storage/WVNXNQU4/1906.html},
  journal = {arXiv:1906.11109 [cs]},
  primaryClass = {cs}
}

@inproceedings{newcombeDynamicFusionReconstructionTracking2015,
  title = {{{DynamicFusion}}: {{Reconstruction}} and Tracking of Non-Rigid Scenes in Real-Time},
  shorttitle = {{{DynamicFusion}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Newcombe, Richard A. and Fox, Dieter and Seitz, Steven M.},
  year = {2015},
  month = jun,
  pages = {343--352},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298631},
  abstract = {We present the first dense SLAM system capable of reconstructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sensors. Our DynamicFusion approach reconstructs scene geometry whilst simultaneously estimating a dense volumetric 6D motion field that warps the estimated geometry into a live frame. Like KinectFusion, our system produces increasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/DynamicFusion-Newcombe et al-2015.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@article{newcombeKinectFusionRealTimeDense2011,
  title = {{{KinectFusion}}: {{Real}}-{{Time Dense Surface Mapping}} and {{Tracking}}},
  author = {Newcombe, Richard A and Davison, Andrew J and Izadi, Shahram and Kohli, Pushmeet and Hilliges, Otmar and Shotton, Jamie and Molyneaux, David and Hodges, Steve and Kim, David and Fitzgibbon, Andrew},
  year = {2011},
  pages = {10},
  abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/KinectFusion-Newcombe et al-.pdf},
  journal = {Proceedings of the 24th annual ACM symposium on User interface software and technology},
  keywords = {neufu_paper},
  language = {en}
}

@article{newellFeaturePartitioningEfficient2019,
  title = {Feature {{Partitioning}} for {{Efficient Multi}}-{{Task Architectures}}},
  author = {Newell, Alejandro and Jiang, Lu and Wang, Chong and Li, Li-Jia and Deng, Jia},
  year = {2019},
  month = aug,
  abstract = {Multi-task learning holds the promise of less data, parameters, and time than training of separate models. We propose a method to automatically search over multi-task architectures while taking resource constraints into consideration. We propose a search space that compactly represents different parameter sharing strategies. This provides more effective coverage and sampling of the space of multi-task architectures. We also present a method for quick evaluation of different architectures by using feature distillation. Together these contributions allow us to quickly optimize for efficient multi-task models. We benchmark on Visual Decathlon, demonstrating that we can automatically search for and identify multi-task architectures that effectively make trade-offs between task resource requirements while achieving a high level of final performance.},
  archivePrefix = {arXiv},
  eprint = {1908.04339},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Feature Partitioning for Efficient Multi-Task Architectures-Newell et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AVRZR2ST/1908.html},
  journal = {arXiv:1908.04339 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ngiamStarNetTargetedComputation2019,
  title = {{{StarNet}}: {{Targeted Computation}} for {{Object Detection}} in {{Point Clouds}}},
  shorttitle = {{{StarNet}}},
  author = {Ngiam, Jiquan and Caine, Benjamin and Han, Wei and Yang, Brandon and Chai, Yuning and Sun, Pei and Zhou, Yin and Yi, Xi and Alsharif, Ouais and Nguyen, Patrick and Chen, Zhifeng and Shlens, Jonathon and Vasudevan, Vijay},
  year = {2019},
  month = aug,
  abstract = {LiDAR sensor systems provide high resolution spatial information about the environment for self-driving cars. Therefore, detecting objects from point clouds derived from LiDAR represents a critical problem. Previous work on object detection from LiDAR has emphasized re-purposing convolutional approaches from traditional camera imagery. In this work, we present an object detection system designed specifically for point cloud data blending aspects of one-stage and two-stage systems. We observe that objects in point clouds are quite distinct from traditional camera images: objects are sparse and vary widely in location, but do not exhibit scale distortions observed in single camera perspective. These two observations suggest that simple and cheap data-driven object proposals to maximize spatial coverage or match the observed densities of point cloud data may suffice. This recognition paired with a local, non-convolutional, point-based network permits building an object detector for point clouds that may be trained only once, but adapted to different computational settings -- targeted to different predictive priorities or spatial regions. We demonstrate this flexibility and the targeted detection strategies on both the KITTI detection dataset as well as on the large-scale Waymo Open Dataset. Furthermore, we find that a single network is competitive with other point cloud detectors across a range of computational budgets, while being more flexible to adapt to contextual priorities.},
  archivePrefix = {arXiv},
  eprint = {1908.11069},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/StarNet-Ngiam et al-2019.pdf;/Users/sunjiaming/Zotero/storage/8DUYD5LL/1908.html},
  journal = {arXiv:1908.11069 [cs]},
  primaryClass = {cs}
}

@article{nguyenRobust3D2DInteractive2018,
  title = {A {{Robust 3D}}-{{2D Interactive Tool}} for {{Scene Segmentation}} and {{Annotation}}},
  author = {Nguyen, Duc Thanh and Hua, Binh-Son and Yu, Lap-Fai and Yeung, Sai-Kit},
  year = {2018},
  month = dec,
  volume = {24},
  pages = {3005--3018},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2017.2772238},
  abstract = {Recent advances of 3D acquisition devices have enabled large-scale acquisition of 3D scene data. Such data, if completely and well annotated, can serve as useful ingredients for a wide spectrum of computer vision and graphics works such as data-driven modeling and scene understanding, object detection and recognition. However, annotating a vast amount of 3D scene data remains challenging due to the lack of an effective tool and/or the complexity of 3D scenes (e.g. clutter, varying illumination conditions). This paper aims to build a robust annotation tool that effectively and conveniently enables the segmentation and annotation of massive 3D data. Our tool works by coupling 2D and 3D information via an interactive framework, through which users can provide high-level semantic annotation for objects. We have experimented our tool and found that a typical indoor scene could be well segmented and annotated in less than 30 minutes by using the tool, as opposed to a few hours if done manually. Along with the tool, we created a dataset of over a hundred 3D scenes associated with complete annotations using our tool. Both the tool and dataset are available at http://scenenn.net.},
  file = {/Users/sunjiaming/Zotero/storage/QYAZNS4H/Nguyen et al. - 2018 - A Robust 3D-2D Interactive Tool for Scene Segmenta.pdf},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  language = {en},
  number = {12}
}

@article{nibaliNumericalCoordinateRegression2018,
  ids = {nibaliNumericalCoordinateRegression2018a},
  title = {Numerical {{Coordinate Regression}} with {{Convolutional Neural Networks}}},
  author = {Nibali, Aiden and He, Zhen and Morgan, Stuart and Prendergast, Luke},
  year = {2018},
  month = jan,
  abstract = {We study deep learning approaches to inferring numerical coordinates for points of interest in an input image. Existing convolutional neural network-based solutions to this problem either take a heatmap matching approach or regress to coordinates with a fully connected output layer. Neither of these approaches is ideal, since the former is not entirely differentiable, and the latter lacks inherent spatial generalization. We propose our differentiable spatial to numerical transform (DSNT) to fill this gap. The DSNT layer adds no trainable parameters, is fully differentiable, and exhibits good spatial generalization. Unlike heatmap matching, DSNT works well with low heatmap resolutions, so it can be dropped in as an output layer for a wide range of existing fully convolutional architectures. Consequently, DSNT offers a better trade-off between inference speed and prediction accuracy compared to existing techniques. When used to replace the popular heatmap matching approach used in almost all state-of-the-art methods for pose estimation, DSNT gives better prediction accuracy for all model architectures tested.},
  archivePrefix = {arXiv},
  eprint = {1801.07372},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Numerical Coordinate Regression with Convolutional Neural Networks-Nibali et al-2018.pdf;/Users/sunjiaming/Zotero/storage/R8L3QMIX/1801.html},
  journal = {arXiv:1801.07372 [cs]},
  primaryClass = {cs}
}

@article{nicastroXSectionCrosssectionPrediction2019,
  title = {X-{{Section}}: {{Cross}}-Section {{Prediction}} for {{Enhanced RGBD Fusion}}},
  shorttitle = {X-{{Section}}},
  author = {Nicastro, Andrea and Clark, Ronald and Leutenegger, Stefan},
  year = {2019},
  month = mar,
  abstract = {Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g.\textbackslash{} in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g.\textbackslash{} for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects.},
  archivePrefix = {arXiv},
  eprint = {1903.00987},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/X-Section-Nicastro et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WU766MPP/1903.html},
  journal = {arXiv:1903.00987 [cs]},
  primaryClass = {cs}
}

@article{niemeyerDifferentiableVolumetricRendering2020,
  title = {Differentiable {{Volumetric Rendering}}: {{Learning Implicit 3D Representations}} without {{3D Supervision}}},
  shorttitle = {Differentiable {{Volumetric Rendering}}},
  author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
  year = {2020},
  month = mar,
  abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
  archivePrefix = {arXiv},
  eprint = {1912.07372},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Differentiable Volumetric Rendering-Niemeyer et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2HWFPNEN/Niemeyer et al. - Supplementary Material for Differentiable Volumetr.pdf;/Users/sunjiaming/Zotero/storage/6G59ZQC7/1912.html},
  journal = {arXiv:1912.07372 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{niessnerRealtime3DReconstruction2013,
  title = {Real-Time {{3D}} Reconstruction at Scale Using Voxel Hashing},
  author = {Nie{\ss}ner, Matthias and Zollh{\"o}fer, Michael and Izadi, Shahram and Stamminger, Marc},
  year = {2013},
  month = nov,
  volume = {32},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/2508363.2508374},
  abstract = {Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-time 3D reconstruction at scale using voxel hashing-Nießner et al-2013.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {neufu_paper},
  language = {en},
  number = {6}
}

@article{nikolicAccelerationNonLinearMinimisation2018,
  title = {Acceleration of {{Non}}-{{Linear Minimisation}} with {{PyTorch}}},
  author = {Nikolic, Bojan},
  year = {2018},
  month = may,
  abstract = {I show that a software framework intended primarily for training of neural networks, PyTorch, is easily applied to a general function minimisation problem in science. The qualities of PyTorch of ease-of-use and very high efficiency are found to be applicable in this domain and lead to two orders of magnitude improvement in time-to-solution with very small software engineering effort.},
  archivePrefix = {arXiv},
  eprint = {1805.07439},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Acceleration of Non-Linear Minimisation with PyTorch-Nikolic-2018.pdf;/Users/sunjiaming/Zotero/storage/7HL4MDFA/1805.html},
  journal = {arXiv:1805.07439 [astro-ph, physics:physics]},
  primaryClass = {astro-ph, physics:physics}
}

@article{nimier-davidMitsubaRetargetableForward,
  title = {Mitsuba 2: {{A Retargetable Forward}} and {{Inverse Renderer}}},
  author = {{Nimier-David}, Merlin and Vicini, Delio and Zeltner, Tizian and Jakob, Wenzel},
  volume = {38},
  pages = {17},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Mitsuba 2-Nimier-David et al-.pdf;/Users/sunjiaming/Zotero/storage/MT3GBUWB/NimierDavidVicini2019Mitsuba2_1.pdf},
  language = {en},
  number = {6}
}

@article{nimier-davidRadiativeBackpropagationAdjoint,
  title = {Radiative {{Backpropagation}}: {{An Adjoint Method}} for {{Lightning}}-{{Fast Di}} Erentiable {{Rendering}}},
  author = {{Nimier-David}, Merlin and Speierer, S{\'e}bastien and Ruiz, Beno{\^i}t and Jakob, Wenzel},
  volume = {39},
  pages = {15},
  file = {/Users/sunjiaming/Zotero/storage/5CFUGFWG/Nimier-David et al. - Radiative Backpropagation An Adjoint Method for L.pdf},
  language = {en},
  number = {4}
}

@incollection{NIPS2018_7512,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {2450--2462},
  publisher = {{Curran Associates, Inc.}}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  annotation = {OCLC: ocm68629100},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Numerical optimization-Nocedal_Wright-2006.pdf},
  isbn = {978-0-387-30303-1},
  language = {en},
  lccn = {QA402.5 .N62 2006},
  series = {Springer Series in Operations Research}
}

@article{novotnyCapturingGeometryObject2018,
  title = {Capturing the {{Geometry}} of {{Object Categories}} from {{Video Supervision}}},
  author = {Novotny, David and Larlus, Diane and Vedaldi, Andrea},
  year = {2018},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2018.2871117},
  abstract = {We propose an unsupervised method to learn the 3D geometry of object categories by looking around them. Differently from traditional approaches, this method does not require CAD models or manual supervision. Instead, using only video sequences showing object instances from a moving viewpoint, the method learns a deep neural network that can predict several aspects of the 3D geometry of such objects from single images. The network has three components. The first is a Siamese viewpoint factorization network that robustly aligns the input videos and learns to predict the absolute viewpoint of the object from a single image. The second is a depth estimation network that performs monocular depth prediction. The third is a shape completion network that predicts the full 3D shape of the object from the output of the monocular depth prediction module. While the three modules solve very different task, we show that they all benefit significantly from allowing networks to perform probabilistic predictions. This results in a self-assessment mechanism which is crucial for obtaining high quality predictions. Our network achieves state-of-the-art results on viewpoint prediction, depth estimation, and 3D point cloud estimation on public benchmarks.},
  file = {/Users/sunjiaming/Zotero/storage/HBH3TQ98/Novotny et al. - 2018 - Capturing the Geometry of Object Categories from V.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en}
}

@article{novotnyLearning3DObject2017,
  title = {Learning {{3D Object Categories}} by {{Looking Around Them}}},
  author = {Novotny, David and Larlus, Diane and Vedaldi, Andrea},
  year = {2017},
  month = may,
  abstract = {Traditional approaches for learning 3D object categories use either synthetic data or manual supervision. In this paper, we propose a method which does not require manual annotations and is instead cued by observing objects from a moving vantage point. Our system builds on two innovations: a Siamese viewpoint factorization network that robustly aligns different videos together without explicitly comparing 3D shapes; and a 3D shape completion network that can extract the full shape of an object from partial observations. We also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as of geometry-aware data augmentation schemes. We obtain state-of-the-art results on publicly-available benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1705.03951},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning 3D Object Categories by Looking Around Them-Novotny et al-2017.pdf;/Users/sunjiaming/Zotero/storage/WUHDU6QL/1705.html},
  journal = {arXiv:1705.03951 [cs]},
  primaryClass = {cs}
}

@article{oechsleTextureFieldsLearning2019,
  title = {Texture {{Fields}}: {{Learning Texture Representations}} in {{Function Space}}},
  shorttitle = {Texture {{Fields}}},
  author = {Oechsle, Michael and Mescheder, Lars and Niemeyer, Michael and Strauss, Thilo and Geiger, Andreas},
  year = {2019},
  month = may,
  abstract = {In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.},
  archivePrefix = {arXiv},
  eprint = {1905.07259},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Texture Fields-Oechsle et al-2019.pdf;/Users/sunjiaming/Zotero/storage/3KGIG4GR/1905.html},
  journal = {arXiv:1905.07259 [cs]},
  primaryClass = {cs}
}

@inproceedings{oleynikovaVoxbloxIncremental3D2017,
  title = {Voxblox: {{Incremental 3D Euclidean Signed Distance Fields}} for on-Board {{MAV}} Planning},
  shorttitle = {Voxblox},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Oleynikova, Helen and Taylor, Zachary and Fehr, Marius and Siegwart, Roland and Nieto, Juan},
  year = {2017},
  month = sep,
  pages = {1366--1373},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  doi = {10.1109/IROS.2017.8202315},
  abstract = {Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and flexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulfill these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs).},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Voxblox-Oleynikova et al-2017.pdf},
  isbn = {978-1-5386-2682-5},
  keywords = {reconstruction},
  language = {en}
}

@article{onoLFNetLearningLocal2018,
  title = {{{LF}}-{{Net}}: {{Learning Local Features}} from {{Images}}},
  shorttitle = {{{LF}}-{{Net}}},
  author = {Ono, Yuki and Trulls, Eduard and Fua, Pascal and Yi, Kwang Moo},
  year = {2018},
  month = nov,
  abstract = {We present a novel deep architecture and a training strategy to learn a local feature pipeline from scratch, using collections of images without the need for human supervision. To do so we exploit depth and relative camera pose cues to create a virtual target that the network should achieve on one image, provided the outputs of the network for the other image. While this process is inherently non-differentiable, we show that we can optimize the network in a two-branch setup by confining it to one branch, while preserving differentiability in the other. We train our method on both indoor and outdoor datasets, with depth data from 3D sensors for the former, and depth estimates from an off-the-shelf Structure-from-Motion solution for the latter. Our models outperform the state of the art on sparse feature matching on both datasets, while running at 60+ fps for QVGA images.},
  archivePrefix = {arXiv},
  eprint = {1805.09662},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LF-Net-Ono et al-2018.pdf;/Users/sunjiaming/Zotero/storage/G6VZB32K/1805.html},
  journal = {arXiv:1805.09662 [cs]},
  primaryClass = {cs}
}

@article{osepLargeScaleObjectMining2019,
  title = {Large-{{Scale Object Mining}} for {{Object Discovery}} from {{Unlabeled Video}}},
  author = {Osep, Aljosa and Voigtlaender, Paul and Luiten, Jonathon and Breuers, Stefan and Leibe, Bastian},
  year = {2019},
  month = feb,
  abstract = {This paper addresses the problem of object discovery from unlabeled driving videos captured in a realistic automotive setting. Identifying recurring object categories in such raw video streams is a very challenging problem. Not only do object candidates first have to be localized in the input images, but many interesting object categories occur relatively infrequently. Object discovery will therefore have to deal with the difficulties of operating in the long tail of the object distribution. We demonstrate the feasibility of performing fully automatic object discovery in such a setting by mining object tracks using a generic object tracker. In order to facilitate further research in object discovery, we release a collection of more than 360,000 automatically mined object tracks from 10+ hours of video data (560,000 frames). We use this dataset to evaluate the suitability of different feature representations and clustering strategies for object discovery.},
  archivePrefix = {arXiv},
  eprint = {1903.00362},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large-Scale Object Mining for Object Discovery from Unlabeled Video-Osep et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Y3JETDKF/1903.html},
  journal = {arXiv:1903.00362 [cs]},
  primaryClass = {cs}
}

@inproceedings{ouerghiComparativeStudyCommercial2020,
  title = {Comparative {{Study}} of a Commercial Tracking Camera and {{ORB}}-{{SLAM2}} for Person Localization},
  booktitle = {15th {{International Conference}} on {{Computer Vision Theory}} and {{Applications}}},
  author = {Ouerghi, Safa and Ragot, Nicolas and Boutteau, R{\'e}mi and Savatier, Xavier},
  year = {2020},
  month = feb,
  pages = {357--364},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Valletta, France}},
  doi = {10.5220/0008980703570364},
  abstract = {Aiming at localizing persons in industrial sites is a major concern towards the development of the factory of the future. During the last years, developments have been made in several active research domains targeting the localization problem, among which the vision-based Simultaneous Localization and Mapping paradigm. This has led to the development of multiple algorithms in this field such as ORB-SLAM2 known to be the most complete method as it incorporates the majority of the state-of-the-art techniques. Recently, new commercial and low-cost systems have also emerged in the market that can estimate the 6-DOF motion. In particular, we refer here to the Intel Realsense T265, a standalone 6-DOF tracking sensor that runs a visual-inertial SLAM algorithm and that accurately estimates the 6-DOF motion as claimed by the Intel company. In this paper, we present an evaluation of the Intel T265 tracking camera by comparing its localization performances to the ORB-SLAM2 algorithm. This benchmarking fits within a specific use-case: the person localization in an industrial site. The experiments have been conducted in a platform equipped with a VICON motion capture system, which physical structure is similar to a one that we could find in an industrial site. The Vicon system is made of fifteen high-speedtracking cameras (100 Hz) which provides highly accurate poses that were used as ground truth reference. The sequences have been recorded using both an Intel RealSense D435 camera to use its stereo images with ORB-SLAM2 and the Intel RealSense T265. The two sets of timestamped poses (VICON and the ones provided by the cameras) were aligned then calibrated using the point set registration method. The Absolute Trajectory Error, the Relative Trajectory Error and the Euclidian Distance Error metrics were employed to benchmark the localization accuracy from ORB-SLAM2 and T265. The results show a competitive accuracy of both systems for a handheld camera in an indoor industrial environment with a better reliability with the T265 Tracking system.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Comparative Study of a commercial tracking camera and ORB-SLAM2 for person-Ouerghi et al-2020.pdf},
  keywords = {benchmarking,Intel T265,ORB-SLAM2,person localization}
}

@article{pahwaLocating3DObject2018,
  title = {Locating {{3D Object Proposals}}: {{A Depth}}-{{Based Online Approach}}},
  shorttitle = {Locating {{3D Object Proposals}}},
  author = {Pahwa, Ramanpreet Singh and Lu, Jiangbo and Jiang, Nianjuan and Ng, Tian Tsong and Do, Minh N.},
  year = {2018},
  month = mar,
  volume = {28},
  pages = {626--639},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2016.2616143},
  abstract = {2D object proposals, quickly detected regions in an image that likely contain an object of interest, are an effective approach for improving the computational efficiency and accuracy of object detection in color images. In this work, we propose a novel online method that generates 3D object proposals in a RGB-D video sequence. Our main observation is that depth images provide important information about the geometry of the scene. Diverging from the traditional goal of 2D object proposals to provide a high recall (lots of 2D bounding boxes near potential objects), we aim for precise 3D proposals. We leverage on depth information per frame and multi-view scene information to obtain accurate 3D object proposals. Using efficient but robust registration enables us to combine multiple frames of a scene in near real time and generate 3D bounding boxes for potential 3D regions of interest. Using standard metrics, such as Precision-Recall curves and F-measure, we show that the proposed approach is significantly more accurate than the current state-of-the-art techniques. Our online approach can be integrated into SLAM based video processing for quick 3D object localization. Our method takes less than a second in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus, has potential to be used in low-power chips in Unmanned Aerial Vehicles (UAVs), quadcopters, and drones.},
  archivePrefix = {arXiv},
  eprint = {1709.02653},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/PZEGKVSU/Pahwa et al. - 2018 - Locating 3D Object Proposals A Depth-Based Online.pdf},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  language = {en},
  number = {3}
}

@article{pahwaTrackingObjectsUsing2017,
  title = {Tracking Objects Using {{3D}} Object Proposals},
  author = {Pahwa, Ramanpreet Singh and Ng, Tian Tsong and Do, Minh N.},
  year = {2017},
  month = dec,
  pages = {1657--1660},
  doi = {10.1109/APSIPA.2017.8282298},
  abstract = {3D object proposals, quickly detected regions in a 3D scene that likely contain an object of interest, are an effective approach to improve the computational efficiency and accuracy of the object detection framework. In this work, we propose a novel online method that uses our previously developed 3D object proposals, in a RGB-D video sequence, to match and track static objects in the scene using shape matching. Our main observation is that depth images provide important information about the geometry of the scene that is often ignored in object matching techniques. Our method takes less than a second in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus, has potential to be used in low-power chips in Unmanned Aerial Vehicles (UAVs), quadcopters, and drones.},
  archivePrefix = {arXiv},
  eprint = {1712.06780},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tracking objects using 3D object proposals-Pahwa et al-2017.pdf;/Users/sunjiaming/Zotero/storage/Y5JVEPDL/1712.html},
  journal = {2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}
}

@article{palazzoloReFusion3DReconstruction2019,
  title = {{{ReFusion}}: {{3D Reconstruction}} in {{Dynamic Environments}} for {{RGB}}-{{D Cameras Exploiting Residuals}}},
  shorttitle = {{{ReFusion}}},
  author = {Palazzolo, Emanuele and Behley, Jens and Lottes, Philipp and Gigu{\`e}re, Philippe and Stachniss, Cyrill},
  year = {2019},
  month = may,
  abstract = {Mapping and localization are essential capabilities of robotic systems. Although the majority of mapping systems focus on static environments, the deployment in real-world situations requires them to handle dynamic objects. In this paper, we propose an approach for an RGB-D sensor that is able to consistently map scenes containing multiple dynamic elements. For localization and mapping, we employ an efficient direct tracking on the truncated signed distance function (TSDF) and leverage color information encoded in the TSDF to estimate the pose of the sensor. The TSDF is efficiently represented using voxel hashing, with most computations parallelized on a GPU. For detecting dynamics, we exploit the residuals obtained after an initial registration, together with the explicit modeling of free space in the model. We evaluate our approach on existing datasets, and provide a new dataset showing highly dynamic scenes. These experiments show that our approach often surpass other state-of-the-art dense SLAM methods. We make available our dataset with the ground truth for both the trajectory of the RGB-D sensor obtained by a motion capture system and the model of the static environment using a high-precision terrestrial laser scanner. Finally, we release our approach as open source code.},
  archivePrefix = {arXiv},
  eprint = {1905.02082},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ReFusion-Palazzolo et al-2019.pdf;/Users/sunjiaming/Zotero/storage/S2VZBCW9/1905.html},
  journal = {arXiv:1905.02082 [cs]},
  primaryClass = {cs}
}

@article{pangLibraRCNNBalanced2019,
  title = {Libra {{R}}-{{CNN}}: {{Towards Balanced Learning}} for {{Object Detection}}},
  shorttitle = {Libra {{R}}-{{CNN}}},
  author = {Pang, Jiangmiao and Chen, Kai and Shi, Jianping and Feng, Huajun and Ouyang, Wanli and Lin, Dahua},
  year = {2019},
  month = apr,
  abstract = {Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection. It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN Faster R-CNN and RetinaNet respectively on MSCOCO.},
  archivePrefix = {arXiv},
  eprint = {1904.02701},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Libra R-CNN-Pang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2ICTVU9K/1904.html},
  journal = {arXiv:1904.02701 [cs]},
  keywords = {2d detection},
  primaryClass = {cs}
}

@article{panIterativeGlobalSimilarity2018,
  title = {Iterative {{Global Similarity Points}} : {{A}} Robust Coarse-to-Fine Integration Solution for Pairwise {{3D}} Point Cloud Registration},
  shorttitle = {Iterative {{Global Similarity Points}}},
  author = {Pan, Yue and Yang, Bisheng and Liang, Fuxun and Dong, Zhen},
  year = {2018},
  month = aug,
  abstract = {In this paper, we propose a coarse-to-fine integration solution inspired by the classical ICP algorithm, to pairwise 3D point cloud registration with two improvements of hybrid metric spaces (eg, BSC feature and Euclidean geometry spaces) and globally optimal correspondences matching. First, we detect the keypoints of point clouds and use the Binary Shape Context (BSC) descriptor to encode their local features. Then, we formulate the correspondence matching task as an energy function, which models the global similarity of keypoints on the hybrid spaces of BSC feature and Euclidean geometry. Next, we estimate the globally optimal correspondences through optimizing the energy function by the Kuhn-Munkres algorithm and then calculate the transformation based on the correspondences. Finally,we iteratively refine the transformation between two point clouds by conducting optimal correspondences matching and transformation calculation in a mutually reinforcing manner, to achieve the coarse-to-fine registration under an unified framework.The proposed method is evaluated and compared to several state-of-the-art methods on selected challenging datasets with repetitive, symmetric and incomplete structures.Comprehensive experiments demonstrate that the proposed IGSP algorithm obtains good performance and outperforms the state-of-the-art methods in terms of both rotation and translation errors.},
  archivePrefix = {arXiv},
  eprint = {1808.03899},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Iterative Global Similarity Points -Pan et al-2018.pdf;/Users/sunjiaming/Zotero/storage/U2SW5BFD/1808.html},
  journal = {arXiv:1808.03899 [cs]},
  primaryClass = {cs}
}

@article{panSpatialDeepSpatial2017,
  title = {Spatial {{As Deep}}: {{Spatial CNN}} for {{Traffic Scene Understanding}}},
  shorttitle = {Spatial {{As Deep}}},
  author = {Pan, Xingang and Shi, Jianping and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2017},
  month = dec,
  abstract = {Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-byslice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset1. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7\% and 4.6\% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53\%.},
  archivePrefix = {arXiv},
  eprint = {1712.06080},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Spatial As Deep-Pan et al-2017.pdf},
  journal = {arXiv:1712.06080 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{panSwitchableWhiteningDeep2019,
  title = {Switchable {{Whitening}} for {{Deep Representation Learning}}},
  author = {Pan, Xingang and Zhan, Xiaohang and Shi, Jianping and Tang, Xiaoou and Luo, Ping},
  year = {2019},
  month = apr,
  abstract = {Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33\% mIoU on the ADE20K dataset. Code and models will be released.},
  archivePrefix = {arXiv},
  eprint = {1904.09739},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Switchable Whitening for Deep Representation Learning-Pan et al-2019.pdf;/Users/sunjiaming/Zotero/storage/PT5A6U7P/1904.html},
  journal = {arXiv:1904.09739 [cs]},
  primaryClass = {cs}
}

@article{papandreouPersonLabPersonPose2018,
  title = {{{PersonLab}}: {{Person Pose Estimation}} and {{Instance Segmentation}} with a {{Bottom}}-{{Up}}, {{Part}}-{{Based}}, {{Geometric Embedding Model}}},
  shorttitle = {{{PersonLab}}},
  author = {Papandreou, George and Zhu, Tyler and Chen, Liang-Chieh and Gidaris, Spyros and Tompson, Jonathan and Murphy, Kevin},
  year = {2018},
  month = mar,
  abstract = {We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.},
  archivePrefix = {arXiv},
  eprint = {1803.08225},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PersonLab-Papandreou et al-2018.pdf;/Users/sunjiaming/Zotero/storage/37CD7W7N/1803.html},
  journal = {arXiv:1803.08225 [cs]},
  primaryClass = {cs}
}

@inproceedings{parkColoredPointCloud2017,
  title = {Colored {{Point Cloud Registration Revisited}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2017},
  month = oct,
  pages = {143--152},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.25},
  abstract = {We present an algorithm for aligning two colored point clouds. The key idea is to optimize a joint photometric and geometric objective that locks the alignment along both the normal direction and the tangent plane. We extend a photometric objective for aligning RGB-D images to point clouds, by locally parameterizing the point cloud with a virtual camera. Experiments demonstrate that our algorithm is more accurate and more robust than prior point cloud registration algorithms, including those that utilize color information. We use the presented algorithms to enhance a state-of-the-art scene reconstruction system. The precision of the resulting system is demonstrated on real-world scenes with accurate ground-truth models.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Colored Point Cloud Registration Revisited-Park et al-2017.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@article{parkDeepSDFLearningContinuous2019,
  title = {{{DeepSDF}}: {{Learning Continuous Signed Distance Functions}} for {{Shape Representation}}},
  shorttitle = {{{DeepSDF}}},
  author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  year = {2019},
  month = jan,
  abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
  archivePrefix = {arXiv},
  eprint = {1901.05103},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepSDF-Park et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LN72EPYE/1901.html},
  journal = {arXiv:1901.05103 [cs]},
  keywords = {3d feature learning,neufu_paper},
  primaryClass = {cs}
}

@article{parkLatentFusionEndtoEndDifferentiable2019,
  title = {{{LatentFusion}}: {{End}}-to-{{End Differentiable Reconstruction}} and {{Rendering}} for {{Unseen Object Pose Estimation}}},
  shorttitle = {{{LatentFusion}}},
  author = {Park, Keunhong and Mousavian, Arsalan and Xiang, Yu and Fox, Dieter},
  year = {2019},
  month = dec,
  abstract = {Current 6D object pose estimation methods usually require a 3D model for each object. These methods also require additional training in order to incorporate new objects. As a result, they are difficult to scale to a large number of objects and cannot be directly applied to unseen objects. In this work, we propose a novel framework for 6D pose estimation of unseen objects. We design an end-to-end neural network that reconstructs a latent 3D representation of an object using a small number of reference views of the object. Using the learned 3D representation, the network is able to render the object from arbitrary views. Using this neural renderer, we directly optimize for pose given an input image. By training our network with a large number of 3D shapes for reconstruction and rendering, our network generalizes well to unseen objects. We present a new dataset for unseen object pose estimation--MOPED. We evaluate the performance of our method for unseen object pose estimation on MOPED as well as the ModelNet dataset.},
  archivePrefix = {arXiv},
  eprint = {1912.00416},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LatentFusion-Park et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LG5AANRG/1912.html},
  journal = {arXiv:1912.00416 [cs]},
  primaryClass = {cs}
}

@article{parkNeuralObjectLearning2020,
  title = {Neural {{Object Learning}} for {{6D Pose Estimation Using}} a {{Few Cluttered Images}}},
  author = {Park, Kiru and Patten, Timothy and Vincze, Markus},
  year = {2020},
  month = may,
  abstract = {Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 10 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene.},
  archivePrefix = {arXiv},
  eprint = {2005.03717},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images-Park et al-2020.pdf;/Users/sunjiaming/Zotero/storage/PGV4AWIN/2005.html},
  journal = {arXiv:2005.03717 [cs]},
  primaryClass = {cs}
}

@article{parkNeuralObjectLearning2020a,
  title = {Neural {{Object Learning}} for {{6D Pose Estimation Using}} a {{Few Cluttered Images}}},
  author = {Park, Kiru and Patten, Timothy and Vincze, Markus},
  year = {2020},
  month = may,
  abstract = {Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 10 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene.},
  archivePrefix = {arXiv},
  eprint = {2005.03717},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images-Park et al-22.pdf;/Users/sunjiaming/Zotero/storage/5LKULXBD/2005.html},
  journal = {arXiv:2005.03717 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{parkPhotoShapePhotorealisticMaterials2018,
  title = {{{PhotoShape}}: {{Photorealistic Materials}} for {{Large}}-{{Scale Shape Collections}}},
  shorttitle = {{{PhotoShape}}},
  author = {Park, Keunhong and Rematas, Konstantinos and Farhadi, Ali and Seitz, Steven M.},
  year = {2018},
  month = dec,
  volume = {37},
  pages = {1--12},
  issn = {07300301},
  doi = {10.1145/3272127.3275066},
  abstract = {Existing online 3D shape repositories contain thousands of 3D models but lack photorealistic appearance. We present an approach to automatically assign high-quality, realistic appearance models to large scale 3D shape collections. The key idea is to jointly leverage three types of online data -- shape collections, material collections, and photo collections, using the photos as reference to guide assignment of materials to shapes. By generating a large number of synthetic renderings, we train a convolutional neural network to classify materials in real photos, and employ 3D-2D alignment techniques to transfer materials to different parts of each shape model. Our system produces photorealistic, relightable, 3D shapes (PhotoShapes).},
  archivePrefix = {arXiv},
  eprint = {1809.09761},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PhotoShape-Park et al-2018.pdf;/Users/sunjiaming/Zotero/storage/RKYR2SKY/1809.html},
  journal = {ACM Transactions on Graphics},
  keywords = {dataset},
  number = {6}
}

@article{parkSeeingWorldBag2020,
  title = {Seeing the {{World}} in a {{Bag}} of {{Chips}}},
  author = {Park, Jeong Joon and Holynski, Aleksander and Seitz, Steve},
  year = {2020},
  month = jan,
  abstract = {We address the dual problems of novel view synthesis and environment reconstruction from hand-held RGBD sensors. Our contributions include 1) modeling highly specular objects, 2) modeling inter-reflections and Fresnel effects, and 3) enabling surface light field reconstruction with the same input needed to reconstruct shape alone. In cases where scene surface has a strong mirror-like material component, we generate highly detailed environment images, revealing room composition, objects, people, buildings, and trees visible through windows. Our approach yields state of the art view synthesis techniques, operates on low dynamic range imagery, and is robust to geometric and calibration errors.},
  archivePrefix = {arXiv},
  eprint = {2001.04642},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Seeing the World in a Bag of Chips-Park et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BMQRJTJN/2001.html},
  journal = {arXiv:2001.04642 [cs]},
  primaryClass = {cs}
}

@article{parmarImageTransformer2018,
  title = {Image {{Transformer}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  year = {2018},
  month = jun,
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  archivePrefix = {arXiv},
  eprint = {1802.05751},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Image Transformer-Parmar et al-2018.pdf;/Users/sunjiaming/Zotero/storage/DLZWHUFT/1802.html},
  journal = {arXiv:1802.05751 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{paschalidouSuperquadricsRevisitedLearning,
  ids = {paschalidouSuperquadricsRevisitedLearning2019},
  title = {Superquadrics {{Revisited}}: {{Learning 3D Shape Parsing}} beyond {{Cuboids}}},
  author = {Paschalidou, Despoina and Ulusoy, Ali Osman and Geiger, Andreas},
  pages = {10},
  abstract = {Abstracting complex 3D shapes with parsimonious partbased representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.},
  archivePrefix = {arXiv},
  eprint = {1904.09970},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Superquadrics Revisited-Paschalidou et al-.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Superquadrics Revisited-Paschalidou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4M9QEVSG/1904.html},
  language = {en}
}

@article{passolunghiSpatialVisualWorking2010,
  title = {Spatial and Visual Working Memory Ability in Children with Difficulties in Arithmetic Word Problem Solving},
  author = {Passolunghi, Maria Chiara and Mammarella, Irene C.},
  year = {2010},
  month = sep,
  volume = {22},
  pages = {944--963},
  issn = {0954-1446, 1464-0635},
  doi = {10.1080/09541440903091127},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Spatial and visual working memory ability in children with difficulties in-Passolunghi_Mammarella-2010.pdf},
  journal = {European Journal of Cognitive Psychology},
  language = {en},
  number = {6}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PyTorch-Paszke et al-2019.pdf;/Users/sunjiaming/Zotero/storage/X6I3XXQ4/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{pathakLearningFeaturesWatching2016,
  title = {Learning {{Features}} by {{Watching Objects Move}}},
  author = {Pathak, Deepak and Girshick, Ross and Doll{\'a}r, Piotr and Darrell, Trevor and Hariharan, Bharath},
  year = {2016},
  month = dec,
  abstract = {This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.},
  archivePrefix = {arXiv},
  eprint = {1612.06370},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Features by Watching Objects Move-Pathak et al-2016.pdf;/Users/sunjiaming/Zotero/storage/QJY59ERS/1612.html},
  journal = {arXiv:1612.06370 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{patilH3DDatasetFullSurround2019,
  title = {The {{H3D Dataset}} for {{Full}}-{{Surround 3D Multi}}-{{Object Detection}} and {{Tracking}} in {{Crowded Urban Scenes}}},
  author = {Patil, Abhishek and Malla, Srikanth and Gang, Haiming and Chen, Yi-Ting},
  year = {2019},
  month = mar,
  abstract = {3D multi-object detection and tracking are crucial for traffic scene understanding. However, the community pays less attention to these areas due to the lack of a standardized benchmark dataset to advance the field. Moreover, existing datasets (e.g., KITTI) do not provide sufficient data and labels to tackle challenging scenes where highly interactive and occluded traffic participants are present. To address the issues, we present the Honda Research Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object detection and tracking dataset collected using a 3D LiDAR scanner. H3D comprises of 160 crowded and highly interactive traffic scenes with a total of 1 million labeled instances in 27,721 frames. With unique dataset size, rich annotations, and complex scenes, H3D is gathered to stimulate research on full-surround 3D multi-object detection and tracking. To effectively and efficiently annotate a large-scale 3D point cloud dataset, we propose a labeling methodology to speed up the overall annotation cycle. A standardized benchmark is created to evaluate full-surround 3D multi-object detection and tracking algorithms. 3D object detection and tracking algorithms are trained and tested on H3D. Finally, sources of errors are discussed for the development of future algorithms.},
  archivePrefix = {arXiv},
  eprint = {1903.01568},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in-Patil et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MN4K9JTE/1903.html},
  journal = {arXiv:1903.01568 [cs]},
  keywords = {dataset},
  primaryClass = {cs}
}

@article{pautratOnlineInvarianceSelection2020,
  title = {Online {{Invariance Selection}} for {{Local Feature Descriptors}}},
  author = {Pautrat, R{\'e}mi and Larsson, Viktor and Oswald, Martin R. and Pollefeys, Marc},
  year = {2020},
  month = jul,
  abstract = {To be invariant, or not to be invariant: that is the question formulated in this work about local descriptors. A limitation of current feature descriptors is the trade-off between generalization and discriminative power: more invariance means less informative descriptors. We propose to overcome this limitation with a disentanglement of invariance in local descriptors and with an online selection of the most appropriate invariance given the context. Our framework consists in a joint learning of multiple local descriptors with different levels of invariance and of meta descriptors encoding the regional variations of an image. The similarity of these meta descriptors across images is used to select the right invariance when matching the local descriptors. Our approach, named Local Invariance Selection at Runtime for Descriptors (LISRD), enables descriptors to adapt to adverse changes in images, while remaining discriminative when invariance is not required. We demonstrate that our method can boost the performance of current descriptors and outperforms state-of-the-art descriptors in several matching tasks, when evaluated on challenging datasets with day-night illumination as well as viewpoint changes.},
  archivePrefix = {arXiv},
  eprint = {2007.08988},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Online Invariance Selection for Local Feature Descriptors-Pautrat et al-2020.pdf;/Users/sunjiaming/Zotero/storage/756HP2RG/2007.html},
  journal = {arXiv:2007.08988 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{pavlakos6DoFObjectPose2017,
  title = {6-{{DoF Object Pose}} from {{Semantic Keypoints}}},
  author = {Pavlakos, Georgios and Zhou, Xiaowei and Chan, Aaron and Derpanis, Konstantinos G. and Daniilidis, Kostas},
  year = {2017},
  month = mar,
  abstract = {This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.},
  archivePrefix = {arXiv},
  eprint = {1703.04670},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/6-DoF Object Pose from Semantic Keypoints-Pavlakos et al-2017.pdf;/Users/sunjiaming/Zotero/storage/D52PYDLP/1703.html},
  journal = {arXiv:1703.04670 [cs]},
  primaryClass = {cs}
}

@article{pavlakosTexturePoseSupervisingHuman2019,
  title = {{{TexturePose}}: {{Supervising Human Mesh Estimation}} with {{Texture Consistency}}},
  shorttitle = {{{TexturePose}}},
  author = {Pavlakos, Georgios and Kolotouros, Nikos and Daniilidis, Kostas},
  year = {2019},
  month = oct,
  abstract = {This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/\textasciitilde pavlakos/projects/texturepose.},
  archivePrefix = {arXiv},
  eprint = {1910.11322},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TexturePose-Pavlakos et al-2019.pdf;/Users/sunjiaming/Zotero/storage/HPX7JJIB/1910.html},
  journal = {arXiv:1910.11322 [cs]},
  primaryClass = {cs}
}

@article{pavlloConvolutionalGenerationTextured2020,
  title = {Convolutional {{Generation}} of {{Textured 3D Meshes}}},
  author = {Pavllo, Dario and Spinks, Graham and Hofmann, Thomas and Moens, Marie-Francine and Lucchi, Aurelien},
  year = {2020},
  month = oct,
  abstract = {While recent generative models for 2D images achieve impressive visual results, they clearly lack the ability to perform 3D reasoning. This heavily restricts the degree of control over generated objects as well as the possible applications of such models. In this work, we bridge this gap by leveraging recent advances in differentiable rendering. We design a framework that can generate triangle meshes and associated high-resolution texture maps, using only 2D supervision from single-view natural images. A key contribution of our work is the encoding of the mesh and texture as 2D representations, which are semantically aligned and can be easily modeled by a 2D convolutional GAN. We demonstrate the efficacy of our method on Pascal3D+ Cars and CUB, both in an unconditional setting and in settings where the model is conditioned on class labels, attributes, and text. Finally, we propose an evaluation methodology that assesses the mesh and texture quality separately.},
  archivePrefix = {arXiv},
  eprint = {2006.07660},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Convolutional Generation of Textured 3D Meshes-Pavllo et al-2020.pdf;/Users/sunjiaming/Zotero/storage/VNWW8RSR/2006.html},
  journal = {arXiv:2006.07660 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{pengConvolutionalOccupancyNetworks2020,
  title = {Convolutional {{Occupancy Networks}}},
  author = {Peng, Songyou and Niemeyer, Michael and Mescheder, Lars and Pollefeys, Marc and Geiger, Andreas},
  year = {2020},
  month = mar,
  abstract = {Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases and Manhattan-world priors, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes and generalizes well from synthetic to real data.},
  archivePrefix = {arXiv},
  eprint = {2003.04618},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Convolutional Occupancy Networks-Peng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/K94EGLRD/2003.html},
  journal = {arXiv:2003.04618 [cs]},
  primaryClass = {cs}
}

@article{pengDeepSnakeRealTime2020,
  title = {Deep {{Snake}} for {{Real}}-{{Time Instance Segmentation}}},
  author = {Peng, Sida and Jiang, Wen and Pi, Huaijin and Bao, Hujun and Zhou, Xiaowei},
  year = {2020},
  month = jan,
  abstract = {This paper introduces a novel contour-based approach named deep snake for real-time instance segmentation. Unlike some recent methods that directly regress the coordinates of the object boundary points from an image, deep snake uses a neural network to iteratively deform an initial contour to the object boundary, which implements the classic idea of snake algorithms with a learning-based approach. For structured feature learning on the contour, we propose to use circular convolution in deep snake, which better exploits the cycle-graph structure of a contour compared against generic graph convolution. Based on deep snake, we develop a two-stage pipeline for instance segmentation: initial contour proposal and contour deformation, which can handle errors in initial object localization. Experiments show that the proposed approach achieves state-of-the-art performances on the Cityscapes, Kins and Sbd datasets while being efficient for real-time instance segmentation, 32.3 fps for 512\$\textbackslash times\$512 images on a 1080Ti GPU. The code will be available at https://github.com/zju3dv/snake/.},
  archivePrefix = {arXiv},
  eprint = {2001.01629},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Snake for Real-Time Instance Segmentation-Peng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9PNHDAIT/2001.html},
  journal = {arXiv:2001.01629 [cs]},
  primaryClass = {cs}
}

@article{pengPVNetPixelwiseVoting2018,
  title = {{{PVNet}}: {{Pixel}}-Wise {{Voting Network}} for {{6DoF Pose Estimation}}},
  shorttitle = {{{PVNet}}},
  author = {Peng, Sida and Liu, Yuan and Huang, Qixing and Bao, Hujun and Zhou, Xiaowei},
  year = {2018},
  month = dec,
  abstract = {This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise unit vectors pointing to the keypoints and use these vectors to vote for keypoint locations using RANSAC. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code will be avaliable at https://zju-3dv.github.io/pvnet/.},
  archivePrefix = {arXiv},
  eprint = {1812.11788},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PVNet-Peng et al-22.pdf;/Users/sunjiaming/Zotero/storage/4MCVSHXY/1812.html},
  journal = {arXiv:1812.11788 [cs]},
  primaryClass = {cs}
}

@article{periyasamyRefining6DObject2019,
  title = {Refining {{6D Object Pose Predictions}} Using {{Abstract Render}}-and-{{Compare}}},
  author = {Periyasamy, Arul Selvam and Schwarz, Max and Behnke, Sven},
  year = {2019},
  month = oct,
  abstract = {Robotic systems often require precise scene analysis capabilities, especially in unstructured, cluttered situations, as occurring in human-made environments. While current deep-learning based methods yield good estimates of object poses, they often struggle with large amounts of occlusion and do not take inter-object effects into account. Vision as inverse graphics is a promising concept for detailed scene analysis. A key element for this idea is a method for inferring scene parameter updates from the rasterized 2D scene. However, the rasterization process is notoriously difficult to invert, both due to the projection and occlusion process, but also due to secondary effects such as lighting or reflections. We propose to remove the latter from the process by mapping the rasterized image into an abstract feature space learned in a self-supervised way from pixel correspondences. Using only a light-weight inverse rendering module, this allows us to refine 6D object pose estimations in highly cluttered scenes by optimizing a simple pixel-wise difference in the abstract image representation. We evaluate our approach on the challenging YCB-Video dataset, where it yields large improvements and demonstrates a large basin of attraction towards the correct object poses.},
  archivePrefix = {arXiv},
  eprint = {1910.03412},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Refining 6D Object Pose Predictions using Abstract Render-and-Compare-Periyasamy et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VSPKFSHK/1910.html},
  journal = {arXiv:1910.03412 [cs]},
  primaryClass = {cs}
}

@article{petercorkeRoboticsVisionControl2012,
  title = {Robotics, {{Vision}} and {{Control}}. {{Fundamental Algorithms}} in {{MATLAB}}},
  author = {{Peter Corke}},
  year = {2012},
  month = oct,
  volume = {39},
  issn = {0143-991X},
  doi = {10.1108/ir.2012.04939faa.005},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robotics, Vision and Control-Peter Corke-2012.pdf},
  journal = {Industrial Robot: An International Journal},
  language = {en},
  number = {6}
}

@article{petersenMatrixCookbook2012,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year = {2012},
  pages = {72},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Matrix Cookbook-Petersen_Pedersen-2012.pdf},
  language = {en}
}

@article{pfrommerTagSLAMRobustSLAM2019,
  title = {{{TagSLAM}}: {{Robust SLAM}} with {{Fiducial Markers}}},
  shorttitle = {{{TagSLAM}}},
  author = {Pfrommer, Bernd and Daniilidis, Kostas},
  year = {2019},
  month = oct,
  abstract = {TagSLAM provides a convenient, flexible, and robust way of performing Simultaneous Localization and Mapping (SLAM) with AprilTag fiducial markers. By leveraging a few simple abstractions (bodies, tags, cameras), TagSLAM provides a front end to the GTSAM factor graph optimizer that makes it possible to rapidly design a range of experiments that are based on tags: full SLAM, extrinsic camera calibration with non-overlapping views, visual localization for ground truth, loop closure for odometry, pose estimation etc. We discuss in detail how TagSLAM initializes the factor graph in a robust way, and present loop closure as an application example. TagSLAM is a ROS based open source package and can be found at https://berndpfrommer.github.io/tagslam\_web.},
  archivePrefix = {arXiv},
  eprint = {1910.00679},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TagSLAM-Pfrommer_Daniilidis-2019.pdf;/Users/sunjiaming/Zotero/storage/TFKER58I/1910.html},
  journal = {arXiv:1910.00679 [cs]},
  primaryClass = {cs}
}

@article{phalakScan2PlanEfficientFloorplan2020,
  title = {{{Scan2Plan}}: {{Efficient Floorplan Generation}} from {{3D Scans}} of {{Indoor Scenes}}},
  shorttitle = {{{Scan2Plan}}},
  author = {Phalak, Ameya and Badrinarayanan, Vijay and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  abstract = {We introduce Scan2Plan, a novel approach for accurate estimation of a floorplan from a 3D scan of the structural elements of indoor environments. The proposed method incorporates a two-stage approach where the initial stage clusters an unordered point cloud representation of the scene into room instances and wall instances using a deep neural network based voting approach. The subsequent stage estimates a closed perimeter, parameterized by a simple polygon, for each individual room by finding the shortest path along the predicted room and wall keypoints. The final floorplan is simply an assembly of all such room perimeters in the global co-ordinate system. The Scan2Plan pipeline produces accurate floorplans for complex layouts, is highly parallelizable and extremely efficient compared to existing methods. The voting module is trained only on synthetic data and evaluated on publicly available Structured3D and BKE datasets to demonstrate excellent qualitative and quantitative results outperforming state-of-the-art techniques.},
  archivePrefix = {arXiv},
  eprint = {2003.07356},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scan2Plan-Phalak et al-2020.pdf;/Users/sunjiaming/Zotero/storage/YAZAXNI7/2003.html},
  journal = {arXiv:2003.07356 [cs]},
  primaryClass = {cs}
}

@article{phamJSIS3DJointSemanticInstance2019,
  ids = {phamJSIS3DJointSemanticInstance},
  title = {{{JSIS3D}}: {{Joint Semantic}}-{{Instance Segmentation}} of {{3D Point Clouds}} with {{Multi}}-{{Task Pointwise Networks}} and {{Multi}}-{{Value Conditional Random Fields}}},
  shorttitle = {{{JSIS3D}}},
  author = {Pham, Quang-Hieu and Nguyen, Duc Thanh and Hua, Binh-Son and Roig, Gemma and Yeung, Sai-Kit},
  year = {2019},
  month = apr,
  abstract = {Deep learning techniques have become the to-go models for most vision-related tasks on 2D images. However, their power has not been fully realised on several tasks in 3D space, e.g., 3D scene understanding. In this work, we jointly address the problems of semantic and instance segmentation of 3D point clouds. Specifically, we develop a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings. We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model. The proposed method is thoroughly evaluated and compared with existing methods on different indoor scene datasets including S3DIS and SceneNN. Experimental results showed the robustness of the proposed joint semantic-instance segmentation scheme over its single components. Our method also achieved state-of-the-art performance on semantic segmentation.},
  archivePrefix = {arXiv},
  eprint = {1904.00699},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/JSIS3D-Pham et al-.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/JSIS3D-Pham et al-2019.pdf;/Users/sunjiaming/Zotero/storage/NAHJFUSZ/1904.html},
  journal = {arXiv:1904.00699 [cs]},
  primaryClass = {cs}
}

@article{pharrPhysicallyBasedRendering,
  title = {Physically {{Based Rendering}}: {{From Theory}} to {{Implementation}}},
  author = {Pharr, Matt},
  pages = {1270},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Physically Based Rendering-Pharr-.pdf},
  language = {en}
}

@article{pidhorskyiAdversarialLatentAutoencoders2020,
  title = {Adversarial {{Latent Autoencoders}}},
  author = {Pidhorskyi, Stanislav and Adjeroh, Donald and Doretto, Gianfranco},
  year = {2020},
  month = apr,
  abstract = {Autoencoder networks are unsupervised approaches aiming at combining generative and representational properties by learning simultaneously an encoder-generator map. Although studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures. We designed two autoencoders: one based on a MLP encoder, and another based on a StyleGAN generator, which we call StyleALAE. We verify the disentanglement properties of both architectures. We show that StyleALAE can not only generate 1024x1024 face images with comparable quality of StyleGAN, but at the same resolution can also produce face reconstructions and manipulations based on real images. This makes ALAE the first autoencoder able to compare with, and go beyond the capabilities of a generator-only type of architecture.},
  archivePrefix = {arXiv},
  eprint = {2004.04467},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Adversarial Latent Autoencoders-Pidhorskyi et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ABQVFRPA/2004.html},
  journal = {arXiv:2004.04467 [cs]},
  primaryClass = {cs}
}

@inproceedings{pighinPracticalLeastsquaresComputer2007,
  title = {Practical Least-Squares for Computer Graphics:},
  shorttitle = {Practical Least-Squares for Computer Graphics},
  booktitle = {{{ACM SIGGRAPH}} 2007 Courses on - {{SIGGRAPH}} '07},
  author = {Pighin, Fred and Lewis, J. P.},
  year = {2007},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{San Diego, California}},
  doi = {10.1145/1281500.1281598},
  abstract = {The course presents an overview of the least-squares technique and its variants. A wide range of problems in computer graphics can be solved using the leastsquares technique (LS). Many graphics problems can be seen as finding the best set of parameters for a model given some data. For instance, a surface can be determined using data and smoothness penalties, a trajectory can be predicted using previous information, joint angles can be determined from end effector positions, etc. All these problems and many others can be formulated as minimizing the sum of squares of the residuals between some features in the model and the data.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Practical least-squares for computer graphics-Pighin_Lewis-2007.pdf},
  isbn = {978-1-4503-1823-5},
  language = {en}
}

@inproceedings{pillaiMonocularSLAMSupported2015,
  title = {Monocular {{SLAM Supported Object Recognition}}},
  booktitle = {Robotics: {{Science}} and {{Systems XI}}},
  author = {Pillai, Sudeep and Leonard, John},
  year = {2015},
  month = jul,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2015.XI.034},
  abstract = {In this work, we develop a monocular SLAMaware object recognition system that is able to achieve considerably stronger recognition performance, as compared to classical object recognition systems that function on a frame-by-frame basis. By incorporating several key ideas including multi-view object proposals and efficient feature encoding methods, our proposed system is able to detect and robustly recognize objects in its environment using a single RGB camera in near-constant time. Through experiments, we illustrate the utility of using such a system to effectively detect and recognize objects, incorporating multiple object viewpoint detections into a unified prediction hypothesis. The performance of the proposed recognition system is evaluated on the UW RGB-D Dataset, showing strong recognition performance and scalable run-time performance compared to current state-of-the-art recognition systems.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Monocular SLAM Supported Object Recognition-Pillai_Leonard-2015.pdf},
  isbn = {978-0-9923747-1-6},
  language = {en}
}

@inproceedings{pizzoliREMODEProbabilisticMonocular2014,
  title = {{{REMODE}}: {{Probabilistic}}, Monocular Dense Reconstruction in Real Time},
  shorttitle = {{{REMODE}}},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
  year = {2014},
  month = may,
  pages = {2609--2616},
  publisher = {{IEEE}},
  address = {{Hong Kong, China}},
  doi = {10.1109/ICRA.2014.6907233},
  abstract = {In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms stateof-the-art techniques in terms of accuracy, while exhibiting high efficiency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation) and the CUDA-based implementation runs at 30Hz on a laptop computer.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/REMODE-Pizzoli et al-2014.pdf},
  isbn = {978-1-4799-3685-4},
  keywords = {neufu_paper},
  language = {en}
}

@article{plautMonocular3DObject2020,
  title = {Monocular {{3D Object Detection}} in {{Cylindrical Images}} from {{Fisheye Cameras}}},
  author = {Plaut, Elad and Yaacov, Erez Ben and Shlomo, Bat El},
  year = {2020},
  month = mar,
  abstract = {Detecting objects in 3D from a monocular camera has been successfully demonstrated using various methods based on convolutional neural networks. These methods have been demonstrated on rectilinear perspective images equivalent to being taken by a pinhole camera, whose geometry is explicitly or implicitly exploited. Such methods fail in images with alternative projections, such as those acquired by fisheye cameras, even when provided with a labeled training set of fisheye images and 3D bounding boxes. In this work, we show how to adapt existing 3D object detection methods to images from fisheye cameras, including in the case that no labeled fisheye data is available for training. We significantly outperform existing art on a benchmark of synthetic data, and we also experiment with an internal dataset of real fisheye images.},
  archivePrefix = {arXiv},
  eprint = {2003.03759},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Monocular 3D Object Detection in Cylindrical Images from Fisheye Cameras-Plaut et al-2020.pdf;/Users/sunjiaming/Zotero/storage/6IC7PA3L/2003.html},
  journal = {arXiv:2003.03759 [cs]},
  primaryClass = {cs}
}

@article{ponObjectCentricStereoMatching2019,
  title = {Object-{{Centric Stereo Matching}} for {{3D Object Detection}}},
  author = {Pon, Alex D. and Ku, Jason and Li, Chengyao and Waslander, Steven L.},
  year = {2019},
  month = sep,
  abstract = {Safe autonomous driving requires reliable 3D object detection-determining the 6 DoF pose and dimensions of objects of interest. Using stereo cameras to solve this task is a cost-effective alternative to the widely used LiDAR sensor. The current state-of-the-art for stereo 3D object detection takes the existing PSMNet stereo matching network, with no modifications, and converts the estimated disparities into a 3D point cloud, and feeds this point cloud into a LiDAR-based 3D object detector. The issue with existing stereo matching networks is that they are designed for disparity estimation, not 3D object detection; the shape and accuracy of object point clouds are not the focus. Stereo matching networks commonly suffer from inaccurate depth estimates at object boundaries, which we define as streaking, because background and foreground points are jointly estimated. Existing networks also penalize disparity instead of the estimated position of object point clouds in their loss functions. We propose a novel 2D box association and object-centric stereo matching method that only estimates the disparities of the objects of interest to address these two issues. Our method achieves state-of-the-art results on the KITTI 3D and BEV benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1909.07566},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object-Centric Stereo Matching for 3D Object Detection-Pon et al-2019.pdf;/Users/sunjiaming/Zotero/storage/EEUQ4MIN/1909.html},
  journal = {arXiv:1909.07566 [cs]},
  primaryClass = {cs}
}

@article{popovCoReNetCoherent3D2020,
  title = {{{CoReNet}}: {{Coherent 3D}} Scene Reconstruction from a Single {{RGB}} Image},
  shorttitle = {{{CoReNet}}},
  author = {Popov, Stefan and Bauszat, Pablo and Ferrari, Vittorio},
  year = {2020},
  month = apr,
  abstract = {Advances in deep learning techniques have allowed recent work to reconstruct the shape of a single object given only one RBG image as input. Building on common encoder-decoder architectures for this task, we propose three extensions: (1) ray-traced skip connections that propagate local 2D information to the output 3D volume in a physically correct manner; (2) a hybrid 3D volume representation that enables building translation equivariant models, while at the same time encoding fine object details without an excessive memory footprint; (3) a reconstruction loss tailored to capture overall object geometry. Furthermore, we adapt our model to address the harder task of reconstructing multiple objects from a single image. We reconstruct all objects jointly in one pass, producing a coherent reconstruction, where all objects live in a single consistent 3D coordinate frame relative to the camera and they do not intersect in 3D space. We also handle occlusions and resolve them by hallucinating the missing object parts in the 3D volume. We validate the impact of our contributions experimentally both on synthetic data from ShapeNet as well as real images from Pix3D. Our method outperforms the state-of-the-art single-object methods on both datasets. Finally, we evaluate performance quantitatively on multiple object reconstruction with synthetic scenes assembled from ShapeNet objects.},
  archivePrefix = {arXiv},
  eprint = {2004.12989},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CoReNet-Popov et al-2020.pdf;/Users/sunjiaming/Zotero/storage/CUUKPL7Z/2004.html},
  journal = {arXiv:2004.12989 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{prabhudesai3DObjectRecognition2020,
  title = {{{3D Object Recognition By Corresponding}} and {{Quantizing Neural 3D Scene Representations}}},
  author = {Prabhudesai, Mihir and Lal, Shamit and Tung, Hsiao-Yu Fish and Harley, Adam W. and Potdar, Shubhankar and Fragkiadaki, Katerina},
  year = {2020},
  month = oct,
  abstract = {We propose a system that learns to detect objects and infer their 3D poses in RGB-D images. Many existing systems can identify objects and infer 3D poses, but they heavily rely on human labels and 3D annotations. The challenge here is to achieve this without relying on strong supervision signals. To address this challenge, we propose a model that maps RGB-D images to a set of 3D visual feature maps in a differentiable fully-convolutional manner, supervised by predicting views. The 3D feature maps correspond to a featurization of the 3D world scene depicted in the images. The object 3D feature representations are invariant to camera viewpoint changes or zooms, which means feature matching can identify similar objects under different camera viewpoints. We can compare the 3D feature maps of two objects by searching alignment across scales and 3D rotations, and, as a result of the operation, we can estimate pose and scale changes without the need for 3D pose annotations. We cluster object feature maps into a set of 3D prototypes that represent familiar objects in canonical scales and orientations. We then parse images by inferring the prototype identity and 3D pose for each detected object. We compare our method to numerous baselines that do not learn 3D feature visual representations or do not attempt to correspond features across scenes, and outperform them by a large margin in the tasks of object retrieval and object pose estimation. Thanks to the 3D nature of the object-centric feature maps, the visual similarity cues are invariant to 3D pose changes or small scale changes, which gives our method an advantage over 2D and 1D methods.},
  archivePrefix = {arXiv},
  eprint = {2010.16279},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Object Recognition By Corresponding and Quantizing Neural 3D Scene-Prabhudesai et al-2020.pdf;/Users/sunjiaming/Zotero/storage/R4WBJTDA/2010.html},
  journal = {arXiv:2010.16279 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{PracticalFoundationsProgramming,
  title = {Practical {{Foundations}} for {{Programming Languages}}: 9781107150300: {{Computer Science Books}} @ {{Amazon}}.Com},
  file = {/Users/sunjiaming/Zotero/storage/MFVPGYIC/(N_A) Robert Harper - Practical Foundations For Programming Languages-Cambridge University Press (2016).pdf;/Users/sunjiaming/Zotero/storage/D3I5E6W6/1107150302.html},
  howpublished = {https://www.amazon.com/Practical-Foundations-Programming-Languages-Robert/dp/1107150302}
}

@article{prakashRePrImprovedTraining2018,
  title = {{{RePr}}: {{Improved Training}} of {{Convolutional Filters}}},
  shorttitle = {{{RePr}}},
  author = {Prakash, Aaditya and Storer, James and Florencio, Dinei and Zhang, Cha},
  year = {2018},
  month = nov,
  abstract = {A well-trained Convolutional Neural Network can easily be pruned without significant loss of performance. This is because of unnecessary overlap in the features captured by the network's filters. Innovations in network architecture such as skip/dense connections and Inception units have mitigated this problem to some extent, but these improvements come with increased computation and memory requirements at run-time. We attempt to address this problem from another angle - not by changing the network structure but by altering the training method. We show that by temporarily pruning and then restoring a subset of the model's filters, and repeating this process cyclically, overlap in the learned features is reduced, producing improved generalization. We show that the existing model-pruning criteria are not optimal for selecting filters to prune in this context and introduce inter-filter orthogonality as the ranking criteria to determine under-expressive filters. Our method is applicable both to vanilla convolutional networks and more complex modern architectures, and improves the performance across a variety of tasks, especially when applied to smaller networks.},
  archivePrefix = {arXiv},
  eprint = {1811.07275},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RePr-Prakash et al-2018.pdf;/Users/sunjiaming/Zotero/storage/HPP2WS63/1811.html},
  journal = {arXiv:1811.07275 [cs]},
  primaryClass = {cs}
}

@inproceedings{prisacariuNonlinearShapeManifolds2011,
  title = {Nonlinear Shape Manifolds as Shape Priors in Level Set Segmentation and Tracking},
  booktitle = {{{CVPR}} 2011},
  author = {Prisacariu, Victor Adrian and Reid, Ian},
  year = {2011},
  month = jun,
  pages = {2185--2192},
  publisher = {{IEEE}},
  address = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995687},
  abstract = {We propose a novel nonlinear, probabilistic and variational method for adding shape information to level setbased segmentation and tracking. Unlike previous work, we represent shapes with elliptic Fourier descriptors and learn their lower dimensional latent space using Gaussian Process Latent Variable Models. Segmentation is done by a nonlinear minimisation of an image-driven energy function in the learned latent space. We combine it with a 2D pose recovery stage, yielding a single, one shot, optimisation of both shape and pose. We demonstrate the performance of our method, both qualitatively and quantitatively, with multiple images, video sequences and latent spaces, capturing both shape kinematics and object class variance.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Nonlinear shape manifolds as shape priors in level set segmentation and tracking-Prisacariu_Reid-2011.pdf},
  isbn = {978-1-4577-0394-2},
  language = {en}
}

@article{prokudinEfficientLearningPoint2019,
  title = {Efficient {{Learning}} on {{Point Clouds}} with {{Basis Point Sets}}},
  author = {Prokudin, Sergey and Lassner, Christoph and Romero, Javier},
  year = {2019},
  month = aug,
  abstract = {With the increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to their unordered structure. One common approach is to apply occupancy grid mapping, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures often use an increased number of parameters and are computationally inefficient. In this work, we propose basis point sets (BPS) as a highly efficient and fully general way to process point clouds with machine learning algorithms. The basis point set representation is a residual representation that can be computed efficiently and can be used with standard neural network architectures and other machine learning algorithms. Using the proposed representation as the input to a simple fully connected network allows us to match the performance of PointNet on a shape classification task while using three orders of magnitude less floating-point operations. In a second experiment, we show how the proposed representation can be used for registering high-resolution meshes to noisy 3D scans. Here, we present the first method for single-pass high-resolution mesh registration, avoiding time-consuming per-scan optimization and allowing real-time execution.},
  archivePrefix = {arXiv},
  eprint = {1908.09186},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Efficient Learning on Point Clouds with Basis Point Sets-Prokudin et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TB2VK7QE/1908.html},
  journal = {arXiv:1908.09186 [cs]},
  primaryClass = {cs}
}

@article{puyFLOTSceneFlow2020,
  title = {{{FLOT}}: {{Scene Flow}} on {{Point Clouds Guided}} by {{Optimal Transport}}},
  shorttitle = {{{FLOT}}},
  author = {Puy, Gilles and Boulch, Alexandre and Marlet, Renaud},
  year = {2020},
  month = jul,
  abstract = {We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT\$\_0\$, which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT.},
  archivePrefix = {arXiv},
  eprint = {2007.11142},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FLOT-Puy et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BKNC5WYX/2007.html},
  journal = {arXiv:2007.11142 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{qi3DMotionDecomposition,
  title = {{{3D Motion Decomposition}} for {{RGBD Future Dynamic Scene Synthesis}}},
  author = {Qi, Xiaojuan and Liu, Zhengzhe and Chen, Qifeng and Jia, Jiaya and Lab, YouTu},
  pages = {10},
  abstract = {A future video is the 2D projection of a 3D scene with predicted camera and object motion. Accurate future video prediction inherently requires understanding of 3D motion and geometry of a scene. In this paper, we propose a RGBD scene forecasting model with 3D motion decomposition. We predict ego-motion and foreground motion that are combined to generate a future 3D dynamic scene, which is then projected into a 2D image plane to synthesize future motion, RGB images and depth maps. Optional semantic maps can be integrated. Experimental results on KITTI and Driving datasets show that our model outperforms other state-ofthe-arts in forecasting future RGBD dynamic scenes.},
  file = {/Users/sunjiaming/Zotero/storage/42877MLD/Qi et al. - 3D Motion Decomposition for RGBD Future Dynamic Sc.pdf},
  language = {en}
}

@article{qiAmodalInstanceSegmentation,
  title = {Amodal {{Instance Segmentation}} with {{KINS Dataset}}},
  author = {Qi, Lu and Jiang, Li and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},
  pages = {9},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Amodal Instance Segmentation with KINS Dataset-Qi et al-.pdf},
  language = {en}
}

@article{qianAssociative3DVolumetricReconstruction2020,
  title = {{{Associative3D}}: {{Volumetric Reconstruction}} from {{Sparse Views}}},
  shorttitle = {{{Associative3D}}},
  author = {Qian, Shengyi and Jin, Linyi and Fouhey, David F.},
  year = {2020},
  month = jul,
  abstract = {This paper studies the problem of 3D volumetric reconstruction from two views of a scene with an unknown camera. While seemingly easy for humans, this problem poses many challenges for computers since it requires simultaneously reconstructing objects in the two views while also figuring out their relationship. We propose a new approach that estimates reconstructions, distributions over the camera/object and camera/camera transformations, as well as an inter-view object affinity matrix. This information is then jointly reasoned over to produce the most likely explanation of the scene. We train and test our approach on a dataset of indoor scenes, and rigorously evaluate the merits of our joint reasoning approach. Our experiments show that it is able to recover reasonable scenes from sparse views, while the problem is still challenging. Project site: https://jasonqsy.github.io/Associative3D},
  archivePrefix = {arXiv},
  eprint = {2007.13727},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Associative3D-Qian et al-2020.pdf;/Users/sunjiaming/Zotero/storage/4VQHL7W9/2007.html},
  journal = {arXiv:2007.13727 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{qiDeepHoughVoting2019,
  title = {Deep {{Hough Voting}} for {{3D Object Detection}} in {{Point Clouds}}},
  author = {Qi, Charles R. and Litany, Or and He, Kaiming and Guibas, Leonidas J.},
  year = {2019},
  month = apr,
  abstract = {Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.},
  archivePrefix = {arXiv},
  eprint = {1904.09664},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Hough Voting for 3D Object Detection in Point Clouds-Qi et al-22.pdf;/Users/sunjiaming/Zotero/storage/76YFFCG3/1904.html},
  journal = {arXiv:1904.09664 [cs]},
  primaryClass = {cs}
}

@article{qiFrustumPointNets3D2017,
  title = {Frustum {{PointNets}} for {{3D Object Detection}} from {{RGB}}-{{D Data}}},
  author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = nov,
  abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
  archivePrefix = {arXiv},
  eprint = {1711.08488},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Frustum PointNets for 3D Object Detection from RGB-D Data-Qi et al-2017.pdf;/Users/sunjiaming/Zotero/storage/43U3JN3A/1711.html},
  journal = {arXiv:1711.08488 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@article{qiImVoteNetBoosting3D2020,
  title = {{{ImVoteNet}}: {{Boosting 3D Object Detection}} in {{Point Clouds}} with {{Image Votes}}},
  shorttitle = {{{ImVoteNet}}},
  author = {Qi, Charles R. and Chen, Xinlei and Litany, Or and Guibas, Leonidas J.},
  year = {2020},
  month = jan,
  abstract = {3D object detection has seen quick progress thanks to advances in deep learning on point clouds. A few recent works have even shown state-of-the-art performance with just point clouds input (e.g. VoteNet). However, point cloud data have inherent limitations. They are sparse, lack color information and often suffer from sensor noise. Images, on the other hand, have high resolution and rich texture. Thus they can complement the 3D geometry provided by point clouds. Yet how to effectively use image information to assist point cloud based detection is still an open question. In this work, we build on top of VoteNet and propose a 3D detection architecture called ImVoteNet specialized for RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes in point clouds. Compared to prior work on multi-modal detection, we explicitly extract both geometric and semantic features from the 2D images. We leverage camera parameters to lift these features to 3D. To improve the synergy of 2D-3D feature fusion, we also propose a multi-tower training scheme. We validate our model on the challenging SUN RGB-D dataset, advancing state-of-the-art results by 5.7 mAP. We also provide rich ablation studies to analyze the contribution of each design choice.},
  archivePrefix = {arXiv},
  eprint = {2001.10692},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Qi et al_2020_ImVoteNet.pdf;/Users/sunjiaming/Zotero/storage/Y8HTTNMS/2001.html},
  journal = {arXiv:2001.10692 [cs]},
  primaryClass = {cs}
}

@article{qinAVPSLAMSemanticVisual2020,
  title = {{{AVP}}-{{SLAM}}: {{Semantic Visual Mapping}} and {{Localization}} for {{Autonomous Vehicles}} in the {{Parking Lot}}},
  shorttitle = {{{AVP}}-{{SLAM}}},
  author = {Qin, Tong and Chen, Tongqing and Chen, Yilun and Su, Qing},
  year = {2020},
  month = jul,
  abstract = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
  archivePrefix = {arXiv},
  eprint = {2007.01813},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/AVP-SLAM-Qin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NZGD6AHK/2007.html},
  journal = {arXiv:2007.01813 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{qinBinaryNeuralNetworks2020,
  title = {Binary {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Binary {{Neural Networks}}},
  author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
  year = {2020},
  month = sep,
  volume = {105},
  pages = {107281},
  issn = {00313203},
  doi = {10.1016/j.patcog.2020.107281},
  abstract = {The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.},
  archivePrefix = {arXiv},
  eprint = {2004.03333},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Binary Neural Networks-Qin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ZMKIYMBK/2004.html},
  journal = {Pattern Recognition},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{qinGeneralOptimizationbasedFramework2019,
  title = {A {{General Optimization}}-Based {{Framework}} for {{Global Pose Estimation}} with {{Multiple Sensors}}},
  author = {Qin, Tong and Cao, Shaozu and Pan, Jie and Shen, Shaojie},
  year = {2019},
  month = jan,
  abstract = {Accurate state estimation is a fundamental problem for autonomous robots. To achieve locally accurate and globally drift-free state estimation, multiple sensors with complementary properties are usually fused together. Local sensors (camera, IMU, LiDAR, etc) provide precise pose within a small region, while global sensors (GPS, magnetometer, barometer, etc) supply noisy but globally drift-free localization in a largescale environment. In this paper, we propose a sensor fusion framework to fuse local states with global sensors, which achieves locally accurate and globally drift-free pose estimation. Local estimations, produced by existing VO/VIO approaches, are fused with global sensors in a pose graph optimization. Within the graph optimization, local estimations are aligned into a global coordinate. Meanwhile, the accumulated drifts are eliminated. We evaluate the performance of our system on public datasets and with real-world experiments. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various global sensors in a unified pose graph optimization. Our implementations are open source1.},
  archivePrefix = {arXiv},
  eprint = {1901.03642},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A General Optimization-based Framework for Global Pose Estimation with Multiple-Qin et al-2019.pdf},
  journal = {arXiv:1901.03642 [cs]},
  keywords = {slam},
  language = {en},
  primaryClass = {cs}
}

@article{qinGeneralOptimizationbasedFramework2019a,
  title = {A {{General Optimization}}-Based {{Framework}} for {{Local Odometry Estimation}} with {{Multiple Sensors}}},
  author = {Qin, Tong and Pan, Jie and Cao, Shaozu and Shen, Shaojie},
  year = {2019},
  month = jan,
  abstract = {Nowadays, more and more sensors are equipped on robots to increase robustness and autonomous ability. We have seen various sensor suites equipped on different platforms, such as stereo cameras on ground vehicles, a monocular camera with an IMU (Inertial Measurement Unit) on mobile phones, and stereo cameras with an IMU on aerial robots. Although many algorithms for state estimation have been proposed in the past, they are usually applied to a single sensor or a specific sensor suite. Few of them can be employed with multiple sensor choices. In this paper, we proposed a general optimization-based framework for odometry estimation, which supports multiple sensor sets. Every sensor is treated as a general factor in our framework. Factors which share common state variables are summed together to build the optimization problem. We further demonstrate the generality with visual and inertial sensors, which form three sensor suites (stereo cameras, a monocular camera with an IMU, and stereo cameras with an IMU). We validate the performance of our system on public datasets and through real-world experiments with multiple sensors. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various sensors in a pose graph optimization. Our implementations are open source1.},
  archivePrefix = {arXiv},
  eprint = {1901.03638},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/A General Optimization-based Framework for Local Odometry Estimation with-Qin et al-2019.pdf},
  journal = {arXiv:1901.03638 [cs]},
  keywords = {neufu_paper,slam},
  language = {en},
  primaryClass = {cs}
}

@article{qinMonoGRNetGeometricReasoning2018,
  title = {{{MonoGRNet}}: {{A Geometric Reasoning Network}} for {{Monocular 3D Object Localization}}},
  shorttitle = {{{MonoGRNet}}},
  author = {Qin, Zengyi and Wang, Jinglu and Lu, Yan},
  year = {2018},
  month = nov,
  abstract = {Localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a single RGB image due to the geometric information loss during imagery projection. We propose MonoGRNet for the amodal 3D object localization from a monocular RGB image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet is a single, unified network composed of four task-specific subnetworks, responsible for 2D object detection, instance depth estimation (IDE), 3D localization and local corner regression. Unlike the pixel-level depth estimation that needs per-pixel annotations, we propose a novel IDE method that directly predicts the depth of the targeting 3D bounding box's center using sparse supervision. The 3D localization is further achieved by estimating the position in the horizontal and vertical dimensions. Finally, MonoGRNet is jointly learned by optimizing the locations and poses of the 3D bounding boxes in the global context. We demonstrate that MonoGRNet achieves state-of-the-art performance on challenging datasets.},
  archivePrefix = {arXiv},
  eprint = {1811.10247},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/MonoGRNet-Qin et al-2018.pdf},
  journal = {arXiv:1811.10247 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{qinTriangulationLearningNetwork,
  title = {Triangulation {{Learning Network}}: From {{Monocular}} to {{Stereo 3D Object Detection}}},
  author = {Qin, Zengyi and Wang, Jinglu and Lu, Yan},
  pages = {9},
  file = {/Users/sunjiaming/Zotero/storage/8MSEJ9WE/Qin et al. - Triangulation Learning Network from Monocular to .pdf},
  language = {en}
}

@article{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: {{Deep Hierarchical Feature Learning}} on {{Point Sets}} in a {{Metric Space}}},
  shorttitle = {{{PointNet}}++},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = jun,
  abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  archivePrefix = {arXiv},
  eprint = {1706.02413},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointNet++-Qi et al-2017.pdf;/Users/sunjiaming/Zotero/storage/NK68Y2M2/1706.html},
  journal = {arXiv:1706.02413 [cs]},
  primaryClass = {cs}
}

@article{qiPointNetDeepLearning2016,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  year = {2016},
  month = dec,
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  archivePrefix = {arXiv},
  eprint = {1612.00593},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointNet-Qi et al-2016.pdf;/Users/sunjiaming/Zotero/storage/7ERESAPN/1612.html},
  journal = {arXiv:1612.00593 [cs]},
  primaryClass = {cs}
}

@article{qiuTracking3DMotion2019,
  title = {Tracking 3-{{D Motion}} of {{Dynamic Objects Using Monocular Visual}}-{{Inertial Sensing}}},
  author = {Qiu, Kejie and Qin, Tong and Gao, Wenliang and Shen, Shaojie},
  year = {2019},
  month = aug,
  volume = {35},
  pages = {799--816},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2019.2909085},
  abstract = {Six degree-of-freedom (6-DoF) visual tracking of dynamic objects is fundamental to a large variety of robotics and augmented reality (AR) applications. A key to this problem is accurate distance measurement of dynamic objects, which is usually obtained via stereo cameras, RGB-D sensors, or LiDARs. In this paper, however, we address the problem using only a monocular camera rigidly mounted with a low-cost inertial measurement unit. This is a light-weight, small-size, and low-cost solution, which is particularly suitable for tracking dynamic objects on drones or on mobile phones. Starting from a generic image-based two-dimensional tracker, we propose a novel method to resolve the object scale ambiguity in monocular vision in a geometric manner based on correlation analysis. This enables accurate metric three-dimensional tracking of arbitrary objects without requiring any prior knowledge about the object shape or size. We discuss the applicability by analyzing the observability condition and degenerated cases for object scale recovery. Simulation and real-world experimental results with ground truth comparison, along with AR application examples, demonstrate the feasibility of the proposed 6-DoF tracking method.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tracking 3-D Motion of Dynamic Objects Using Monocular Visual-Inertial Sensing-Qiu et al-2019.pdf},
  journal = {IEEE Transactions on Robotics},
  language = {en},
  number = {4}
}

@article{quigleyROSOpensourceRobot,
  title = {{{ROS}}: An Open-Source {{Robot Operating System}}},
  author = {Quigley, Morgan and Gerkey, Brian and Conley, Ken and Faust, Josh and Foote, Tully and Leibs, Jeremy and Berger, Eric and Wheeler, Rob and Ng, Andrew},
  pages = {6},
  abstract = {This paper gives an overview of ROS, an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and briefly overview some of the available application software which uses ROS.},
  file = {/Users/sunjiaming/Zotero/storage/YEZ6LMRB/Quigley et al. - ROS an open-source Robot Operating System.pdf},
  language = {en}
}

@article{radosavovicDesigningNetworkDesign2020,
  title = {Designing {{Network Design Spaces}}},
  author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2020},
  month = mar,
  abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.},
  archivePrefix = {arXiv},
  eprint = {2003.13678},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Designing Network Design Spaces-Radosavovic et al-2020.pdf;/Users/sunjiaming/Zotero/storage/IU5B2B9K/2003.html},
  journal = {arXiv:2003.13678 [cs]},
  primaryClass = {cs}
}

@article{rajalinghamLargeScaleHighResolutionComparison2018,
  title = {Large-{{Scale}}, {{High}}-{{Resolution Comparison}} of the {{Core Visual Object Recognition Behavior}} of {{Humans}}, {{Monkeys}}, and {{State}}-of-the-{{Art Deep Artificial Neural Networks}}},
  author = {Rajalingham, Rishi and Issa, Elias B. and Bashivan, Pouya and Kar, Kohitij and Schmidt, Kailyn and DiCarlo, James J.},
  year = {2018},
  month = aug,
  volume = {38},
  pages = {7255--7269},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0388-18.2018},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large-Scale, High-Resolution Comparison of the Core Visual Object Recognition-Rajalingham et al-2018.pdf},
  journal = {The Journal of Neuroscience},
  keywords = {human recognition},
  language = {en},
  number = {33}
}

@article{rangeshGroundPlanePolling2018,
  title = {Ground {{Plane Polling}} for {{6DoF Pose Estimation}} of {{Objects}} on the {{Road}}},
  author = {Rangesh, Akshay and Trivedi, Mohan M.},
  year = {2018},
  month = nov,
  abstract = {This paper introduces an approach to produce accurate 3D detection boxes for objects on the ground using single monocular images. We do so by merging 2D visual cues, 3D object dimensions, and ground plane constraints to produce boxes that are robust against small errors and incorrect predictions. First, we train a single-shot convolutional neural network (CNN) that produces multiple visual and geometric cues of interest: 2D bounding boxes, 2D keypoints of interest, coarse object orientations and object dimensions. Subsets of these cues are then used to poll probable ground planes from a pre-computed database of ground planes, to identify the ``best fit'' plane with highest consensus. Once identified, the ``best fit'' plane provides enough constraints to successfully construct the desired 3D detection box, without directly predicting the 6DoF pose of the object. The entire ground plane polling (GPP) procedure is constructed as a non-parametrized layer of the CNN that outputs the desired ``best fit'' plane and the corresponding 3D keypoints, which together define the final 3D bounding box. This single-stage, single-pass CNN results in superior localization compared to more complex and computationally expensive approaches.},
  archivePrefix = {arXiv},
  eprint = {1811.06666},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Ground Plane Polling for 6DoF Pose Estimation of Objects on the Road-Rangesh_Trivedi-2018.pdf},
  journal = {arXiv:1811.06666 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{rechtTourReinforcementLearning2018,
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  shorttitle = {A {{Tour}} of {{Reinforcement Learning}}},
  author = {Recht, Benjamin},
  year = {2018},
  month = jun,
  abstract = {This manuscript surveys reinforcement learning from the perspective of optimization and control with a focus on continuous control applications. It surveys the general formulation, terminology, and typical experimental implementations of reinforcement learning and reviews competing solution paradigms. In order to compare the relative merits of various techniques, this survey presents a case study of the Linear Quadratic Regulator (LQR) with unknown dynamics, perhaps the simplest and best-studied problem in optimal control. The manuscript describes how merging techniques from learning theory and control can provide non-asymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. This survey concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and control might be combined to approach these challenges.},
  archivePrefix = {arXiv},
  eprint = {1806.09460},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Tour of Reinforcement Learning-Recht-2018.pdf;/Users/sunjiaming/Zotero/storage/URRPMXJC/1806.html},
  journal = {arXiv:1806.09460 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{rematasNeuralVoxelRenderer2019,
  title = {Neural {{Voxel Renderer}}: {{Learning}} an {{Accurate}} and {{Controllable Rendering Tool}}},
  shorttitle = {Neural {{Voxel Renderer}}},
  author = {Rematas, Konstantinos and Ferrari, Vittorio},
  year = {2019},
  month = dec,
  abstract = {We present a neural rendering framework that maps a voxelized scene into a high quality image. Highly-textured objects and scene element interactions are realistically rendered by our method, despite having a rough representation as an input. Moreover, our approach allows controllable rendering: geometric and appearance modifications in the input are accurately propagated to the output. The user can move, rotate and scale an object, change its appearance and texture or modify the position of the light and all these edits are represented in the final rendering. We demonstrate the effectiveness of our approach by rendering scenes with varying appearance, from single color per object to complex, high-frequency textures. We show that our rerendering network can generate very detailed images that represent precisely the appearance of the input scene. Our experiments illustrate that our approach achieves more accurate image synthesis results compared to alternatives and can also handle low voxel grid resolutions. Finally, we show how our neural rendering framework can capture and faithfully render objects from real images and from a diverse set of classes.},
  archivePrefix = {arXiv},
  eprint = {1912.04591},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Voxel Renderer-Rematas_Ferrari-2019.pdf;/Users/sunjiaming/Zotero/storage/B5RMPX8M/1912.html},
  journal = {arXiv:1912.04591 [cs]},
  primaryClass = {cs}
}

@article{remelliMeshSDFDifferentiableIsoSurface2020,
  title = {{{MeshSDF}}: {{Differentiable Iso}}-{{Surface Extraction}}},
  shorttitle = {{{MeshSDF}}},
  author = {Remelli, Edoardo and Lukoianov, Artem and Richter, Stephan R. and Guillard, Beno{\^i}t and Bagautdinov, Timur and Baque, Pierre and Fua, Pascal},
  year = {2020},
  month = jun,
  abstract = {Geometric Deep Learning has recently made striking progress with the advent of continuous Deep Implicit Fields. They allow for detailed modeling of watertight surfaces of arbitrary topology while not relying on a 3D Euclidean grid, resulting in a learnable parameterization that is not limited in resolution. Unfortunately, these methods are often not suitable for applications that require an explicit mesh-based surface representation because converting an implicit field to such a representation relies on the Marching Cubes algorithm, which cannot be differentiated with respect to the underlying implicit field. In this work, we remove this limitation and introduce a differentiable way to produce explicit surface mesh representations from Deep Signed Distance Functions. Our key insight is that by reasoning on how implicit field perturbations impact local surface geometry, one can ultimately differentiate the 3D location of surface samples with respect to the underlying deep implicit field. We exploit this to define MeshSDF, an end-to-end differentiable mesh representation which can vary its topology. We use two different applications to validate our theoretical insight: Single-View Reconstruction via Differentiable Rendering and Physically-Driven Shape Optimization. In both cases our differentiable parameterization gives us an edge over state-of-the-art algorithms.},
  archivePrefix = {arXiv},
  eprint = {2006.03997},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MeshSDF-Remelli et al-2020.pdf;/Users/sunjiaming/Zotero/storage/E5HZMMWD/2006.html},
  journal = {arXiv:2006.03997 [cs]},
  keywords = {30L05,Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.4.8,J.6},
  primaryClass = {cs}
}

@inproceedings{ren3DObjectDetection2018,
  title = {{{3D Object Detection}} with {{Latent Support Surfaces}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ren, Zhile and Sudderth, Erik B.},
  year = {2018},
  month = jun,
  pages = {937--946},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00104},
  abstract = {We develop a 3D object detection algorithm that uses latent support surfaces to capture contextual relationships in indoor scenes. Existing 3D representations for RGB-D images capture the local shape and appearance of object categories, but have limited power to represent objects with different visual styles. The detection of small objects is also challenging because the search space is very large in 3D scenes. However, we observe that much of the shape variation within 3D object categories can be explained by the location of a latent support surface, and smaller objects are often supported by larger objects. Therefore, we explicitly use latent support surfaces to better represent the 3D appearance of large objects, and provide contextual cues to improve the detection of small objects. We evaluate our model with 19 object categories from the SUN RGB-D database, and demonstrate state-of-the-art performance.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Object Detection with Latent Support Surfaces-Ren_Sudderth-2018.pdf},
  isbn = {978-1-5386-6420-9},
  keywords = {3d detection},
  language = {en}
}

@article{renFasterRCNNRealTime2015,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  month = jun,
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archivePrefix = {arXiv},
  eprint = {1506.01497},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Faster R-CNN-Ren et al-2015.pdf;/Users/sunjiaming/Zotero/storage/UGFN3HSH/1506.html},
  journal = {arXiv:1506.01497 [cs]},
  keywords = {2d detection},
  primaryClass = {cs}
}

@article{RepresentationRecognitionSpatial1978,
  title = {Representation and Recognition of the Spatial Organization of Three-Dimensional Shapes},
  year = {1978},
  month = feb,
  volume = {200},
  pages = {269--294},
  issn = {2053-9193},
  doi = {10.1098/rspb.1978.0020},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Representation and recognition of the spatial organization of three-dimensional-1978.pdf},
  journal = {Proceedings of the Royal Society of London. Series B. Biological Sciences},
  language = {en},
  number = {1140}
}

@article{revaudR2D2RepeatableReliable,
  title = {{{R2D2}}: {{Repeatable}} and {{Reliable Detector}} and {{Descriptor}}},
  author = {Revaud, Jerome and Weinzaepfel, Philippe and Souza, C{\'e}sar De and Humenberger, Martin},
  pages = {11},
  abstract = {Interest point detection and local feature description are fundamental steps in many computer vision applications. Classical approaches are based on a detect-thendescribe paradigm where separate handcrafted methods are used to first identify repeatable keypoints and then represent them with a local descriptor. Neural networks trained with metric learning losses have recently caught up with these techniques, focusing on learning repeatable saliency maps for keypoint detection or learning descriptors at the detected keypoint locations. In this work, we argue that repeatable regions are not necessarily discriminative and can therefore lead to select suboptimal keypoints. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence. We thus propose to jointly learn keypoint detection and description together with a predictor of the local descriptor discriminativeness. This allows to avoid ambiguous areas, thus leading to reliable keypoint detection and description. Our detection-and-description approach simultaneously outputs sparse, repeatable and reliable keypoints that outperforms state-of-the-art detectors and descriptors on the HPatches dataset and on the recent Aachen Day-Night localization benchmark.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/R2D2-Revaud et al-.pdf},
  language = {en}
}

@article{rezatofighiGeneralizedIntersectionUnion2019,
  title = {Generalized {{Intersection}} over {{Union}}: {{A Metric}} and {{A Loss}} for {{Bounding Box Regression}}},
  shorttitle = {Generalized {{Intersection}} over {{Union}}},
  author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  year = {2019},
  month = feb,
  abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that \$IoU\$ can be directly used as a regression loss. However, \$IoU\$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of \$IoU\$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized \$IoU\$ (\$GIoU\$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, \$IoU\$ based, and new, \$GIoU\$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
  archivePrefix = {arXiv},
  eprint = {1902.09630},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Generalized Intersection over Union-Rezatofighi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/BMIUZJ7I/1902.html},
  journal = {arXiv:1902.09630 [cs]},
  primaryClass = {cs}
}

@article{ribaKorniaOpenSource2019,
  title = {Kornia: An {{Open Source Differentiable Computer Vision Library}} for {{PyTorch}}},
  shorttitle = {Kornia},
  author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
  year = {2019},
  month = oct,
  abstract = {This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are also provided including a benchmark comparing to existing vision libraries.},
  archivePrefix = {arXiv},
  eprint = {1910.02190},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Kornia-Riba et al-2019.pdf;/Users/sunjiaming/Zotero/storage/NJLQUE5P/1910.html},
  journal = {arXiv:1910.02190 [cs]},
  primaryClass = {cs}
}

@article{robertsHypersimPhotorealisticSynthetic2020,
  title = {Hypersim: {{A Photorealistic Synthetic Dataset}} for {{Holistic Indoor Scene Understanding}}},
  shorttitle = {Hypersim},
  author = {Roberts, Mike and Paczan, Nathan},
  year = {2020},
  month = nov,
  abstract = {For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects. Together, these features make our dataset well-suited for geometric learning problems that require direct 3D supervision, multi-task learning problems that require reasoning jointly over multiple input and output modalities, and inverse rendering problems. We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, annotation effort, and computation time. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a state-of-the-art natural language processing model. All the code we used to generate our dataset will be made available online.},
  archivePrefix = {arXiv},
  eprint = {2011.02523},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Hypersim-Roberts_Paczan-2020.pdf;/Users/sunjiaming/Zotero/storage/3IQLREKL/2011.html},
  journal = {arXiv:2011.02523 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{roccoEfficientNeighbourhoodConsensus2020,
  title = {Efficient {{Neighbourhood Consensus Networks}} via {{Submanifold Sparse Convolutions}}},
  author = {Rocco, Ignacio and Arandjelovi{\'c}, Relja and Sivic, Josef},
  year = {2020},
  month = apr,
  abstract = {In this work we target the problem of estimating accurately localised correspondences between a pair of images. We adopt the recent Neighbourhood Consensus Networks that have demonstrated promising performance for difficult correspondence problems and propose modifications to overcome their main limitations: large memory consumption, large inference time and poorly localised correspondences. Our proposed modifications can reduce the memory footprint and execution time more than \$10\textbackslash times\$, with equivalent results. This is achieved by sparsifying the correlation tensor containing tentative matches, and its subsequent processing with a 4D CNN using submanifold sparse convolutions. Localisation accuracy is significantly improved by processing the input images in higher resolution, which is possible due to the reduced memory footprint, and by a novel two-stage correspondence relocalisation module. The proposed Sparse-NCNet method obtains state-of-the-art results on the HPatches Sequences and InLoc visual localisation benchmarks, and competitive results in the Aachen Day-Night benchmark.},
  archivePrefix = {arXiv},
  eprint = {2004.10566},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Efficient Neighbourhood Consensus Networks via Submanifold Sparse Convolutions-Rocco et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MNDW9PXM/2004.html},
  journal = {arXiv:2004.10566 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{roccoNeighbourhoodConsensusNetworks2018,
  title = {Neighbourhood {{Consensus Networks}}},
  author = {Rocco, Ignacio and Cimpoi, Mircea and Arandjelovi{\'c}, Relja and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
  year = {2018},
  month = oct,
  abstract = {We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints, we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.},
  archivePrefix = {arXiv},
  eprint = {1810.10510},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Neighbourhood Consensus Networks-Rocco et al-2018.pdf},
  journal = {arXiv:1810.10510 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{roddickOrthographicFeatureTransform2018,
  title = {Orthographic {{Feature Transform}} for {{Monocular 3D Object Detection}}},
  author = {Roddick, Thomas and Kendall, Alex and Cipolla, Roberto},
  year = {2018},
  month = nov,
  abstract = {3D object detection from monocular images has proven to be an enormously challenging task, with the performance of leading systems not yet achieving even 10\textbackslash\% of that of LiDAR-based counterparts. One explanation for this performance gap is that existing systems are entirely at the mercy of the perspective image-based representation, in which the appearance and scale of objects varies drastically with depth and meaningful distances are difficult to infer. In this work we argue that the ability to reason about the world in 3D is an essential element of the 3D object detection task. To this end, we introduce the orthographic feature transform, which enables us to escape the image domain by mapping image-based features into an orthographic 3D space. This allows us to reason holistically about the spatial configuration of the scene in a domain where scale is consistent and distances between objects are meaningful. We apply this transformation as part of an end-to-end deep learning architecture and achieve state-of-the-art performance on the KITTI 3D object benchmark.\textbackslash footnote\{We will release full source code and pretrained models upon acceptance of this manuscript for publication.\vphantom\}},
  archivePrefix = {arXiv},
  eprint = {1811.08188},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Orthographic Feature Transform for Monocular 3D Object Detection-Roddick et al-2018.pdf},
  journal = {arXiv:1811.08188 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{rolinekDeepGraphMatching2020,
  title = {Deep {{Graph Matching}} via {{Blackbox Differentiation}} of {{Combinatorial Solvers}}},
  author = {Rol{\'i}nek, Michal and Swoboda, Paul and Zietlow, Dominik and Paulus, Anselm and Musil, V{\'i}t and Martius, Georg},
  year = {2020},
  month = aug,
  abstract = {Building on recent progress at the intersection of combinatorial optimization and deep learning, we propose an end-to-end trainable architecture for deep graph matching that contains unmodified combinatorial solvers. Using the presence of heavily optimized combinatorial solvers together with some improvements in architecture design, we advance state-of-the-art on deep graph matching benchmarks for keypoint correspondence. In addition, we highlight the conceptual advantages of incorporating solvers into deep learning architectures, such as the possibility of post-processing with a strong multi-graph matching solver or the indifference to changes in the training setting. Finally, we propose two new challenging experimental setups. The code is available at https://github.com/martius-lab/blackbox-deep-graph-matching},
  archivePrefix = {arXiv},
  eprint = {2003.11657},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers-Rolínek et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GEFCK5KN/2003.html},
  journal = {arXiv:2003.11657 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{romanoniTAPAMVSTexturelessAwarePAtchMatch,
  title = {{{TAPA}}-{{MVS}}: {{Textureless}}-{{Aware PAtchMatch Multi}}-{{View Stereo}}},
  author = {Romanoni, Andrea and Matteucci, Matteo},
  pages = {10},
  abstract = {One of the most successful approaches in Multi-View Stereo estimates a depth map and a normal map for each view via PatchMatch-based optimization and fuses them into a consistent 3D points cloud. This relies on photoconsistency to evaluate the goodness of a depth estimate. It generally produces very accurate results, however, the reconstructed model often lacks completeness, especially in correspondence of broad untextured areas where the photoconsistency metrics are unreliable. Assuming the untextured areas piecewise planar, in this paper we generate novel PatchMatch hypotheses so to expand reliable depth estimates in neighboring untextured regions. At the same time, we modify the photo-consistency measure such to favor standard or novel PatchMatch depth hypotheses depending on the textureness of the considered area. Finally, we propose a depth refinement step to filter out wrong estimates and to fill gaps on both the depth and normal maps, while preserving discontinuities. Our method proved its effectiveness against several state of the art algorithms in the publicly available ETH3D dataset containing a wide variety of high and low-resolution images.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/TAPA-MVS-Romanoni_Matteucci-.pdf},
  language = {en}
}

@article{romaszkoLearningDirectOptimization2018,
  title = {Learning {{Direct Optimization}} for {{Scene Understanding}}},
  author = {Romaszko, Lukasz and Williams, Christopher K. I. and Winn, John},
  year = {2018},
  month = dec,
  abstract = {We introduce a Learning Direct Optimization method for the refinement of a latent variable model that describes input image x. Our goal is to explain a single image x with a 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g(z), which can be compared to the image x. The standard way to proceed is then to measure the error E(x, g(z)) between the two, and use an optimizer to minimize the error. Our novel Learning Direct Optimization (LiDO) approach trains a Prediction Network to predict an update directly to correct z, rather than minimizing the error with respect to z. Experiments show that our LiDO method converges rapidly as it does not need to perform a search on the error landscape, produces better solutions, and is able to handle the mismatch between the data and the fitted scene model. We apply the LiDO to a realistic synthetic dataset, and show that the method transfers to work well with real images.},
  archivePrefix = {arXiv},
  eprint = {1812.07524},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Direct Optimization for Scene Understanding-Romaszko et al-2018.pdf},
  journal = {arXiv:1812.07524 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{romaszkoVisionasInverseGraphicsObtainingRich2017,
  title = {Vision-as-{{Inverse}}-{{Graphics}}: {{Obtaining}} a {{Rich 3D Explanation}} of a {{Scene}} from a {{Single Image}}},
  shorttitle = {Vision-as-{{Inverse}}-{{Graphics}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Romaszko, Lukasz and Williams, Christopher K. I. and Moreno, Pol and Kohli, Pushmeet},
  year = {2017},
  month = oct,
  pages = {940--948},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCVW.2017.115},
  abstract = {We develop an inverse graphics approach to the problem of scene understanding, obtaining a rich representation that includes descriptions of the objects in the scene and their spatial layout, as well as global latent variables like the camera parameters and lighting. The framework's stages include object detection, the prediction of the camera and lighting variables, and prediction of object-specific variables (shape, appearance and pose). This acts like the encoder of an autoencoder, with graphics rendering as the decoder. Importantly the scene representation is interpretable and is of variable dimension to match the detected number of objects plus the global variables. For the prediction of the camera latent variables we introduce a novel architecture termed Probabilistic HoughNets (PHNs), which provides a principled approach to combining information from multiple detections. We demonstrate the quality of the reconstructions obtained quantitatively on synthetic data, and qualitatively on real scenes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Vision-as-Inverse-Graphics-Romaszko et al-2017.pdf},
  isbn = {978-1-5386-1034-3},
  language = {en}
}

@article{rosenbaumLearningModelsVisual2018,
  title = {Learning Models for Visual {{3D}} Localization with Implicit Mapping},
  author = {Rosenbaum, Dan and Besse, Frederic and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali},
  year = {2018},
  month = jul,
  abstract = {We consider learning based methods for visual localization that do not require the construction of explicit maps in the form of point clouds or voxels. The goal is to learn an implicit representation of the environment at a higher, more abstract level. We propose to use a generative approach based on Generative Query Networks (GQNs, Eslami et al. 2018), asking the following questions: 1) Can GQN capture more complex scenes than those it was originally demonstrated on? 2) Can GQN be used for localization in those scenes? To study this approach we consider procedurally generated Minecraft worlds, for which we can generate images of complex 3D scenes along with camera pose coordinates. We first show that GQNs, enhanced with a novel attention mechanism can capture the structure of 3D scenes in Minecraft, as evidenced by their samples. We then apply the models to the localization problem, comparing the results to a discriminative baseline, and comparing the ways each approach captures the task uncertainty.},
  archivePrefix = {arXiv},
  eprint = {1807.03149},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning models for visual 3D localization with implicit mapping-Rosenbaum et al-2018.pdf;/Users/sunjiaming/Zotero/storage/AEYAQTZD/1807.html},
  journal = {arXiv:1807.03149 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{rosinol3DDynamicScene2020,
  title = {{{3D Dynamic Scene Graphs}}: {{Actionable Spatial Perception}} with {{Places}}, {{Objects}}, and {{Humans}}},
  shorttitle = {{{3D Dynamic Scene Graphs}}},
  author = {Rosinol, Antoni and Gupta, Arjun and Abate, Marcus and Shi, Jingnan and Carlone, Luca},
  year = {2020},
  month = feb,
  abstract = {We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI},
  archivePrefix = {arXiv},
  eprint = {2002.06289},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Dynamic Scene Graphs-Rosinol et al-2020.pdf;/Users/sunjiaming/Zotero/storage/LSBBDIM8/2002.html},
  journal = {arXiv:2002.06289 [cs]},
  primaryClass = {cs}
}

@article{rosinolKimeraOpenSourceLibrary2019,
  title = {Kimera: An {{Open}}-{{Source Library}} for {{Real}}-{{Time Metric}}-{{Semantic Localization}} and {{Mapping}}},
  shorttitle = {Kimera},
  author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
  year = {2019},
  month = oct,
  abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
  archivePrefix = {arXiv},
  eprint = {1910.02490},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Kimera-Rosinol et al-2019.pdf;/Users/sunjiaming/Zotero/storage/XD5BJ6HI/1910.html},
  journal = {arXiv:1910.02490 [cs]},
  primaryClass = {cs}
}

@article{rosuSemiSupervisedSemanticMapping2019,
  title = {Semi-{{Supervised Semantic Mapping}} through {{Label Propagation}} with {{Semantic Texture Meshes}}},
  author = {Rosu, Radu Alexandru and Quenzel, Jan and Behnke, Sven},
  year = {2019},
  month = jun,
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01187-z},
  abstract = {Scene understanding is an important capability for robots acting in unstructured environments. While most SLAM approaches provide a geometrical representation of the scene, a semantic map is necessary for more complex interactions with the surroundings. Current methods treat the semantic map as part of the geometry which limits scalability and accuracy. We propose to represent the semantic map as a geometrical mesh and a semantic texture coupled at independent resolution. The key idea is that in many environments the geometry can be greatly simplified without loosing fidelity, while semantic information can be stored at a higher resolution, independent of the mesh. We construct a mesh from depth sensors to represent the scene geometry and fuse information into the semantic texture from segmentations of individual RGB views of the scene. Making the semantics persistent in a global mesh enables us to enforce temporal and spatial consistency of the individual view predictions. For this, we propose an efficient method of establishing consensus between individual segmentations by iteratively retraining semantic segmentation with the information stored within the map and using the retrained segmentation to re-fuse the semantics. We demonstrate the accuracy and scalability of our approach by reconstructing semantic maps of scenes from NYUv2 and a scene spanning large buildings.},
  archivePrefix = {arXiv},
  eprint = {1906.07029},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Semi-Supervised Semantic Mapping through Label Propagation with Semantic-Rosu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/GI9FEINX/1906.html},
  journal = {International Journal of Computer Vision}
}

@inproceedings{rozumnyiLearnedSemanticMultiSensor2019,
  title = {Learned {{Semantic Multi}}-{{Sensor Depth Map Fusion}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {Rozumnyi, Denys and Cherabier, Ian and Pollefeys, Marc and Oswald, Martin},
  year = {2019},
  month = oct,
  pages = {2089--2099},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCVW.2019.00264},
  abstract = {Volumetric depth map fusion based on truncated signed distance functions has become a standard method and is used in many 3D reconstruction pipelines. In this paper, we are generalizing this classic method in multiple ways: 1) Semantics: Semantic information enriches the scene representation and is incorporated into the fusion process. 2) Multi-Sensor: Depth information can originate from different sensors or algorithms with very different noise and outlier statistics which are considered during data fusion. 3) Scene denoising and completion: Sensors can fail to recover depth for certain materials and light conditions, or data is missing due to occlusions. Our method denoises the geometry, closes holes and computes a watertight surface for every semantic class. 4) Learning: We propose a neural network reconstruction method that unifies all these properties within a single powerful framework. Our method learns sensor or algorithm properties jointly with semantic depth fusion and scene completion and can also be used as an expert system, e.g. to unify the strengths of various photometric stereo algorithms. Our approach is the first to unify all these properties. Experimental evaluations on both synthetic and real data sets demonstrate clear improvements.},
  file = {/Users/sunjiaming/Zotero/storage/YXIZZ9DV/Rozumnyi et al. - 2019 - Learned Semantic Multi-Sensor Depth Map Fusion.pdf},
  isbn = {978-1-72815-023-9},
  language = {en}
}

@inproceedings{runzMaskFusionRealTimeRecognition2018,
  title = {{{MaskFusion}}: {{Real}}-{{Time Recognition}}, {{Tracking}} and {{Reconstruction}} of {{Multiple Moving Objects}}},
  shorttitle = {{{MaskFusion}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {Runz, M. and Buffier, M. and Agapito, L.},
  year = {2018},
  month = oct,
  pages = {10--20},
  doi = {10.1109/ISMAR.2018.00024},
  abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera. As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable realtime object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic. Code will be made available.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MaskFusion-Runz et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MaskFusion-Runz et al-2018.pdf;/Users/sunjiaming/Zotero/storage/JRP6LC5P/8613746.html},
  keywords = {reconstruction}
}

@article{rusinkiewiczSymmetricObjectiveFunction2019,
  title = {A Symmetric Objective Function for {{ICP}}},
  author = {Rusinkiewicz, Szymon},
  year = {2019},
  month = jul,
  volume = {38},
  pages = {1--7},
  issn = {07300301},
  doi = {10.1145/3306346.3323037},
  file = {/Users/sunjiaming/Zotero/storage/NW7TZE2C/Rusinkiewicz - 2019 - A symmetric objective function for ICP.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {4}
}

@article{sabourDynamicRoutingCapsules,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  pages = {11},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Dynamic Routing Between Capsules-Sabour et al-.pdf},
  language = {en}
}

@inproceedings{sadowskiModernCodeReview2018,
  title = {Modern Code Review: A Case Study at Google},
  shorttitle = {Modern Code Review},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering Software Engineering}} in {{Practice}} - {{ICSE}}-{{SEIP}} '18},
  author = {Sadowski, Caitlin and S{\"o}derberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
  year = {2018},
  pages = {181--190},
  publisher = {{ACM Press}},
  address = {{Gothenburg, Sweden}},
  doi = {10.1145/3183519.3183525},
  abstract = {Employing lightweight, tool-based code review of code changes (aka modern code review) has become the norm for a wide variety of open-source and industrial systems. In this paper, we make an exploratory investigation of modern code review at Google. Google introduced code review early on and evolved it over the years; our study sheds light on why Google introduced this practice and analyzes its current status, after the process has been refined through decades of code changes and millions of code reviews. By means of 12 interviews, a survey with 44 respondents, and the analysis of review logs for 9 million reviewed changes, we investigate motivations behind code review at Google, current practices, and developers' satisfaction and challenges.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Modern code review-Sadowski et al-2018.pdf},
  isbn = {978-1-4503-5659-6},
  language = {en}
}

@article{sahinCategorylevel6DObject2018,
  title = {Category-Level {{6D Object Pose Recovery}} in {{Depth Images}}},
  author = {Sahin, Caner and Kim, Tae-Kyun},
  year = {2018},
  month = aug,
  abstract = {Intra-class variations, distribution shifts among source and target domains are the major challenges of category-level tasks. In this study, we address category-level full 6D object pose estimation in the context of depth modality, introducing a novel part-based architecture that can tackle the above-mentioned challenges. Our architecture particularly adapts the distribution shifts arising from shape discrepancies, and naturally removes the variations of texture, illumination, pose, etc., so we call it as "Intrinsic Structure Adaptor (ISA)". We engineer ISA based on the followings: i) "Semantically Selected Centers (SSC)" are proposed in order to define the "6D pose" at the level of categories. ii) 3D skeleton structures, which we derive as shape-invariant features, are used to represent the parts extracted from the instances of given categories, and privileged one-class learning is employed based on these parts. iii) Graph matching is performed during training in such a way that the adaptation/generalization capability of the proposed architecture is improved across unseen instances. Experiments validate the promising performance of the proposed architecture on both synthetic and real datasets.},
  archivePrefix = {arXiv},
  eprint = {1808.00255},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Category-level 6D Object Pose Recovery in Depth Images-Sahin_Kim-2018.pdf;/Users/sunjiaming/Zotero/storage/WDYQUTEX/1808.html},
  journal = {arXiv:1808.00255 [cs]},
  primaryClass = {cs}
}

@article{sahinInstanceCategorylevel6D2019,
  title = {Instance- and {{Category}}-Level {{6D Object Pose Estimation}}},
  author = {Sahin, Caner and {Garcia-Hernando}, Guillermo and Sock, Juil and Kim, Tae-Kyun},
  year = {2019},
  month = mar,
  abstract = {6D object pose estimation is an important task that determines the 3D position and 3D rotation of an object in camera-centred coordinates. By utilizing such a task, one can propose promising solutions for various problems related to scene understanding, augmented reality, control and navigation of robotics. Recent developments on visual depth sensors and low-cost availability of depth data significantly facilitate object pose estimation. Using depth information from RGB-D sensors, substantial progress has been made in the last decade by the methods addressing the challenges such as viewpoint variability, occlusion and clutter, and similar looking distractors. Particularly, with the recent advent of convolutional neural networks, RGB-only based solutions have been presented. However, improved results have only been reported for recovering the pose of known instances, i.e., for the instance-level object pose estimation tasks. More recently, state-of-the-art approaches target to solve object pose estimation problem at the level of categories, recovering the 6D pose of unknown instances. To this end, they address the challenges of the category-level tasks such as distribution shift among source and target domains, high intra-class variations, and shape discrepancies between objects.},
  archivePrefix = {arXiv},
  eprint = {1903.04229},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Instance- and Category-level 6D Object Pose Estimation-Sahin et al-2019.pdf;/Users/sunjiaming/Zotero/storage/G89CF2LE/1903.html},
  journal = {arXiv:1903.04229 [cs]},
  primaryClass = {cs}
}

@article{sahinReviewObjectPose2020,
  title = {A {{Review}} on {{Object Pose Recovery}}: From {{3D Bounding Box Detectors}} to {{Full 6D Pose Estimators}}},
  shorttitle = {A {{Review}} on {{Object Pose Recovery}}},
  author = {Sahin, Caner and {Garcia-Hernando}, Guillermo and Sock, Juil and Kim, Tae-Kyun},
  year = {2020},
  month = jan,
  abstract = {Object pose recovery has gained increasing attention in the computer vision field as it has become an important problem in rapidly evolving technological areas related to autonomous driving, robotics, and augmented reality. Existing review-related studies have addressed the problem at visual level in 2D, going through the methods which produce 2D bounding boxes of objects of interest in RGB images. The 2D search space is enlarged either using the geometry information available in the 3D space along with RGB (Mono/Stereo) images, or utilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box detectors, producing category-level amodal 3D bounding boxes, are evaluated on gravity aligned images, while full 6D object pose estimators are mostly tested at instance-level on the images where the alignment constraint is removed. Recently, 6D object pose estimation is tackled at the level of categories. In this paper, we present the first comprehensive and most recent review of the methods on object pose recovery, from 3D bounding box detectors to full 6D pose estimators. The methods mathematically model the problem as a classification, regression, classification \& regression, template matching, and point-pair feature matching task. Based on this, a mathematical-model-based categorization of the methods is established. Datasets used for evaluating the methods are investigated with respect to the challenges, and evaluation metrics are studied. Quantitative results of experiments in the literature are analysed to show which category of methods best performs across what types of challenges. The analyses are further extended comparing two methods, which are our own implementations, so that the outcomes from the public results are further solidified. Current position of the field is summarized regarding object pose recovery, and possible research directions are identified.},
  archivePrefix = {arXiv},
  eprint = {2001.10609},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Sahin et al_2020_A Review on Object Pose Recovery.pdf;/Users/sunjiaming/Zotero/storage/2LNGUTW5/2001.html},
  journal = {arXiv:2001.10609 [cs]},
  primaryClass = {cs}
}

@article{saikiaAutoDispNetImprovingDisparity2019,
  title = {{{AutoDispNet}}: {{Improving Disparity Estimation}} with {{AutoML}}},
  shorttitle = {{{AutoDispNet}}},
  author = {Saikia, Tonmoy and Marrakchi, Yassine and Zela, Arber and Hutter, Frank and Brox, Thomas},
  year = {2019},
  month = may,
  abstract = {Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large company-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {1905.07443},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/AutoDispNet-Saikia et al-2019.pdf;/Users/sunjiaming/Zotero/storage/S6XL6T6I/1905.html},
  journal = {arXiv:1905.07443 [cs]},
  primaryClass = {cs}
}

@article{saitoPIFuPixelAlignedImplicit2019,
  title = {{{PIFu}}: {{Pixel}}-{{Aligned Implicit Function}} for {{High}}-{{Resolution Clothed Human Digitization}}},
  shorttitle = {{{PIFu}}},
  author = {Saito, Shunsuke and Huang, Zeng and Natsume, Ryota and Morishima, Shigeo and Kanazawa, Angjoo and Li, Hao},
  year = {2019},
  month = may,
  abstract = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.},
  archivePrefix = {arXiv},
  eprint = {1905.05172},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PIFu-Saito et al-2019.pdf;/Users/sunjiaming/Zotero/storage/X6LPBS7Z/1905.html},
  journal = {arXiv:1905.05172 [cs]},
  primaryClass = {cs}
}

@article{saranAUGMENTEDANNOTATIONSINDOOR,
  title = {{{AUGMENTED ANNOTATIONS}}: {{INDOOR DATASET GENERATION WITH AUGMENTED REALITY}}},
  author = {Saran, Vedant and Lin, James and Zakhor, Avideh},
  pages = {7},
  abstract = {The proliferation of machine learning applied to 3D computer vision tasks such as object detection has heightened the need for large, high-quality datasets of labeled 3D scans for training and testing purposes. Current methods of producing these datasets require first scanning the environment, then transferring the resulting point cloud or mesh to a separate tool for it to be annotated with semantic information, both of which are time consuming processes. In this paper, we introduce Augmented Annotations, a novel approach to bounding box data annotation that solves the scanning and annotation processes of an environment in parallel. Leveraging knowledge of the user's position in 3D space during scanning, we use augmented reality (AR) to place persistent digital annotations directly on top of indoor real world objects. We test our system with seven human subjects, and demonstrate that this approach can produce annotated 3D data faster than the state-of-the-art. Additionally, we show that Augmented Annotations can also be adapted to automatically produce 2D labeled image data from many viewpoints, a much needed augmentation technique for 2D object detection and recognition. Finally, we release our work to the public as an open-source iPad application designed for efficient 3D data collection.},
  file = {/Users/sunjiaming/Zotero/storage/D9VCLZZR/Saran et al. - AUGMENTED ANNOTATIONS INDOOR DATASET GENERATION W.pdf},
  language = {en}
}

@article{sarlinSuperGlueLearningFeature2019,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2019},
  month = nov,
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems.},
  archivePrefix = {arXiv},
  eprint = {1911.11763},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/QHAJ86XF/1911.html},
  journal = {arXiv:1911.11763 [cs]},
  primaryClass = {cs}
}

@article{sarlinSuperGlueLearningFeature2020,
  title = {{{SuperGlue}}: {{Learning Feature Matching}} with {{Graph Neural Networks}}},
  shorttitle = {{{SuperGlue}}},
  author = {Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  abstract = {This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork.},
  archivePrefix = {arXiv},
  eprint = {1911.11763},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SuperGlue-Sarlin et al-2020.pdf},
  journal = {arXiv:1911.11763 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{sarodePCRNetPointCloud2019,
  title = {{{PCRNet}}: {{Point Cloud Registration Network}} Using {{PointNet Encoding}}},
  shorttitle = {{{PCRNet}}},
  author = {Sarode, Vinit and Li, Xueqian and Goforth, Hunter and Aoki, Yasuhiro and Srivatsan, Rangaprasad Arun and Lucey, Simon and Choset, Howie},
  year = {2019},
  month = aug,
  abstract = {PointNet has recently emerged as a popular representation for unstructured point cloud data, allowing application of deep learning to tasks such as object detection, segmentation and shape completion. However, recent works in literature have shown the sensitivity of the PointNet representation to pose misalignment. This paper presents a novel framework that uses the PointNet representation to align point clouds and perform registration for applications such as tracking, 3D reconstruction and pose estimation. We develop a framework that compares PointNet features of template and source point clouds to find the transformation that aligns them accurately. Depending on the prior information about the shape of the object formed by the point clouds, our framework can produce approaches that are shape specific or general to unseen shapes. The shape specific approach uses a Siamese architecture with fully connected (FC) layers and is robust to noise and initial misalignment in data. We perform extensive simulation and real-world experiments to validate the efficacy of our approach and compare the performance with state-of-art approaches.},
  archivePrefix = {arXiv},
  eprint = {1908.07906},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PCRNet-Sarode et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WPH925JV/1908.html},
  journal = {arXiv:1908.07906 [cs]},
  primaryClass = {cs}
}

@inproceedings{sattlerFastImagebasedLocalization2011,
  title = {Fast Image-Based Localization Using Direct {{2D}}-to-{{3D}} Matching},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Sattler, Torsten and Leibe, Bastian and Kobbelt, Leif},
  year = {2011},
  month = nov,
  pages = {667--674},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCV.2011.6126302},
  abstract = {Recently developed Structure from Motion (SfM) reconstruction approaches enable the creation of large scale 3D models of urban scenes. These compact scene representations can then be used for accurate image-based localization, creating the need for localization approaches that are able to efficiently handle such large amounts of data. An important bottleneck is the computation of 2D-to-3D correspondences required for pose estimation. Current stateof-the-art approaches use indirect matching techniques to accelerate this search. In this paper we demonstrate that direct 2D-to-3D matching methods have a considerable potential for improving registration performance. We derive a direct matching framework based on visual vocabulary quantization and a prioritized correspondence search. Through extensive experiments, we show that our framework efficiently handles large datasets and outperforms current state-of-the-art methods.},
  file = {/Users/sunjiaming/Zotero/storage/6ZY9GBIY/Sattler et al. - 2011 - Fast image-based localization using direct 2D-to-3.pdf},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  language = {en}
}

@inproceedings{sattlerFastImagebasedLocalization2011a,
  title = {Fast Image-Based Localization Using Direct {{2D}}-to-{{3D}} Matching},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Sattler, Torsten and Leibe, Bastian and Kobbelt, Leif},
  year = {2011},
  month = nov,
  pages = {667--674},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICCV.2011.6126302},
  abstract = {Recently developed Structure from Motion (SfM) reconstruction approaches enable the creation of large scale 3D models of urban scenes. These compact scene representations can then be used for accurate image-based localization, creating the need for localization approaches that are able to efficiently handle such large amounts of data. An important bottleneck is the computation of 2D-to-3D correspondences required for pose estimation. Current stateof-the-art approaches use indirect matching techniques to accelerate this search. In this paper we demonstrate that direct 2D-to-3D matching methods have a considerable potential for improving registration performance. We derive a direct matching framework based on visual vocabulary quantization and a prioritized correspondence search. Through extensive experiments, we show that our framework efficiently handles large datasets and outperforms current state-of-the-art methods.},
  file = {/Users/sunjiaming/Zotero/storage/T3GPKTT5/Sattler et al. - 2011 - Fast image-based localization using direct 2D-to-3.pdf},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  language = {en}
}

@inproceedings{savarese3DGenericObject2007,
  title = {{{3D}} Generic Object Categorization, Localization and Pose Estimation},
  booktitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  author = {Savarese, Silvio and {Li Fei-Fei}},
  year = {2007},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro, Brazil}},
  doi = {10.1109/ICCV.2007.4408987},
  abstract = {We propose a novel and robust model to represent and learn generic 3D object categories. We aim to solve the problem of true 3D object categorization for handling arbitrary rotations and scale changes. Our approach is to capture a compact model of an object category by linking together diagnostic parts of the objects from different viewing points. We emphasize on the fact that our ``parts'' are large and discriminative regions of the objects that are composed of many local invariant features. Instead of recovering a full 3D geometry, we connect these parts through their mutual homographic transformation. The resulting model is a compact summarization of both the appearance and geometry information of the object class. We propose a framework in which learning is done via minimal supervision compared to previous works. Our results on categorization show superior performances to state-of-the-art algorithms such as [23]. Furthermore, we have compiled a new 3D object dataset that consists of 10 different object categories. We have tested our algorithm on this dataset and have obtained highly promising results.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D generic object categorization, localization and pose estimation-Savarese_Li Fei-Fei-2007.pdf},
  isbn = {978-1-4244-1630-1},
  language = {en}
}

@article{savinovDiscreteOptimizationRay2019,
  title = {Discrete {{Optimization}} of {{Ray Potentials}} for {{Semantic 3D Reconstruction}}},
  author = {Savinov, Nikolay and Ladicky, Lubor and Haene, Christian and Pollefeys, Marc},
  year = {2019},
  month = jun,
  abstract = {Dense semantic 3D reconstruction is typically formulated as a discrete or continuous problem over label assignments in a voxel grid, combining semantic and depth likelihoods in a Markov Random Field framework. The depth and semantic information is incorporated as a unary potential, smoothed by a pairwise regularizer. However, modelling likelihoods as a unary potential does not model the problem correctly leading to various undesirable visibility artifacts. We propose to formulate an optimization problem that directly optimizes the reprojection error of the 3D model with respect to the image estimates, which corresponds to the optimization over rays, where the cost function depends on the semantic class and depth of the first occupied voxel along the ray. The 2-label formulation is made feasible by transforming it into a graph-representable form under QPBO relaxation, solvable using graph cut. The multi-label problem is solved by applying alpha-expansion using the same relaxation in each expansion move. Our method was indeed shown to be feasible in practice, running comparably fast to the competing methods, while not suffering from ray potential approximation artifacts.},
  archivePrefix = {arXiv},
  eprint = {1906.10491},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Discrete Optimization of Ray Potentials for Semantic 3D Reconstruction-Savinov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/SSN49FHA/1906.html},
  journal = {arXiv:1906.10491 [cs]},
  primaryClass = {cs}
}

@article{savinovSemantic3DReconstruction2019,
  title = {Semantic {{3D Reconstruction}} with {{Continuous Regularization}} and {{Ray Potentials Using}} a {{Visibility Consistency Constraint}}},
  author = {Savinov, Nikolay and Haene, Christian and Ladicky, Lubor and Pollefeys, Marc},
  year = {2019},
  month = aug,
  abstract = {We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods.},
  archivePrefix = {arXiv},
  eprint = {1604.02885},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Semantic 3D Reconstruction with Continuous Regularization and Ray Potentials-Savinov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YML49GEB/1604.html},
  journal = {arXiv:1604.02885 [cs]},
  primaryClass = {cs}
}

@article{saxenaPWOC3DDeepOcclusionAware2019,
  title = {{{PWOC}}-{{3D}}: {{Deep Occlusion}}-{{Aware End}}-to-{{End Scene Flow Estimation}}},
  shorttitle = {{{PWOC}}-{{3D}}},
  author = {Saxena, Rohan and Schuster, Ren{\'e} and Wasenm{\"u}ller, Oliver and Stricker, Didier},
  year = {2019},
  month = apr,
  abstract = {In the last few years, convolutional neural networks (CNNs) have demonstrated increasing success at learning many computer vision tasks including dense estimation problems such as optical flow and stereo matching. However, the joint prediction of these tasks, called scene flow, has traditionally been tackled using slow classical methods based on primitive assumptions which fail to generalize. The work presented in this paper overcomes these drawbacks efficiently (in terms of speed and accuracy) by proposing PWOC-3D, a compact CNN architecture to predict scene flow from stereo image sequences in an end-to-end supervised setting. Further, large motion and occlusions are well-known problems in scene flow estimation. PWOC-3D employs specialized design decisions to explicitly model these challenges. In this regard, we propose a novel self-supervised strategy to predict occlusions from images (learned without any labeled occlusion data). Leveraging several such constructs, our network achieves competitive results on the KITTI benchmark and the challenging FlyingThings3D dataset. Especially on KITTI, PWOC-3D achieves the second place among end-to-end deep learning methods with 48 times fewer parameters than the top-performing method.},
  archivePrefix = {arXiv},
  eprint = {1904.06116},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PWOC-3D-Saxena et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PWOC-3D-Saxena et al-22.pdf;/Users/sunjiaming/Zotero/storage/3NAKI8UW/1904.html},
  journal = {arXiv:1904.06116 [cs]},
  primaryClass = {cs}
}

@article{scaramuzzaVisualInertialOdometryAerial2019,
  title = {Visual-{{Inertial Odometry}} of {{Aerial Robots}}},
  author = {Scaramuzza, Davide and Zhang, Zichao},
  year = {2019},
  month = jun,
  abstract = {Visual-Inertial odometry (VIO) is the process of estimating the state (pose and velocity) of an agent (e.g., an aerial robot) by using only the input of one or more cameras plus one or more Inertial Measurement Units (IMUs) attached to it. VIO is the only viable alternative to GPS and lidar-based odometry to achieve accurate state estimation. Since both cameras and IMUs are very cheap, these sensor types are ubiquitous in all today's aerial robots.},
  archivePrefix = {arXiv},
  eprint = {1906.03289},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual-Inertial Odometry of Aerial Robots-Scaramuzza_Zhang-2019.pdf;/Users/sunjiaming/Zotero/storage/3NE3FN47/1906.html},
  journal = {arXiv:1906.03289 [cs]},
  primaryClass = {cs}
}

@article{schmidtSelfSupervisedVisualDescriptor2017,
  title = {Self-{{Supervised Visual Descriptor Learning}} for {{Dense Correspondence}}},
  author = {Schmidt, Tanner and Newcombe, Richard and Fox, Dieter},
  year = {2017},
  month = apr,
  volume = {2},
  pages = {420--427},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2016.2634089},
  abstract = {Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labelled, typically) training data is required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong 3D generative model to automatically label correspondences in RGB-D video data. A fully-convolutional network is trained using a contrastive loss to produce viewpoint- and lighting-invariant descriptors. As a proof of concept, we collected two datasets: the first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects re-arranged within. Our datasets focus on re-visitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes towards identifying non-labelled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-the-art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-Supervised Visual Descriptor Learning for Dense Correspondence-Schmidt et al-2017.pdf},
  journal = {IEEE Robotics and Automation Letters},
  language = {en},
  number = {2}
}

@article{schmittJointEstimationPose,
  title = {On {{Joint Estimation}} of {{Pose}}, {{Geometry}} and {{svBRDF}} from a {{Handheld Scanner}}},
  author = {Schmitt, Carolin and Donne, Simon and Riegler, Gernot and Koltun, Vladlen and Geiger, Andreas},
  pages = {11},
  abstract = {We propose a novel formulation for joint recovery of camera pose, object geometry and spatially-varying BRDF. The input to our approach is a sequence of RGB-D images captured by a mobile, hand-held scanner that actively illuminates the scene with point light sources. Compared to previous works that jointly estimate geometry and materials from a hand-held scanner, we formulate this problem using a single objective function that can be minimized using off-the-shelf gradient-based solvers. By integrating material clustering as a differentiable operation into the optimization process, we avoid pre-processing heuristics and demonstrate that our model is able to determine the correct number of specular materials independently. We provide a study on the importance of each component in our formulation and on the requirements of the initial geometry. We show that optimizing over the poses is crucial for accurately recovering fine details and that our approach naturally results in a semantically meaningful material segmentation.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On Joint Estimation of Pose, Geometry and svBRDF from a Handheld Scanner-Schmitt et al-.pdf},
  language = {en}
}

@article{schollerSimplerBetterConstant2019,
  title = {The {{Simpler}} the {{Better}}: {{Constant Velocity}} for {{Pedestrian Motion Prediction}}},
  shorttitle = {The {{Simpler}} the {{Better}}},
  author = {Sch{\"o}ller, Christoph and Aravantinos, Vincent and Lay, Florian and Knoll, Alois},
  year = {2019},
  month = mar,
  abstract = {Pedestrian motion prediction is a fundamental task for autonomous robots and vehicles to operate safely. In recent years many complex models have been proposed to address this problem. While complex models can be justified, simple models should be preferred given the same or better performance. In this work we show that a simple Constant Velocity Model can achieve competitive performance on this task. We evaluate the Constant Velocity Model using two popular benchmark datasets for pedestrian motion prediction and show that it outperforms state-of-the-art models and several common baselines. The success of this model indicates that either neural networks are not able to make use of the additional information they are provided with, or it is not as relevant as commonly believed. Therefore, we analyze how neural networks process this information and how it impacts their predictions. Our analysis shows that neural networks implicitly learn environmental priors that have a negative impact on their generalization capability, most of the pedestrian's motion history is ignored and interactions - while happening - are too complex to predict. These findings explain the success of the Constant Velocity Model and lead to a better understanding of the problem at hand.},
  archivePrefix = {arXiv},
  eprint = {1903.07933},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Simpler the Better-Schöller et al-2019.pdf;/Users/sunjiaming/Zotero/storage/44PCMDRM/1903.html},
  journal = {arXiv:1903.07933 [cs]},
  primaryClass = {cs}
}

@incollection{schonbergerPixelwiseViewSelection2016,
  title = {Pixelwise {{View Selection}} for {{Unstructured Multi}}-{{View Stereo}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Sch{\"o}nberger, Johannes L. and Zheng, Enliang and Frahm, Jan-Michael and Pollefeys, Marc},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9907},
  pages = {501--518},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46487-9_31},
  abstract = {This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate stateof-the-art performance in terms of accuracy, completeness, and efficiency.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pixelwise View Selection for Unstructured Multi-View Stereo-Schönberger et al-2016.pdf},
  isbn = {978-3-319-46486-2 978-3-319-46487-9},
  keywords = {neufu_paper},
  language = {en}
}

@article{schonbergerSemanticVisualLocalization,
  title = {Semantic {{Visual Localization}}},
  author = {Schonberger, Johannes L and Pollefeys, Marc and Geiger, Andreas and Sattler, Torsten},
  pages = {11},
  abstract = {Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, e.g., in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Semantic Visual Localization-Schonberger et al-.pdf},
  keywords = {semantic slam},
  language = {en}
}

@inproceedings{schonbergerStructurefromMotionRevisited2016,
  title = {Structure-from-{{Motion Revisited}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  year = {2016},
  month = jun,
  pages = {4104--4113},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.445},
  abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
  file = {/Users/sunjiaming/Zotero/storage/ITADPNXC/Schonberger and Frahm - 2016 - Structure-from-Motion Revisited.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}

@inproceedings{schops3DModelingGo2015,
  title = {{{3D Modeling}} on the {{Go}}: {{Interactive 3D Reconstruction}} of {{Large}}-{{Scale Scenes}} on {{Mobile Devices}}},
  shorttitle = {{{3D Modeling}} on the {{Go}}},
  booktitle = {2015 {{International Conference}} on {{3D Vision}}},
  author = {Schops, Thomas and Sattler, Torsten and Hane, Christian and Pollefeys, Marc},
  year = {2015},
  month = oct,
  pages = {291--299},
  publisher = {{IEEE}},
  address = {{Lyon, France}},
  doi = {10.1109/3DV.2015.40},
  abstract = {This paper presents a system for 3D reconstruction of large-scale outdoor scenes based on monocular motion stereo. Ours is the first such system to run at interactive frame rates on a mobile device (Google Project Tango Tablet), thus allowing a user to reconstruct scenes ``on the go'' by simply walking around them. We utilize the device's GPU to compute depth maps using plane sweep stereo. We then fuse the depth maps into a global model of the environment represented as a truncated signed distance function in a spatially hashed voxel grid. We observe that in contrast to reconstructing objects in a small volume of interest, or using the near outlier-free data provided by depth sensors, one can rely less on free-space measurements for suppressing outliers in unbounded large-scale scenes. Consequently, we propose a set of simple filtering operations to remove unreliable depth estimates and experimentally demonstrate the benefit of strongly filtering depth maps. We extensively evaluate the system with real as well as synthetic datasets.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Modeling on the Go-Schops et al-2015.pdf},
  isbn = {978-1-4673-8332-5},
  keywords = {neufu_paper},
  language = {en}
}

@article{schopsBADSLAMBundle,
  ids = {schopsBADSLAMBundlea},
  title = {{{BAD SLAM}}: {{Bundle Adjusted Direct RGB}}-{{D SLAM}}},
  author = {Schops, Thomas and Sattler, Torsten and Pollefeys, Marc},
  pages = {11},
  abstract = {A key component of Simultaneous Localization and Mapping (SLAM) systems is the joint optimization of the estimated 3D map and camera trajectory. Bundle adjustment (BA) is the gold standard for this. Due to the large number of variables in dense RGB-D SLAM, previous work has focused on approximating BA. In contrast, in this paper we present a novel, fast direct BA formulation which we implement in a real-time dense RGB-D SLAM algorithm.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/BAD SLAM-Schops et al-.pdf;/Users/sunjiaming/Zotero/storage/NQTWM5SK/Schops et al. - BAD SLAM Bundle Adjusted Direct RGB-D SLAM.pdf},
  language = {en}
}

@article{schopsSurfelMeshingOnlineSurfelBased2018,
  title = {{{SurfelMeshing}}: {{Online Surfel}}-{{Based Mesh Reconstruction}}},
  shorttitle = {{{SurfelMeshing}}},
  author = {Sch{\"o}ps, Thomas and Sattler, Torsten and Pollefeys, Marc},
  year = {2018},
  month = oct,
  abstract = {We address the problem of mesh reconstruction from live RGB-D video, assuming a calibrated camera and poses provided externally (e.g., by a SLAM system). In contrast to most existing approaches, we do not fuse depth measurements in a volume but in a dense surfel cloud. We asynchronously (re)triangulate the smoothed surfels to reconstruct a surface mesh. This novel approach enables to maintain a dense surface representation of the scene during SLAM which can quickly adapt to loop closures. This is possible by deforming the surfel cloud and asynchronously remeshing the surface where necessary. The surfel-based representation also naturally supports strongly varying scan resolution. In particular, it reconstructs colors at the input camera's resolution. Moreover, in contrast to many volumetric approaches, ours can reconstruct thin objects since objects do not need to enclose a volume. We demonstrate our approach in a number of experiments, showing that it produces reconstructions that are competitive with the state-of-the-art, and we discuss its advantages and limitations. The algorithm (excluding loop closure functionality) is available as open source at https://github.com/puzzlepaint/surfelmeshing.},
  archivePrefix = {arXiv},
  eprint = {1810.00729},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SurfelMeshing-Schöps et al-2018.pdf;/Users/sunjiaming/Zotero/storage/ZXB5YUT8/1810.html},
  journal = {arXiv:1810.00729 [cs]},
  keywords = {reconstruction},
  primaryClass = {cs}
}

@inproceedings{schreiberhuberScalableFusionHighresolutionMeshbased2019,
  title = {{{ScalableFusion}}: {{High}}-Resolution {{Mesh}}-Based {{Real}}-Time {{3D Reconstruction}}},
  shorttitle = {{{ScalableFusion}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Schreiberhuber, Simon and Prankl, Johann and Patten, Timothy and Vincze, Markus},
  year = {2019},
  month = may,
  pages = {140--146},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICRA.2019.8793654},
  abstract = {Dense 3D reconstructions generate globally consisent data of the environment suitable for many robot applications. Current RGB-D based reconstructions, however, only maintain the color resolution equal to the depth resolution of the used sensor. This firmly limits the precision and realism of the generated reconstructions. In this paper we present a real-time approach for creating and maintaining a surface reconstruction in as high as possible geometrical fidelity with full sensor resolution for its colorization (or surface texture). A multi-scale memory management process and a Level of Detail scheme enable equally detailed reconstructions to be generated at small scales, such as objects, as well as large scales, such as rooms or buildings. We showcase the benefit of this novel pipeline with a PrimeSense RGB-D camera as well as combining the depth channel of this camera with a high resolution global shutter camera. Further experiments show that our memory management approach allows us to scale up to larger domains that are not achievable with current state-of-the-art methods.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ScalableFusion-Schreiberhuber et al-2019.pdf},
  isbn = {978-1-5386-6027-0},
  language = {en}
}

@article{schubertTUMVIBenchmark2018,
  title = {The {{TUM VI Benchmark}} for {{Evaluating Visual}}-{{Inertial Odometry}}},
  author = {Schubert, David and Goll, Thore and Demmel, Nikolaus and Usenko, Vladyslav and St{\"u}ckler, J{\"o}rg and Cremers, Daniel},
  year = {2018},
  month = oct,
  pages = {1680--1687},
  doi = {10.1109/IROS.2018.8593419},
  abstract = {Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.},
  archivePrefix = {arXiv},
  eprint = {1804.06120},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The TUM VI Benchmark for Evaluating Visual-Inertial Odometry-Schubert et al-2018.pdf;/Users/sunjiaming/Zotero/storage/QSUP2H9U/1804.html},
  journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@article{schultDualConvMeshNetJointGeodesic2020,
  title = {{{DualConvMesh}}-{{Net}}: {{Joint Geodesic}} and {{Euclidean Convolutions}} on {{3D Meshes}}},
  shorttitle = {{{DualConvMesh}}-{{Net}}},
  author = {Schult, Jonas and Engelmann, Francis and Kontogianni, Theodora and Leibe, Bastian},
  year = {2020},
  month = apr,
  abstract = {We propose DualConvMesh-Nets (DCM-Net) a family of deep hierarchical convolutional networks over 3D geometric data that combines two types of convolutions. The first type, geodesic convolutions, defines the kernel weights over mesh surfaces or graphs. That is, the convolutional kernel weights are mapped to the local surface of a given mesh. The second type, Euclidean convolutions, is independent of any underlying mesh structure. The convolutional kernel is applied on a neighborhood obtained from a local affinity representation based on the Euclidean distance between 3D points. Intuitively, geodesic convolutions can easily separate objects that are spatially close but have disconnected surfaces, while Euclidean convolutions can represent interactions between nearby objects better, as they are oblivious to object surfaces. To realize a multi-resolution architecture, we borrow well-established mesh simplification methods from the geometry processing domain and adapt them to define mesh-preserving pooling and unpooling operations. We experimentally show that combining both types of convolutions in our architecture leads to significant performance gains for 3D semantic segmentation, and we report competitive results on three scene segmentation benchmarks. Our models and code are publicly available.},
  archivePrefix = {arXiv},
  eprint = {2004.01002},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DualConvMesh-Net-Schult et al-2020.pdf;/Users/sunjiaming/Zotero/storage/TA45B5FZ/2004.html},
  journal = {arXiv:2004.01002 [cs]},
  primaryClass = {cs}
}

@article{schwaneckeErstgutachterProfDr,
  title = {{Erstgutachter: Prof. Dr. Elmar Scho\textasciidieresis mer}},
  author = {Schwanecke, Dr Ulrich},
  pages = {222},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Erstgutachter-Schwanecke-.pdf},
  language = {de}
}

@book{scottProgrammingLanguagePragmatics2016,
  title = {Programming Language Pragmatics},
  author = {Scott, Michael Lee},
  year = {2016},
  edition = {Fourth edition},
  publisher = {{Morgan Kaufmann, an imprint of Elsevier}},
  address = {{Waltham, MA}},
  annotation = {OCLC: ocn933264678},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Programming language pragmatics-Scott-2016.pdf},
  isbn = {978-0-12-410409-9},
  language = {en},
  lccn = {QA76.7 .S38 2016}
}

@article{senguptaNeuralInverseRendering2019,
  title = {Neural {{Inverse Rendering}} of an {{Indoor Scene}} from a {{Single Image}}},
  author = {Sengupta, Soumyadip and Gu, Jinwei and Kim, Kihwan and Liu, Guilin and Jacobs, David W. and Kautz, Jan},
  year = {2019},
  month = sep,
  abstract = {Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning-based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.},
  archivePrefix = {arXiv},
  eprint = {1901.02453},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Inverse Rendering of an Indoor Scene from a Single Image-Sengupta et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VYH7IJ6F/1901.html},
  journal = {arXiv:1901.02453 [cs]},
  primaryClass = {cs}
}

@article{shahDeepContinuousClustering2018,
  title = {Deep {{Continuous Clustering}}},
  author = {Shah, Sohil Atul and Koltun, Vladlen},
  year = {2018},
  month = mar,
  abstract = {Clustering high-dimensional datasets is hard because interpoint distances become less informative in high-dimensional spaces. We present a clustering algorithm that performs nonlinear dimensionality reduction and clustering jointly. The data is embedded into a lower-dimensional space by a deep autoencoder. The autoencoder is optimized as part of the clustering process. The resulting network produces clustered data. The presented approach does not rely on prior knowledge of the number of ground-truth clusters. Joint nonlinear dimensionality reduction and clustering are formulated as optimization of a global continuous objective. We thus avoid discrete reconfigurations of the objective that characterize prior clustering algorithms. Experiments on datasets from multiple domains demonstrate that the presented algorithm outperforms state-of-the-art clustering schemes, including recent methods that use deep networks.},
  archivePrefix = {arXiv},
  eprint = {1803.01449},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Continuous Clustering-Shah_Koltun-2018.pdf;/Users/sunjiaming/Zotero/storage/IUBGFWHH/1803.html},
  journal = {arXiv:1803.01449 [cs]},
  primaryClass = {cs}
}

@article{shallueMeasuringEffectsData2018,
  title = {Measuring the {{Effects}} of {{Data Parallelism}} on {{Neural Network Training}}},
  author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and {Sohl-Dickstein}, Jascha and Frostig, Roy and Dahl, George E.},
  year = {2018},
  month = nov,
  abstract = {Recent hardware developments have made unprecedented amounts of data parallelism available for accelerating neural network training. Among the simplest ways to harness nextgeneration accelerators is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured in the number of steps necessary to reach a goal out-of-sample error. Eventually, increasing the batch size will no longer reduce the number of training steps required, but the exact relationship between the batch size and how many training steps are necessary is of critical importance to practitioners, researchers, and hardware designers alike. We study how this relationship varies with the training algorithm, model, and data set and find extremely large variation between workloads. Along the way, we reconcile disagreements in the literature on whether batch size affects model quality. Finally, we discuss the implications of our results for efforts to train neural networks much faster in the future.},
  archivePrefix = {arXiv},
  eprint = {1811.03600},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Measuring the Effects of Data Parallelism on Neural Network Training-Shallue et al-2018.pdf},
  journal = {arXiv:1811.03600 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sharmaCompositionalScalableObject,
  title = {Compositional {{Scalable Object SLAM}}},
  author = {Sharma, Akash and Dong, Wei and Kaess, Michael},
  pages = {7},
  abstract = {We present a fast, scalable, and accurate Simultaneous Localization and Mapping (SLAM) system that represents indoor scenes as a graph of objects. Leveraging the observation that artificial environments are structured and occupied by recognizable objects, we show that a compositional scalable object mapping formulation is amenable to a robust SLAM solution for drift-free large scale indoor reconstruction. To achieve this, we propose a novel semantically assisted data association strategy that obtains unambiguous persistent object landmarks, and a 2.5D compositional rendering method that enables reliable frame-to-model RGB-D tracking. Consequently, we deliver an optimized online implementation that can run at near frame rate with a single graphics card, and provide a comprehensive evaluation against state of the art baselines. An open source implementation will be provided at https://placeholder.},
  file = {/Users/sunjiaming/Zotero/storage/5MUSVD3Z/Sharma et al. - Compositional Scalable Object SLAM.pdf},
  language = {en}
}

@article{sharmaPixelsLeveragingGeometry2018,
  title = {Beyond {{Pixels}}: {{Leveraging Geometry}} and {{Shape Cues}} for {{Online Multi}}-{{Object Tracking}}},
  shorttitle = {Beyond {{Pixels}}},
  author = {Sharma, Sarthak and Ansari, Junaid Ahmed and Murthy, J. Krishna and Krishna, K. Madhava},
  year = {2018},
  month = feb,
  abstract = {This paper introduces geometry and novel object shape and pose costs for multi-object tracking in road scenes. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry\_ ObjectShape\_MOT/. Code and data to reproduce our experiments and results are now available at https://github. com/JunaidCS032/MOTBeyondPixels.},
  archivePrefix = {arXiv},
  eprint = {1802.09298},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Beyond Pixels-Sharma et al-2018.pdf},
  journal = {arXiv:1802.09298 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{sharpPointTriNetLearnedTriangulation2020,
  title = {{{PointTriNet}}: {{Learned Triangulation}} of {{3D Point Sets}}},
  shorttitle = {{{PointTriNet}}},
  author = {Sharp, Nicholas and Ovsjanikov, Maks},
  year = {2020},
  month = apr,
  abstract = {This work considers a new task in geometric deep learning: generating a triangulation among a set of points in 3D space. We present PointTriNet, a differentiable and scalable approach enabling point set triangulation as a layer in 3D learning pipelines. The method iteratively applies two neural networks to generate a triangulation: a classification network predicts whether a candidate triangle should appear in the triangulation, while a proposal network suggests additional candidates. Both networks are structured as PointNets over nearby points and triangles, using a novel triangle-relative input encoding. Since these learning problems operate on local geometric data, our method scales effectively to large input sets and unseen shape categories, and we can train the networks in an unsupervised manner from a collection of shapes represented as meshes or point clouds. We demonstrate the effectiveness of this approach for classical meshing tasks, robustness to outliers, and as a component in end-to-end learning systems.},
  archivePrefix = {arXiv},
  eprint = {2005.02138},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointTriNet-Sharp_Ovsjanikov-2020.pdf;/Users/sunjiaming/Zotero/storage/A5B22NEE/2005.html},
  journal = {arXiv:2005.02138 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shefferPnPNetHybridPerspectivenPoint2020,
  title = {{{PnP}}-{{Net}}: {{A}} Hybrid {{Perspective}}-n-{{Point Network}}},
  shorttitle = {{{PnP}}-{{Net}}},
  author = {Sheffer, Roy and Wiesel, Ami},
  year = {2020},
  month = mar,
  abstract = {We consider the robust Perspective-n-Point (PnP) problem using a hybrid approach that combines deep learning with model based algorithms. PnP is the problem of estimating the pose of a calibrated camera given a set of 3D points in the world and their corresponding 2D projections in the image. In its more challenging robust version, some of the correspondences may be mismatched and must be efficiently discarded. Classical solutions address PnP via iterative robust non-linear least squares method that exploit the problem's geometry but are either inaccurate or computationally intensive. In contrast, we propose to combine a deep learning initial phase followed by a model-based fine tuning phase. This hybrid approach, denoted by PnP-Net, succeeds in estimating the unknown pose parameters under correspondence errors and noise, with low and fixed computational complexity requirements. We demonstrate its advantages on both synthetic data and real world data.},
  archivePrefix = {arXiv},
  eprint = {2003.04626},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PnP-Net-Sheffer_Wiesel-2020.pdf;/Users/sunjiaming/Zotero/storage/EYXGSMQZ/2003.html},
  journal = {arXiv:2003.04626 [cs]},
  primaryClass = {cs}
}

@article{shenFrustumVoxNet3D2019,
  title = {Frustum {{VoxNet}} for {{3D}} Object Detection from {{RGB}}-{{D}} or {{Depth}} Images},
  author = {Shen, Xiaoke and Stamos, Ioannis},
  year = {2019},
  month = oct,
  abstract = {Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB, or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast, and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art , achieving a 2x speedup.},
  archivePrefix = {arXiv},
  eprint = {1910.05483},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Frustum VoxNet for 3D object detection from RGB-D or Depth images-Shen_Stamos-2019.pdf;/Users/sunjiaming/Zotero/storage/2II24KGK/1910.html},
  journal = {arXiv:1910.05483 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{shenoiJRMOTRealTime3D2020,
  title = {{{JRMOT}}: {{A Real}}-{{Time 3D Multi}}-{{Object Tracker}} and a {{New Large}}-{{Scale Dataset}}},
  shorttitle = {{{JRMOT}}},
  author = {Shenoi, Abhijeet and Patel, Mihir and Gwak, JunYoung and Goebel, Patrick and Sadeghian, Amir and Rezatofighi, Hamid and {Mart{\'i}n-Mart{\'i}n}, Roberto and Savarese, Silvio},
  year = {2020},
  month = mar,
  abstract = {An autonomous navigating agent needs to perceive and track the motion of objects and other agents in its surroundings to achieve robust and safe motion planning and execution. While autonomous navigation requires a multi-object tracking (MOT) system to provide 3D information, most research has been done in 2D MOT from RGB videos. In this work we present JRMOT, a novel 3D MOT system that integrates information from 2D RGB images and 3D point clouds into a real-time performing framework. Our system leverages advancements in neural-network based re-identification as well as 2D and 3D detection and descriptors. We incorporate this into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture to achieve online, real-time 3D MOT. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. The dataset contains over 60 minutes of data including 360 degree cylindrical RGB video and 3D pointclouds. The presented 3D MOT system demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as a competitive 3D tracking baseline for our dataset and benchmark.},
  archivePrefix = {arXiv},
  eprint = {2002.08397},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/JRMOT-Shenoi et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ELL796RD/2002.html},
  journal = {arXiv:2002.08397 [cs]},
  primaryClass = {cs}
}

@article{shenPowerNormRethinkingBatch2020,
  title = {{{PowerNorm}}: {{Rethinking Batch Normalization}} in {{Transformers}}},
  shorttitle = {{{PowerNorm}}},
  author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
  year = {2020},
  month = jun,
  abstract = {The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \textbackslash url\{https://github.com/sIncerass/powernorm\}.},
  archivePrefix = {arXiv},
  eprint = {2003.07845},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PowerNorm-Shen et al-2020.pdf;/Users/sunjiaming/Zotero/storage/5R6CUIKM/2003.html},
  journal = {arXiv:2003.07845 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{shenRANSACFlowGenericTwostage2020,
  title = {{{RANSAC}}-{{Flow}}: Generic Two-Stage Image Alignment},
  shorttitle = {{{RANSAC}}-{{Flow}}},
  author = {Shen, Xi and Darmon, Fran{\c c}ois and Efros, Alexei A. and Aubry, Mathieu},
  year = {2020},
  month = jul,
  abstract = {This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/\textasciitilde shenx/RANSAC-Flow/},
  archivePrefix = {arXiv},
  eprint = {2004.01526},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/DA3ZYH7S/Shen et al. - 2020 - RANSAC-Flow generic two-stage image alignment.pdf;/Users/sunjiaming/Zotero/storage/Q55P9QZM/2004.html},
  journal = {arXiv:2004.01526 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{shinMultilayerDepthEpipolar2019,
  title = {Multi-Layer {{Depth}} and {{Epipolar Feature Transformers}} for {{3D Scene Reconstruction}}},
  author = {Shin, Daeyun and Ren, Zhile and Sudderth, Erik B. and Fowlkes, Charless C.},
  year = {2019},
  month = feb,
  abstract = {We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel "Epipolar Feature Transformer" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.},
  archivePrefix = {arXiv},
  eprint = {1902.06729},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction-Shin et al-2019.pdf;/Users/sunjiaming/Zotero/storage/BUNVRHCU/1902.html},
  journal = {arXiv:1902.06729 [cs]},
  primaryClass = {cs}
}

@article{shinPixelsVoxelsViews2018,
  title = {Pixels, Voxels, and Views: {{A}} Study of Shape Representations for Single View {{3D}} Object Shape Prediction},
  shorttitle = {Pixels, Voxels, and Views},
  author = {Shin, Daeyun and Fowlkes, Charless C. and Hoiem, Derek},
  year = {2018},
  month = apr,
  abstract = {The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.},
  archivePrefix = {arXiv},
  eprint = {1804.06032},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pixels, voxels, and views-Shin et al-2018.pdf;/Users/sunjiaming/Zotero/storage/FKI7I56E/1804.html},
  journal = {arXiv:1804.06032 [cs]},
  primaryClass = {cs}
}

@article{shiPartANet3D2019,
  title = {Part-{{A}}\^2 {{Net}}: {{3D Part}}-{{Aware}} and {{Aggregation Neural Network}} for {{Object Detection}} from {{Point Cloud}}},
  shorttitle = {Part-{{A}}\^2 {{Net}}},
  author = {Shi, Shaoshuai and Wang, Zhe and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = jul,
  abstract = {In this paper, we propose the part-aware and aggregation neural network (Part-A\^2 net) for 3D object detection from point cloud. The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage learns to simultaneously predict coarse 3D proposals and accurate intra-object part locations with the free-of-charge supervisions derived from 3D ground-truth boxes. The predicted intra-object part locations within the same proposals are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the features of 3D proposals. Then the part-aggregation stage learns to re-score the box and refine the box location based on the pooled part locations. We present extensive experiments on the KITTI 3D object detection dataset, which demonstrate that both the predicted intra-object part locations and the proposed RoI-aware point cloud pooling scheme benefit 3D object detection and our Part-A\^2 net outperforms state-of-the-art methods by utilizing only point cloud data.},
  archivePrefix = {arXiv},
  eprint = {1907.03670},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Part-A^2 Net-Shi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/PYYNHRBX/1907.html},
  journal = {arXiv:1907.03670 [cs]},
  primaryClass = {cs}
}

@article{shiPointRCNN3DObject2018,
  title = {{{PointRCNN}}: {{3D Object Proposal Generation}} and {{Detection}} from {{Point Cloud}}},
  shorttitle = {{{PointRCNN}}},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  year = {2018},
  month = dec,
  abstract = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for refining proposals in the canonical coordinates to obtain the final detection results. Instead of generating proposals from RGB image or projecting point cloud to bird's view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box refinement and confidence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input.},
  archivePrefix = {arXiv},
  eprint = {1812.04244},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/PointRCNN-Shi et al-2018.pdf},
  journal = {arXiv:1812.04244 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{shiPVRCNNPointVoxelFeature2019,
  title = {{{PV}}-{{RCNN}}: {{Point}}-{{Voxel Feature Set Abstraction}} for {{3D Object Detection}}},
  shorttitle = {{{PV}}-{{RCNN}}},
  author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = dec,
  abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds.},
  archivePrefix = {arXiv},
  eprint = {1912.13192},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PV-RCNN-Shi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/FB3BMFL8/1912.html},
  journal = {arXiv:1912.13192 [cs, eess]},
  primaryClass = {cs, eess}
}

@inproceedings{shottonSceneCoordinateRegression2013,
  title = {Scene {{Coordinate Regression Forests}} for {{Camera Relocalization}} in {{RGB}}-{{D Images}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shotton, Jamie and Glocker, Ben and Zach, Christopher and Izadi, Shahram and Criminisi, Antonio and Fitzgibbon, Andrew},
  year = {2013},
  month = jun,
  pages = {2930--2937},
  publisher = {{IEEE}},
  address = {{Portland, OR, USA}},
  doi = {10.1109/CVPR.2013.377},
  abstract = {We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images-Shotton et al-2013.pdf},
  isbn = {978-0-7695-4989-7},
  language = {en}
}

@article{sibleySlidingWindowFilter,
  title = {A {{Sliding Window Filter}} for {{SLAM}}},
  author = {Sibley, Gabe},
  pages = {17},
  abstract = {This note describes a Sliding Window Filter that is an on-line constanttime approximation to the feature-based 6-degree-of-freedom full Batch Least Squares Simultaneous Localization and Mapping (SLAM) problem. We contend that for SLAM to be useful in large environments and over extensive run-times, its computational time complexity must be constant, and its memory requirements should be at most linear. Under this constraint, the ``best'' algorithm will be the one that comes closest to matching the all-time maximum-likelihood estimate of the full SLAM problem, while also maintaining consistency. We start by formulating SLAM as a Batch Least Squares state estimation problem, and then show how to modify the Batch estimator into an approximate Sliding Window Batch/Recursive framework that achieves constant time complexity and linear space complexity. We argue that viewing SLAM from the Sliding Window Least Squares perspective is very useful for understanding the structure of the problem. This perspective is general, capable of subsuming a number of common estimation techniques such as Bundle Adjustment and Extended Kalman Filter SLAM. By tuning the sliding window, the algorithm can scale from exhaustive Batch solutions to fast incremental solutions; if the window encompasses all time, the solution is algebraically equivalent to full SLAM; if only one time step is maintained, the solution is algebraically equivalent to the Extended Kalman Filter SLAM solution. The Sliding Window Filter enables other interesting properties, like continuous sub-mapping, lazy data association, undelayed or delayed landmark initialization, and incremental robust estimation. We test the algorithm in simulations using stereo vision exterioceptive sensors and inertial measurement proprioceptive sensors. Initial experiments show that the SWF approaches the performance of the optimal batch estimator, even for small windows on the order of 5-10 frames.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Sliding Window Filter for SLAM-Sibley-.pdf},
  language = {en}
}

@article{simonComplexerYOLORealTime3D2019,
  title = {Complexer-{{YOLO}}: {{Real}}-{{Time 3D Object Detection}} and {{Tracking}} on {{Semantic Point Clouds}}},
  shorttitle = {Complexer-{{YOLO}}},
  author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and S{\"a}mann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
  year = {2019},
  month = apr,
  abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20\textbackslash\% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
  archivePrefix = {arXiv},
  eprint = {1904.07537},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Complexer-YOLO-Simon et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Y4KVEY96/1904.html},
  journal = {arXiv:1904.07537 [cs]},
  primaryClass = {cs}
}

@article{simonelliDisentanglingMonocular3D2019,
  title = {Disentangling {{Monocular 3D Object Detection}}},
  author = {Simonelli, Andrea and Bul{\`o}, Samuel Rota Rota and Porzi, Lorenzo and {L{\'o}pez-Antequera}, Manuel and Kontschieder, Peter},
  year = {2019},
  month = may,
  abstract = {In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-the-art results on object category car by large margins.},
  archivePrefix = {arXiv},
  eprint = {1905.12365},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Disentangling Monocular 3D Object Detection-Simonelli et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2UPN384U/1905.html},
  journal = {arXiv:1905.12365 [cs]},
  primaryClass = {cs}
}

@article{singhRapidPoseLabel2020,
  title = {Rapid {{Pose Label Generation}} through {{Sparse Representation}} of {{Unknown Objects}}},
  author = {Singh, Rohan Pratap and Benallegue, Mehdi and Yoshiyasu, Yusuke and Kanehiro, Fumio},
  year = {2020},
  month = nov,
  abstract = {Deep Convolutional Neural Networks (CNNs) have been successfully deployed on robots for 6-DoF object pose estimation through visual perception. However, obtaining labeled data on a scale required for the supervised training of CNNs is a difficult task - exacerbated if the object is novel and a 3D model is unavailable. To this end, this work presents an approach for rapidly generating real-world, pose-annotated RGB-D data for unknown objects. Our method not only circumvents the need for a prior 3D object model (textured or otherwise) but also bypasses complicated setups of fiducial markers, turntables, and sensors. With the help of a human user, we first source minimalistic labelings of an ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then, by solving an optimization problem, we combine these labels under a world frame to recover a sparse, keypoint-based representation of the object. The sparse representation leads to the development of a dense model and the pose labels for each image frame in the set of scenes. We show that the sparse model can also be efficiently used for scaling to a large number of new scenes. We demonstrate the practicality of the generated labeled dataset by training a pipeline for 6-DoF object pose estimation and a pixel-wise segmentation network.},
  archivePrefix = {arXiv},
  eprint = {2011.03790},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/NBMSN9FL/Singh et al. - 2020 - Rapid Pose Label Generation through Sparse Represe.pdf},
  journal = {arXiv:2011.03790 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{sinhaDELTASDepthEstimation2020,
  title = {{{DELTAS}}: {{Depth Estimation}} by {{Learning Triangulation And}} Densification of {{Sparse}} Points},
  shorttitle = {{{DELTAS}}},
  author = {Sinha, Ayan and Murez, Zak and Bartolozzi, James and Badrinarayanan, Vijay and Rabinovich, Andrew},
  year = {2020},
  month = aug,
  abstract = {Multi-view stereo (MVS) is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems. However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An end-to-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines. Code is available at https://github.com/magicleap/DELTAS},
  archivePrefix = {arXiv},
  eprint = {2003.08933},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DELTAS-Sinha et al-2020.pdf;/Users/sunjiaming/Zotero/storage/LQFB7SSA/2003.html},
  journal = {arXiv:2003.08933 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{sinhaDepthEstimationLearning2020,
  title = {Depth {{Estimation}} by {{Learning Triangulation}} and {{Densification}} of {{Sparse Points}} for {{Multi}}-View {{Stereo}}},
  author = {Sinha, Ayan and Murez, Zak and Bartolozzi, James and Badrinarayanan, Vijay and Rabinovich, Andrew},
  year = {2020},
  month = mar,
  abstract = {Multi-view stereo (MVS) is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems. However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An end-to-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate that state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines.},
  archivePrefix = {arXiv},
  eprint = {2003.08933},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Depth Estimation by Learning Triangulation and Densification of Sparse Points-Sinha et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ZG7KEUW5/2003.html},
  journal = {arXiv:2003.08933 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{sitzmannDeepVoxelsLearningPersistent2018,
  title = {{{DeepVoxels}}: {{Learning Persistent 3D Feature Embeddings}}},
  shorttitle = {{{DeepVoxels}}},
  author = {Sitzmann, Vincent and Thies, Justus and Heide, Felix and Nie{\ss}ner, Matthias and Wetzstein, Gordon and Zollh{\"o}fer, Michael},
  year = {2018},
  month = dec,
  abstract = {In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D object without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach thus combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging objects.},
  archivePrefix = {arXiv},
  eprint = {1812.01024},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/DeepVoxels-Sitzmann et al-2018.pdf;/Users/sunjiaming/Zotero/storage/5T4MFND7/1812.html},
  journal = {arXiv:1812.01024 [cs]},
  keywords = {feature learning},
  primaryClass = {cs}
}

@article{sitzmannImplicitNeuralRepresentations2020,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  year = {2020},
  month = jun,
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
  archivePrefix = {arXiv},
  eprint = {2006.09661},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Implicit Neural Representations with Periodic Activation Functions-Sitzmann et al-2020.pdf;/Users/sunjiaming/Zotero/storage/8Z5D2ZBD/2006.html},
  journal = {arXiv:2006.09661 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  primaryClass = {cs, eess}
}

@article{sitzmannMetaSDFMetalearningSigned2020,
  title = {{{MetaSDF}}: {{Meta}}-Learning {{Signed Distance Functions}}},
  shorttitle = {{{MetaSDF}}},
  author = {Sitzmann, Vincent and Chan, Eric R. and Tucker, Richard and Snavely, Noah and Wetzstein, Gordon},
  year = {2020},
  month = jun,
  abstract = {Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.},
  archivePrefix = {arXiv},
  eprint = {2006.09662},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MetaSDF-Sitzmann et al-2020.pdf;/Users/sunjiaming/Zotero/storage/AUCFFYB6/2006.html},
  journal = {arXiv:2006.09662 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{sitzmannSceneRepresentationNetworks2019,
  title = {Scene {{Representation Networks}}: {{Continuous 3D}}-{{Structure}}-{{Aware Neural Scene Representations}}},
  shorttitle = {Scene {{Representation Networks}}},
  author = {Sitzmann, Vincent and Zollh{\"o}fer, Michael and Wetzstein, Gordon},
  year = {2019},
  month = jun,
  abstract = {The advent of deep learning has given rise to neural scene representations - learned mathematical models of a 3D environment. However, many of these representations do not explicitly reason about geometry and thus do not account for the underlying 3D structure of the scene. In contrast, geometric deep learning has explored 3D-structure-aware representations of scene geometry, but requires explicit 3D supervision. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D observations, without access to depth or geometry. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.},
  archivePrefix = {arXiv},
  eprint = {1906.01618},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scene Representation Networks-Sitzmann et al-2019.pdf;/Users/sunjiaming/Zotero/storage/HAGELTPJ/1906.html},
  journal = {arXiv:1906.01618 [cs]},
  primaryClass = {cs}
}

@inproceedings{slavchevaKillingFusionNonrigid3D2017,
  title = {{{KillingFusion}}: {{Non}}-Rigid {{3D Reconstruction}} without {{Correspondences}}},
  shorttitle = {{{KillingFusion}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Slavcheva, Miroslava and Baust, Maximilian and Cremers, Daniel and Ilic, Slobodan},
  year = {2017},
  month = jul,
  pages = {5474--5483},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.581},
  abstract = {We introduce a geometry-driven approach for real-time 3D reconstruction of deforming surfaces from a single RGB-D stream without any templates or shape priors. To this end, we tackle the problem of non-rigid registration by level set evolution without explicit correspondence search. Given a pair of signed distance fields (SDFs) representing the shapes of interest, we estimate a dense deformation field that aligns them. It is defined as a displacement vector field of the same resolution as the SDFs and is determined iteratively via variational minimization. To ensure it generates plausible shapes, we propose a novel regularizer that imposes local rigidity by requiring the deformation to be a smooth and approximately Killing vector field, i.e. generating nearly isometric motions. Moreover, we enforce that the level set property of unity gradient magnitude is preserved over iterations. As a result, KillingFusion reliably reconstructs objects that are undergoing topological changes and fast inter-frame motion. In addition to incrementally building a model from scratch, our system can also deform complete surfaces. We demonstrate these capabilities on several public datasets and introduce our own sequences that permit both qualitative and quantitative comparison to related approaches.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/KillingFusion-Slavcheva et al-2017.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{slinkoTrainingDeepSLAM2019,
  title = {Training {{Deep SLAM}} on {{Single Frames}}},
  author = {Slinko, Igor and Vorontsova, Anna and Zhukov, Dmitry and Barinova, Olga and Konushin, Anton},
  year = {2019},
  month = dec,
  abstract = {Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.},
  archivePrefix = {arXiv},
  eprint = {1912.05405},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Training Deep SLAM on Single Frames-Slinko et al-2019.pdf;/Users/sunjiaming/Zotero/storage/93M9L5K7/1912.html},
  journal = {arXiv:1912.05405 [cs]},
  primaryClass = {cs}
}

@article{smirnovDeepParametricShape2019,
  title = {Deep {{Parametric Shape Predictions}} Using {{Distance Fields}}},
  author = {Smirnov, Dmitriy and Fisher, Matthew and Kim, Vladimir G. and Zhang, Richard and Solomon, Justin},
  year = {2019},
  month = apr,
  abstract = {Many tasks in graphics and vision demand machinery for converting shapes into representations with sparse sets of parameters; these representations facilitate rendering, editing, and storage. When the source data is noisy or ambiguous, however, artists and engineers often manually construct such representations, a tedious and potentially time-consuming process. While advances in deep learning have been successfully applied to noisy geometric data, the task of generating parametric shapes has so far been difficult for these methods. Hence, we propose a new framework for predicting parametric shape primitives using deep learning. We use distance fields to transition between shape parameters like control points and input data on a raster grid. We demonstrate efficacy on 2D and 3D tasks, including font vectorization and surface abstraction.},
  archivePrefix = {arXiv},
  eprint = {1904.08921},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Parametric Shape Predictions using Distance Fields-Smirnov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4RIC68W7/1904.html},
  journal = {arXiv:1904.08921 [cs]},
  primaryClass = {cs}
}

@article{smithSuperConvergenceVeryFast2017,
  title = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle = {Super-{{Convergence}}},
  author = {Smith, Leslie N. and Topin, Nicholay},
  year = {2017},
  month = aug,
  abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  archivePrefix = {arXiv},
  eprint = {1708.07120},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Super-Convergence-Smith_Topin-2017.pdf;/Users/sunjiaming/Zotero/storage/CIPRA8IX/1708.html},
  journal = {arXiv:1708.07120 [cs, stat]},
  keywords = {optimization},
  primaryClass = {cs, stat}
}

@article{solomonDiscoveryIntrinsicPrimitives2011,
  title = {Discovery of {{Intrinsic Primitives}} on {{Triangle Meshes}}},
  author = {Solomon, Justin and Ben-Chen, Mirela and Butscher, Adrian and Guibas, Leonidas},
  year = {2011},
  volume = {30},
  pages = {365--374},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2011.01867.x},
  abstract = {The discovery of meaningful parts of a shape is required for many geometry processing applications, such as parameterization, shape correspondence, and animation. It is natural to consider primitives such as spheres, cylinders and cones as the building blocks of shapes, and thus to discover parts by fitting such primitives to a given surface. This approach, however, will break down if primitive parts have undergone almost-isometric deformations, as is the case, for example, for articulated human models. We suggest that parts can be discovered instead by finding intrinsic primitives, which we define as parts that posses an approximate intrinsic symmetry. We employ the recently-developed method of computing discrete approximate Killing vector fields (AKVFs) to discover intrinsic primitives by investigating the relationship between the AKVFs of a composite object and the AKVFs of its parts. We show how to leverage this relationship with a standard clustering method to extract k intrinsic primitives and remaining asymmetric parts of a shape for a given k. We demonstrate the value of this approach for identifying the prominent symmetry generators of the parts of a given shape. Additionally, we show how our method can be modified slightly to segment an entire surface without marking asymmetric connecting regions and compare this approach to state-of-the-art methods using the Princeton Segmentation Benchmark.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2011.01867.x},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Discovery of Intrinsic Primitives on Triangle Meshes-Solomon et al-2011.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {2}
}

@book{solomonNumericalAlgorithmsMethods2015,
  title = {Numerical {{Algorithms}}: {{Methods}} for {{Computer Vision}}, {{Machine Learning}}, and {{Graphics}}},
  shorttitle = {Numerical {{Algorithms}}},
  author = {Solomon, Justin},
  year = {2015},
  month = jun,
  publisher = {{A K Peters/CRC Press}},
  doi = {10.1201/b18657},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Numerical Algorithms-Solomon-2015.pdf},
  isbn = {978-1-4822-5189-0},
  keywords = {math},
  language = {en}
}

@inproceedings{sommerJointRepresentationPrimitive2018,
  title = {Joint {{Representation}} of {{Primitive}} and {{Non}}-Primitive {{Objects}} for {{3D Vision}}},
  booktitle = {2018 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Sommer, Christiane and Cremers, Daniel},
  year = {2018},
  month = sep,
  pages = {160--169},
  publisher = {{IEEE}},
  address = {{Verona}},
  doi = {10.1109/3DV.2018.00028},
  abstract = {The use of structural information in 3D scanning is becoming more and more popular. However, most approaches exploit this structural information either in the form of geometric primitives (mostly planes) or known rigid bodies, but not both. We overcome this limitation and propose an object representation that combines primitive and nonprimitive objects using one unified formulation that is based on signed distance fields. Object pose manifolds are introduced to represent the rigid movement of primitives and non-primitives in a natural way. We show that different components of volumetric scanning, such as global trajectory optimization or geometry completion and denoising, benefit from our formulation.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Representation of Primitive and Non-primitive Objects for 3D Vision-Sommer_Cremers-2018.pdf},
  isbn = {978-1-5386-8425-2},
  language = {en}
}

@article{songAdaStereoSimpleEfficient2020,
  title = {{{AdaStereo}}: {{A Simple}} and {{Efficient Approach}} for {{Adaptive Stereo Matching}}},
  shorttitle = {{{AdaStereo}}},
  author = {Song, Xiao and Yang, Guorun and Zhu, Xinge and Zhou, Hui and Wang, Zhe and Shi, Jianping},
  year = {2020},
  month = apr,
  abstract = {In this paper, we attempt to solve the domain adaptation problem for deep stereo matching networks. Instead of resorting to black-box structures or layers to find implicit connections across domains, we focus on investigating adaptation gaps for stereo matching. By visual inspections and extensive experiments, we conclude that low-level aligning is crucial for adaptive stereo matching, since main gaps across domains lie in the inconsistent input color and cost volume distributions. Correspondingly, we design a bottom-up domain adaptation method, in which two particular approaches are proposed, i.e. color transfer and cost regularization, that can be easily integrated into existing stereo matching models. The color transfer enables transferring a large amount of synthetic data to the same color spaces with target domains during training. The cost regularization can further constrain both the lower-layer features and cost volumes to domain-invariant distributions. Although our proposed strategies are simple and have no parameters to learn, they do improve the generalization ability of existing disparity networks by a large margin. We conduct experiments across multiple datasets, including Scene Flow, KITTI, Middlebury, ETH3D and DrivingStereo. Without whistles and bells, our synthetic-data pretrained models achieve state-of-the-art cross-domain performance compared to previous domain-invariant methods, even outperform state-of-the-art disparity networks fine-tuned with target domain ground-truths on multiple stereo matching benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2004.04627},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/AdaStereo-Song et al-2020.pdf;/Users/sunjiaming/Zotero/storage/FVU7BCGV/2004.html},
  journal = {arXiv:2004.04627 [cs]},
  primaryClass = {cs}
}

@article{songApolloCar3DLarge3D2018,
  title = {{{ApolloCar3D}}: {{A Large 3D Car Instance Understanding Benchmark}} for {{Autonomous Driving}}},
  shorttitle = {{{ApolloCar3D}}},
  author = {Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},
  year = {2018},
  month = nov,
  abstract = {Autonomous driving has attracted remarkable attention from both industry and academia. An important task is to estimate 3D properties (e.g. translation, rotation and shape) of a moving or parked vehicle on the road. This task, while critical, is still under-researched in the computer vision community \textendash{} partially owing to the lack of large scale and fully-annotated 3D car database suitable for autonomous driving research. In this paper, we contribute the first largescale database suitable for 3D car instance understanding \textendash{} ApolloCar3D. The dataset contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20\texttimes{} larger than PASCAL3D+ [65] and KITTI [21], the current state-of-the-art. To enable efficient labelling in 3D, we build a pipeline by considering 2D-3D keypoint correspondences for a single instance and 3D relationship among multiple instances. Equipped with such dataset, we build various baseline algorithms with the state-of-the-art deep convolutional neural networks. Specifically, we first segment each car with a pre-trained Mask R-CNN [22], and then regress towards its 3D pose and shape based on a deformable 3D car model with or without using semantic keypoints. We show that using keypoints significantly improves fitting performance. Finally, we develop a new 3D metric jointly considering 3D pose and 3D shape, allowing for comprehensive evaluation and ablation study. By comparing with human performance we suggest several future directions for further improvements.},
  archivePrefix = {arXiv},
  eprint = {1811.12222},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/ApolloCar3D-Song et al-2018.pdf},
  journal = {arXiv:1811.12222 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{songJointSFMDetection2015,
  title = {Joint {{SFM}} and Detection Cues for Monocular {{3D}} Localization in Road Scenes},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Song, Shiyu and Chandraker, Manmohan},
  year = {2015},
  month = jun,
  pages = {3734--3742},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298997},
  abstract = {We present a system for fast and highly accurate 3D localization of objects like cars in autonomous driving applications, using a single camera. Our localization framework jointly uses information from complementary modalities such as structure from motion (SFM) and object detection to achieve high localization accuracy in both near and far fields. This is in contrast to prior works that rely purely on detector outputs, or motion segmentation based on sparse feature tracks. Rather than completely commit to tracklets generated by a 2D tracker, we make novel use of raw detection scores to allow our 3D bounding boxes to adapt to better quality 3D cues. To extract SFM cues, we demonstrate the advantages of dense tracking over sparse mechanisms in autonomous driving scenarios. In contrast to complex scene understanding, our formulation for 3D localization is efficient and can be regarded as an extension of sparse bundle adjustment to incorporate object detection cues. Experiments on the KITTI dataset show the efficacy of our cues, as well as the accuracy and robustness of our 3D object localization relative to ground truth and prior works.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint SFM and detection cues for monocular 3D localization in road scenes-Song_Chandraker-2015.pdf},
  isbn = {978-1-4673-6964-0},
  language = {en}
}

@incollection{songSlidingShapes3D2014,
  title = {Sliding {{Shapes}} for {{3D Object Detection}} in {{Depth Images}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Song, Shuran and Xiao, Jianxiong},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8694},
  pages = {634--651},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10599-4_41},
  abstract = {The depth information of RGB-D sensors has greatly simplified some common challenges in computer vision and enabled breakthroughs for several tasks. In this paper, we propose to use depth maps for object detection and design a 3D detector to overcome the major difficulties for recognition, namely the variations of texture, illumination, shape, viewpoint, clutter, occlusion, selfocclusion and sensor noises. We take a collection of 3D CAD models and render each CAD model from hundreds of viewpoints to obtain synthetic depth maps. For each depth rendering, we extract features from the 3D point cloud and train an Exemplar-SVM classifier. During testing and hard-negative mining, we slide a 3D detection window in 3D space. Experiment results show that our 3D detector significantly outperforms the state-of-the-art algorithms for both RGB and RGBD images, and achieves about \texttimes 1.7 improvement on average precision compared to DPM and R-CNN. All source code and data are available online.},
  file = {/Users/sunjiaming/Zotero/storage/25HJDLDY/Song and Xiao - 2014 - Sliding Shapes for 3D Object Detection in Depth Im.pdf},
  isbn = {978-3-319-10598-7 978-3-319-10599-4},
  language = {en}
}

@article{sorkineAsRigidAsPossibleSurfaceModeling,
  title = {As-{{Rigid}}-{{As}}-{{Possible Surface Modeling}}},
  author = {Sorkine, Olga and Alexa, Marc},
  pages = {8},
  abstract = {Modeling tasks, such as surface deformation and editing, can be analyzed by observing the local behavior of the surface. We argue that defining a modeling operation by asking for rigidity of the local transformations is useful in various settings. Such formulation leads to a non-linear, yet conceptually simple energy formulation, which is to be minimized by the deformed surface under particular modeling constraints. We devise a simple iterative mesh editing scheme based on this principle, that leads to detail-preserving and intuitive deformations. Our algorithm is effective and notably easy to implement, making it attractive for practical modeling applications.},
  file = {/Users/sunjiaming/Zotero/storage/4B44EGHH/Sorkine and Alexa - As-Rigid-As-Possible Surface Modeling.pdf},
  language = {en}
}

@article{spectorGoogleHybridApproach2012,
  title = {Google's Hybrid Approach to Research},
  author = {Spector, Alfred and Norvig, Peter and Petrov, Slav},
  year = {2012},
  month = jul,
  volume = {55},
  pages = {34},
  issn = {00010782},
  doi = {10.1145/2209249.2209262},
  file = {/Users/sunjiaming/Zotero/storage/Z5TTDMVG/Spector et al. - 2012 - Google's hybrid approach to research.pdf},
  journal = {Communications of the ACM},
  keywords = {discussion},
  language = {en},
  number = {7}
}

@book{sraOptimizationMachineLearning2012,
  title = {Optimization for Machine Learning},
  editor = {Sra, Suvrit and Nowozin, Sebastian and Wright, Stephen J.},
  year = {2012},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: ocn701493361},
  file = {/Users/sunjiaming/Zotero/storage/JCUYGAB8/Sra et al. - 2012 - Optimization for machine learning.pdf},
  isbn = {978-0-262-01646-9},
  language = {en},
  lccn = {Q325.5 .O65 2012},
  series = {Neural Information Processing Series}
}

@article{sridharMultiviewAggregationLearning2019,
  title = {Multiview {{Aggregation}} for {{Learning Category}}-{{Specific Shape Reconstruction}}},
  author = {Sridhar, Srinath and Rempe, Davis and Valentin, Julien and Bouaziz, Sofien and Guibas, Leonidas J.},
  year = {2019},
  month = jul,
  abstract = {We investigate the problem of learning category-specific 3D surface shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations, or assume a fixed number of views. We present a method that can estimate dense 3D shape, and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance, we propose a representation that encodes the dense shape of the visible object surface parts as well as the surface behind line of sight and occluded by the visible surface. When multiple input views are available, the shape representation is designed to be aggregated into a single 3D shape using an inexpesive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense reconstructions of objects, and is able to produce better results as more views are added.},
  archivePrefix = {arXiv},
  eprint = {1907.01085},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Sridhar et al_2019_Multiview Aggregation for Learning Category-Specific Shape Reconstruction.pdf;/Users/sunjiaming/Zotero/storage/DZM3PYH2/1907.html},
  journal = {arXiv:1907.01085 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{srinivasanLighthousePredictingLighting2020,
  title = {Lighthouse: {{Predicting Lighting Volumes}} for {{Spatially}}-{{Coherent Illumination}}},
  shorttitle = {Lighthouse},
  author = {Srinivasan, Pratul P. and Mildenhall, Ben and Tancik, Matthew and Barron, Jonathan T. and Tucker, Richard and Snavely, Noah},
  year = {2020},
  month = mar,
  abstract = {We present a deep learning solution for estimating the incident illumination at any 3D location within a scene from an input narrow-baseline stereo image pair. Previous approaches for predicting global illumination from images either predict just a single illumination for the entire scene, or separately estimate the illumination at each 3D location without enforcing that the predictions are consistent with the same 3D scene. Instead, we propose a deep learning model that estimates a 3D volumetric RGBA model of a scene, including content outside the observed field of view, and then uses standard volume rendering to estimate the incident illumination at any 3D location within that volume. Our model is trained without any ground truth 3D data and only requires a held-out perspective view near the input stereo pair and a spherical panorama taken within each scene as supervision, as opposed to prior methods for spatially-varying lighting estimation, which require ground truth scene geometry for training. We demonstrate that our method can predict consistent spatially-varying lighting that is convincing enough to plausibly relight and insert highly specular virtual objects into real images.},
  archivePrefix = {arXiv},
  eprint = {2003.08367},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Lighthouse-Srinivasan et al-2020.pdf;/Users/sunjiaming/Zotero/storage/79ZB26TS/2003.html},
  journal = {arXiv:2003.08367 [cs]},
  primaryClass = {cs}
}

@article{srivastavaLearning2D3D2019,
  title = {Learning {{2D}} to {{3D Lifting}} for {{Object Detection}} in {{3D}} for {{Autonomous Vehicles}}},
  author = {Srivastava, Siddharth and Jurie, Frederic and Sharma, Gaurav},
  year = {2019},
  month = mar,
  abstract = {We address the problem of 3D object detection from 2D monocular images in autonomous driving scenarios. We propose to lift the 2D images to 3D representations using learned neural networks and leverage existing networks working directly on 3D to perform 3D object detection and localization. We show that, with carefully designed training mechanism and automatically selected minimally noisy data, such a method is not only feasible, but gives higher results than many methods working on actual 3D inputs acquired from physical sensors. On the challenging KITTI benchmark, we show that our 2D to 3D lifted method outperforms many recent competitive 3D networks while significantly outperforming previous state of the art for 3D detection from monocular images. We also show that a late fusion of the output of the network trained on generated 3D images, with that trained on real 3D images, improves performance. We find the results very interesting and argue that such a method could serve as a highly reliable backup in case of malfunction of expensive 3D sensors, if not potentially making them redundant, at least in the case of low human injury risk autonomous navigation scenarios like warehouse automation.},
  archivePrefix = {arXiv},
  eprint = {1904.08494},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles-Srivastava et al-2019.pdf;/Users/sunjiaming/Zotero/storage/WZUH9IFC/1904.html},
  journal = {arXiv:1904.08494 [cs]},
  primaryClass = {cs}
}

@article{strasdatVisualSLAMWhy2012,
  title = {Visual {{SLAM}}: {{Why}} Filter?},
  shorttitle = {Visual {{SLAM}}},
  author = {Strasdat, Hauke and Montiel, J.M.M. and Davison, Andrew J.},
  year = {2012},
  month = feb,
  volume = {30},
  pages = {65--77},
  issn = {02628856},
  doi = {10.1016/j.imavis.2012.02.009},
  abstract = {While the most accurate solution to off-line Structure from Motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform batch optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM \textemdash{} also called visual SLAM (Simultaneous Localisation and Mapping) \textemdash{} have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process.},
  file = {/Users/sunjiaming/Zotero/storage/UK7LW7ZR/Strasdat et al. - 2012 - Visual SLAM Why filter.pdf},
  journal = {Image and Vision Computing},
  keywords = {slam},
  language = {en},
  number = {2}
}

@article{straubReplicaDatasetDigital2019,
  title = {The {{Replica Dataset}}: {{A Digital Replica}} of {{Indoor Spaces}}},
  shorttitle = {The {{Replica Dataset}}},
  author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and {Mur-Artal}, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and De Nardi, Renzo and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
  year = {2019},
  month = jun,
  abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
  archivePrefix = {arXiv},
  eprint = {1906.05797},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Replica Dataset-Straub et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TCIZYYW3/1906.html},
  journal = {arXiv:1906.05797 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{streckeEMFusionDynamicObjectLevel2019,
  title = {{{EM}}-{{Fusion}}: {{Dynamic Object}}-{{Level SLAM}} with {{Probabilistic Data Association}}},
  shorttitle = {{{EM}}-{{Fusion}}},
  author = {Strecke, Michael and St{\"u}ckler, J{\"o}rg},
  year = {2019},
  month = apr,
  abstract = {The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy.},
  archivePrefix = {arXiv},
  eprint = {1904.11781},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/EM-Fusion-Strecke_Stückler-2019.pdf;/Users/sunjiaming/Zotero/storage/P8VY6AEZ/1904.html},
  journal = {arXiv:1904.11781 [cs]},
  primaryClass = {cs}
}

@article{streckeWhereDoesIt2020,
  title = {Where {{Does It End}}? -- {{Reasoning About Hidden Surfaces}} by {{Object Intersection Constraints}}},
  shorttitle = {Where {{Does It End}}?},
  author = {Strecke, Michael and Stueckler, Joerg},
  year = {2020},
  month = apr,
  abstract = {Dynamic scene understanding is an essential capability in robotics and VR/AR. In this paper we propose Co-Section, an optimization-based approach to 3D dynamic scene reconstruction, which infers hidden shape information from intersection constraints. An object-level dynamic SLAM frontend detects, segments, tracks and maps dynamic objects in the scene. Our optimization backend completes the shapes using hull and intersection constraints between the objects. In experiments, we demonstrate our approach on real and synthetic dynamic scene datasets. We also assess the shape completion performance of our method quantitatively. To the best of our knowledge, our approach is the first method to incorporate such physical plausibility constraints on object intersections for shape completion of dynamic objects in an energy minimization framework.},
  archivePrefix = {arXiv},
  eprint = {2004.04630},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Where Does It End-Strecke_Stueckler-2020.pdf;/Users/sunjiaming/Zotero/storage/V24F9YFE/2004.html},
  journal = {arXiv:2004.04630 [cs]},
  primaryClass = {cs}
}

@article{strunkElementsStyle,
  title = {The {{Elements}} of {{Style}}},
  author = {Strunk, White},
  pages = {184},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/The Elements of Style-Strunk-.pdf}
}

@article{stutzLearning3DShape2018,
  title = {Learning {{3D Shape Completion}} under {{Weak Supervision}}},
  author = {Stutz, David and Geiger, Andreas},
  year = {2018},
  month = oct,
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-018-1126-y},
  abstract = {We address the problem of 3D shape completion from sparse and noisy point clouds, a fundamental problem in computer vision and robotics. Recent approaches are either data-driven or learning-based: Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations; Learning-based approaches, in contrast, avoid the expensive optimization step by learning to directly predict complete shapes from incomplete observations in a fully-supervised setting. However, full supervision is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. On synthetic benchmarks based on ShapeNet and ModelNet as well as on real robotics data from KITTI and Kinect, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with recent fully supervised baselines and outperforms data-driven approaches, while requiring less supervision and being significantly faster.},
  archivePrefix = {arXiv},
  eprint = {1805.07290},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning 3D Shape Completion under Weak Supervision-Stutz_Geiger-2018.pdf;/Users/sunjiaming/Zotero/storage/WXQLXIVM/1805.html},
  journal = {International Journal of Computer Vision}
}

@article{sucarNeuralObjectDescriptors2020,
  title = {Neural {{Object Descriptors}} for {{Multi}}-{{View Shape Reconstruction}}},
  author = {Sucar, Edgar and Wada, Kentaro and Davison, Andrew},
  year = {2020},
  month = apr,
  abstract = {The choice of scene representation is crucial in both the shape inference algorithms it requires and the smart applications it enables. We present efficient and optimisable multi-class learned object descriptors together with a novel probabilistic and differential rendering engine, for principled full object shape inference from one or more RGB-D images. Our framework allows for accurate and robust 3D object reconstruction which enables multiple applications including robot grasping and placing, augmented reality, and the first object-level SLAM system capable of optimising object poses and shapes jointly with camera trajectory.},
  archivePrefix = {arXiv},
  eprint = {2004.04485},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Object Descriptors for Multi-View Shape Reconstruction-Sucar et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ADC32H5P/2004.html},
  journal = {arXiv:2004.04485 [cs]},
  primaryClass = {cs}
}

@article{sunBetterGlobalLoss2020,
  title = {Towards a {{Better Global Loss Landscape}} of {{GANs}}},
  author = {Sun, Ruoyu and Fang, Tiantian and Schwing, Alex},
  year = {2020},
  month = nov,
  abstract = {Understanding of GAN training is still very limited. One major challenge is its non-convex-non-concave min-max objective, which may lead to sub-optimal local minima. In this work, we perform a global landscape analysis of the empirical loss of GANs. We prove that a class of separable-GAN, including the original JS-GAN, has exponentially many bad basins which are perceived as mode-collapse. We also study the relativistic pairing GAN (RpGAN) loss which couples the generated samples and the true samples. We prove that RpGAN has no bad basins. Experiments on synthetic data show that the predicted bad basin can indeed appear in training. We also perform experiments to support our theory that RpGAN has a better landscape than separable-GAN. For instance, we empirically show that RpGAN performs better than separable-GAN with relatively narrow neural nets. The code is available at https://github.com/AilsaF/RS-GAN.},
  archivePrefix = {arXiv},
  eprint = {2011.04926},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Towards a Better Global Loss Landscape of GANs-Sun et al-2020.pdf},
  journal = {arXiv:2011.04926 [cs, math]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  language = {en},
  primaryClass = {cs, math}
}

@article{sunDeepHighResolutionRepresentation2019,
  title = {Deep {{High}}-{{Resolution Representation Learning}} for {{Human Pose Estimation}}},
  author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  year = {2019},
  month = feb,
  abstract = {In this paper, we are interested in the human pose estimation problem with a focus on learning reliable highresolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process.},
  archivePrefix = {arXiv},
  eprint = {1902.09212},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep High-Resolution Representation Learning for Human Pose Estimation-Sun et al-2019.pdf},
  journal = {arXiv:1902.09212 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{sunExploringTemporalRelationships,
  title = {Exploring {{Temporal Relationships}} in {{Robotics Perception}}},
  author = {Sun, Jiaming},
  pages = {22},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Exploring Temporal Relationships in Robotics Perception-Sun-.pdf},
  language = {en}
}

@article{sunFishNetVersatileBackbone,
  title = {{{FishNet}}: {{A Versatile Backbone}} for {{Image}}, {{Region}}, and {{Pixel Level Prediction}}},
  author = {Sun, Shuyang and Pang, Jiangmiao and Shi, Jianping and Yi, Shuai and Ouyang, Wanli},
  pages = {11},
  abstract = {The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixellevel, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixellevel or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot directly propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet-1k, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/FishNet-Sun et al-.pdf},
  language = {en}
}

@inproceedings{sunLocalLayeringJoint2014,
  title = {Local {{Layering}} for {{Joint Motion Estimation}} and {{Occlusion Detection}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Deqing and Liu, Ce and Pfister, Hanspeter},
  year = {2014},
  month = jun,
  pages = {1098--1105},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.144},
  abstract = {Most motion estimation algorithms (optical flow, layered models) cannot handle large amount of occlusion in textureless regions, as motion is often initialized with no occlusion assumption despite that occlusion may be included in the final objective. To handle such situations, we propose a local layering model where motion and occlusion relationships are inferred jointly. In particular, the uncertainties of occlusion relationships are retained so that motion is inferred by considering all the possibilities of local occlusion relationships. In addition, the local layering model handles articulated objects with self-occlusion. We demonstrate that the local layering model can handle motion and occlusion well for both challenging synthetic and real sequences.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Local Layering for Joint Motion Estimation and Occlusion Detection-Sun et al-2014.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@article{sunPix3DDatasetMethods2018,
  title = {{{Pix3D}}: {{Dataset}} and {{Methods}} for {{Single}}-{{Image 3D Shape Modeling}}},
  shorttitle = {{{Pix3D}}},
  author = {Sun, Xingyuan and Wu, Jiajun and Zhang, Xiuming and Zhang, Zhoutong and Zhang, Chengkai and Xue, Tianfan and Tenenbaum, Joshua B. and Freeman, William T.},
  year = {2018},
  month = apr,
  abstract = {We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.},
  archivePrefix = {arXiv},
  eprint = {1804.04610},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pix3D-Sun et al-2018.pdf;/Users/sunjiaming/Zotero/storage/84G43KNY/1804.html},
  journal = {arXiv:1804.04610 [cs]},
  primaryClass = {cs}
}

@article{sunPWCNetCNNsOptical2017,
  title = {{{PWC}}-{{Net}}: {{CNNs}} for {{Optical Flow Using Pyramid}}, {{Warping}}, and {{Cost Volume}}},
  shorttitle = {{{PWC}}-{{Net}}},
  author = {Sun, Deqing and Yang, Xiaodong and Liu, Ming-Yu and Kautz, Jan},
  year = {2017},
  month = sep,
  abstract = {We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWCNet is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024\texttimes 436) images. Our models are available on https://github.com/NVlabs/PWC-Net.},
  archivePrefix = {arXiv},
  eprint = {1709.02371},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/2N8ZQKK8/Sun et al. - 2017 - PWC-Net CNNs for Optical Flow Using Pyramid, Warp.pdf},
  journal = {arXiv:1709.02371 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{sunScalabilityPerceptionAutonomous2019,
  title = {Scalability in {{Perception}} for {{Autonomous Driving}}: {{An Open Dataset Benchmark}}},
  shorttitle = {Scalability in {{Perception}} for {{Autonomous Driving}}},
  author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and YuZhang and Shlens, Jon and Chen, Zhifeng and Anguelov, Dragomir},
  year = {2019},
  month = dec,
  abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
  archivePrefix = {arXiv},
  eprint = {1912.04838},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Scalability in Perception for Autonomous Driving-Sun et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QRWC7PS5/1912.html},
  journal = {arXiv:1912.04838 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{suSPLATNetSparseLattice2018,
  title = {{{SPLATNet}}: {{Sparse Lattice Networks}} for {{Point Cloud Processing}}},
  shorttitle = {{{SPLATNet}}},
  author = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming-Hsuan and Kautz, Jan},
  year = {2018},
  month = feb,
  abstract = {We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Na\textasciidieresis\i vely applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.},
  archivePrefix = {arXiv},
  eprint = {1802.08275},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SPLATNet-Su et al-2018.pdf},
  journal = {arXiv:1802.08275 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{suwajanakornDiscoveryLatent3D2018,
  title = {Discovery of {{Latent 3D Keypoints}} via {{End}}-to-End {{Geometric Reasoning}}},
  author = {Suwajanakorn, Supasorn and Snavely, Noah and Tompson, Jonathan and Norouzi, Mohammad},
  year = {2018},
  month = nov,
  abstract = {This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at http://keypointnet.github.io/.},
  archivePrefix = {arXiv},
  eprint = {1807.03146},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning-Suwajanakorn et al-2018.pdf;/Users/sunjiaming/Zotero/storage/QUB87YQS/1807.html},
  journal = {arXiv:1807.03146 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{tairaInLocIndoorVisual2018,
  title = {{{InLoc}}: {{Indoor Visual Localization}} with {{Dense Matching}} and {{View Synthesis}}},
  shorttitle = {{{InLoc}}},
  author = {Taira, Hajime and Okutomi, Masatoshi and Sattler, Torsten and Cimpoi, Mircea and Pollefeys, Marc and Sivic, Josef and Pajdla, Tomas and Torii, Akihiko},
  year = {2018},
  month = apr,
  abstract = {We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor environments. The method proceeds along three steps: (i) efficient retrieval of candidate poses that ensures scalability to large-scale environments, (ii) pose estimation using dense matching rather than local features to deal with textureless indoor scenes, and (iii) pose verification by virtual view synthesis to cope with significant changes in viewpoint, scene layout, and occluders. Second, we collect a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data.},
  archivePrefix = {arXiv},
  eprint = {1803.10368},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/InLoc-Taira et al-2018.pdf;/Users/sunjiaming/Zotero/storage/XQ8N7YMF/1803.html},
  journal = {arXiv:1803.10368 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tallecUnbiasingTruncatedBackpropagation2017,
  title = {Unbiasing {{Truncated Backpropagation Through Time}}},
  author = {Tallec, Corentin and Ollivier, Yann},
  year = {2017},
  month = may,
  abstract = {Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT.},
  archivePrefix = {arXiv},
  eprint = {1705.08209},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/Q623DXZG/Tallec and Ollivier - 2017 - Unbiasing Truncated Backpropagation Through Time.pdf;/Users/sunjiaming/Zotero/storage/AH7LMDPB/1705.html},
  journal = {arXiv:1705.08209 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  year = {2020},
  month = jun,
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archivePrefix = {arXiv},
  eprint = {2006.10739},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional-Tancik et al-2020.pdf;/Users/sunjiaming/Zotero/storage/55WX95R8/2006.html},
  journal = {arXiv:2006.10739 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{tanEfficientDetScalableEfficient2019,
  ids = {tanEfficientDetScalableEfficient2020},
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study various neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations, we have developed a new family of object detectors, called EfficientDet, which consistently achieve an order-of-magnitude better efficiency than prior art across a wide spectrum of resource constraints. In particular, without bells and whistles, our EfficientDet-D7 achieves stateof-the-art 51.0 mAP on COCO dataset with 52M parameters and 326B FLOPS1 , being 4x smaller and using 9.3x fewer FLOPS yet still more accurate (+0.3\% mAP) than the best previous detector.},
  archivePrefix = {arXiv},
  eprint = {1911.09070},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/EfficientDet-Tan et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/EfficientDet-Tan et al-2020.pdf;/Users/sunjiaming/Zotero/storage/5GZB95IC/1911.html;/Users/sunjiaming/Zotero/storage/DKF4EN6N/1911.html},
  journal = {arXiv:1911.09070 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{tangBANetDenseBundle2018,
  title = {{{BA}}-{{Net}}: {{Dense Bundle Adjustment Network}}},
  shorttitle = {{{BA}}-{{Net}}},
  author = {Tang, Chengzhou and Tan, Ping},
  year = {2018},
  month = jun,
  abstract = {This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.},
  archivePrefix = {arXiv},
  eprint = {1806.04807},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/BA-Net-Tang_Tan-2018.pdf;/Users/sunjiaming/Zotero/storage/MXCI6R53/1806.html},
  journal = {arXiv:1806.04807 [cs]},
  keywords = {neufu_paper,optimization},
  primaryClass = {cs}
}

@article{tangLSMLearningSubspace2020,
  title = {{{LSM}}: {{Learning Subspace Minimization}} for {{Low}}-Level {{Vision}}},
  shorttitle = {{{LSM}}},
  author = {Tang, Chengzhou and Yuan, Lu and Tan, Ping},
  year = {2020},
  month = apr,
  abstract = {We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.},
  archivePrefix = {arXiv},
  eprint = {2004.09197},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LSM-Tang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/UHUU5UIC/2004.html},
  journal = {arXiv:2004.09197 [cs]},
  primaryClass = {cs}
}

@article{tangNeuralOutlierRejection2019,
  title = {Neural {{Outlier Rejection}} for {{Self}}-{{Supervised Keypoint Learning}}},
  author = {Tang, Jiexiong and Kim, Hanme and Guizilini, Vitor and Pillai, Sudeep and Ambrus, Rares},
  year = {2019},
  month = dec,
  abstract = {Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks. However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1912.10615},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Outlier Rejection for Self-Supervised Keypoint Learning-Tang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/SGY9H942/1912.html},
  journal = {arXiv:1912.10615 [cs]},
  primaryClass = {cs}
}

@article{tangSearchingEfficient3D2020,
  title = {Searching {{Efficient 3D Architectures}} with {{Sparse Point}}-{{Voxel Convolution}}},
  author = {Tang, Haotian and Liu, Zhijian and Zhao, Shengyu and Lin, Yujun and Lin, Ji and Wang, Hanrui and Han, Song},
  year = {2020},
  month = aug,
  abstract = {Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3\%, ranking 1st on the competitive SemanticKITTI leaderboard. It also achieves 8x computation reduction and 3x measured speedup over MinkowskiNet with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI.},
  archivePrefix = {arXiv},
  eprint = {2007.16100},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution-Tang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GFJQK8B2/2007.html},
  journal = {arXiv:2007.16100 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper},
  primaryClass = {cs}
}

@article{tangSelfSupervised3DKeypoint2019,
  title = {Self-{{Supervised 3D Keypoint Learning}} for {{Ego}}-Motion {{Estimation}}},
  author = {Tang, Jiexiong and Ambrus, Rares and Guizilini, Vitor and Pillai, Sudeep and Kim, Hanme and Gaidon, Adrien},
  year = {2019},
  month = dec,
  abstract = {Generating reliable illumination and viewpoint invariant keypoints is critical for feature-based SLAM and SfM. State-of-the-art learning-based methods often rely on generating training samples by employing homography adaptation to create 2D synthetic views. While such approaches trivially solve data association between views, they cannot effectively learn from real illumination and non-planar 3D scenes. In this work, we propose a fully self-supervised approach towards learning depth-aware keypoints \textbackslash textit\{purely\} from unlabeled videos by incorporating a differentiable pose estimation module that jointly optimizes the keypoints and their depths in a Structure-from-Motion setting. We introduce 3D Multi-View Adaptation, a technique that exploits the temporal context in videos to self-supervise keypoint detection and matching in an end-to-end differentiable manner. Finally, we show how a fully self-supervised keypoint detection and description network can be trivially incorporated as a front-end into a state-of-the-art visual odometry framework that is robust and accurate.},
  archivePrefix = {arXiv},
  eprint = {1912.03426},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self-Supervised 3D Keypoint Learning for Ego-motion Estimation-Tang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/NVKZAK69/1912.html},
  journal = {arXiv:1912.03426 [cs]},
  primaryClass = {cs}
}

@article{tangTransferableSemisupervised3D2019,
  title = {Transferable {{Semi}}-Supervised {{3D Object Detection}} from {{RGB}}-{{D Data}}},
  author = {Tang, Yew Siang and Lee, Gim Hee},
  year = {2019},
  month = apr,
  abstract = {We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.},
  archivePrefix = {arXiv},
  eprint = {1904.10300},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Transferable Semi-supervised 3D Object Detection from RGB-D Data-Tang_Lee-2019.pdf;/Users/sunjiaming/Zotero/storage/DXDJFSEH/1904.html},
  journal = {arXiv:1904.10300 [cs]},
  primaryClass = {cs}
}

@article{tankovichHITNetHierarchicalIterative2020,
  title = {{{HITNet}}: {{Hierarchical Iterative Tile Refinement Network}} for {{Real}}-Time {{Stereo Matching}}},
  shorttitle = {{{HITNet}}},
  author = {Tankovich, Vladimir and H{\"a}ne, Christian and Fanello, Sean and Zhang, Yinda and Izadi, Shahram and Bouaziz, Sofien},
  year = {2020},
  month = jul,
  abstract = {This paper presents HITNet, a novel neural network architecture for real-time stereo matching. Contrary to many recent neural network approaches that operate on a full cost volume and rely on 3D convolutions, our approach does not explicitly build a volume and instead relies on a fast multi-resolution initialization step, differentiable 2D geometric propagation and warping mechanisms to infer disparity hypotheses. To achieve a high level of accuracy, our network not only geometrically reasons about disparities but also infers slanted plane hypotheses allowing to more accurately perform geometric warping and upsampling operations. Our architecture is inherently multi-resolution allowing the propagation of information at different levels. Multiple experiments prove the effectiveness of the proposed approach at a fraction of the computation required by recent state-of-the-art methods. At time of writing, HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two view stereo and ranks 1st on the popular KITTI 2012 and 2015 benchmarks among the published methods faster than 100ms.},
  archivePrefix = {arXiv},
  eprint = {2007.12140},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/HITNet-Tankovich et al-2020.pdf;/Users/sunjiaming/Zotero/storage/QLSTVJA3/2007.html},
  journal = {arXiv:2007.12140 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tatarchenkoWhatSingleview3D2019,
  title = {What {{Do Single}}-View {{3D Reconstruction Networks Learn}}?},
  author = {Tatarchenko, Maxim and Richter, Stephan R. and Ranftl, Ren{\'e} and Li, Zhuwen and Koltun, Vladlen and Brox, Thomas},
  year = {2019},
  month = may,
  abstract = {Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.},
  archivePrefix = {arXiv},
  eprint = {1905.03678},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What Do Single-view 3D Reconstruction Networks Learn-Tatarchenko et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Z2SKAF9X/1905.html},
  journal = {arXiv:1905.03678 [cs]},
  primaryClass = {cs}
}

@inproceedings{tatenoCNNSLAMRealTimeDense2017,
  title = {{{CNN}}-{{SLAM}}: {{Real}}-{{Time Dense Monocular SLAM}} with {{Learned Depth Prediction}}},
  shorttitle = {{{CNN}}-{{SLAM}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir},
  year = {2017},
  month = jul,
  pages = {6565--6574},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.695},
  abstract = {Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CNN-SLAM-Tateno et al-2017.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@inproceedings{tatenoWhen5DNot2016,
  title = {When 2.{{5D}} Is Not Enough: {{Simultaneous}} Reconstruction, Segmentation and Recognition on Dense {{SLAM}}},
  shorttitle = {When 2.{{5D}} Is Not Enough},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tateno, Keisuke and Tombari, Federico and Navab, Nassir},
  year = {2016},
  month = may,
  pages = {2295--2302},
  publisher = {{IEEE}},
  address = {{Stockholm, Sweden}},
  doi = {10.1109/ICRA.2016.7487378},
  abstract = {While the main trend of 3D object recognition has been to infer object detection from single views of the scene \textemdash{} i.e., 2.5D data \textemdash{} this work explores the direction on performing object recognition on 3D data that is reconstructed from multiple viewpoints, under the conjecture that such data can improve the robustness of an object recognition system. To achieve this goal, we propose a framework which is able (i) to carry out incremental real-time segmentation of a 3D scene while being reconstructed via Simultaneous Localization And Mapping (SLAM), and (ii) to simultaneously and incrementally carry out 3D object recognition and pose estimation on the reconstructed and segmented 3D representations. Experimental results demonstrate the advantages of our approach with respect to traditional single view-based object recognition and pose estimation approaches, as well as its usefulness in robotic perception and augmented reality applications.},
  file = {/Users/sunjiaming/Zotero/storage/JKEI6YFD/Tateno et al. - 2016 - When 2.5D is not enough Simultaneous reconstructi.pdf},
  isbn = {978-1-4673-8026-3},
  language = {en}
}

@article{tayEfficientTransformersSurvey2020,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2020},
  month = sep,
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archivePrefix = {arXiv},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/8R2PQ9B9/Tay et al. - 2020 - Efficient Transformers A Survey.pdf;/Users/sunjiaming/Zotero/storage/B879KQ3G/2009.html},
  journal = {arXiv:2009.06732 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{tayLongRangeArena2020,
  title = {Long {{Range Arena}}: {{A Benchmark}} for {{Efficient Transformers}}},
  shorttitle = {Long {{Range Arena}}},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  year = {2020},
  month = nov,
  abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
  archivePrefix = {arXiv},
  eprint = {2011.04006},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Long Range Arena-Tay et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BI2VEBUV/2011.html},
  journal = {arXiv:2011.04006 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{teedDeepV2DVideoDepth2018,
  ids = {teedDeepV2DVideoDepth2018a,teedDeepV2DVideoDepth2018b},
  title = {{{DeepV2D}}: {{Video}} to {{Depth}} with {{Differentiable Structure}} from {{Motion}}},
  shorttitle = {{{DeepV2D}}},
  author = {Teed, Zachary and Deng, Jia},
  year = {2018},
  month = dec,
  abstract = {We propose DeepV2D, an end-to-end differentiable deep learning architecture for predicting depth from a video sequence. We incorporate elements of classical Structure from Motion into an end-to-end trainable pipeline by designing a set of differentiable geometric modules. Our full system alternates between predicting depth and refining camera pose. We estimate depth by building a cost volume over learned features and apply a multi-scale 3D convolutional network for stereo matching. The predicted depth is then sent to the motion module which performs iterative pose updates by mapping optical flow to a camera motion update. We evaluate our proposed system on NYU, KITTI, and SUN3D datasets and show improved results over monocular baselines and deep and classical stereo reconstruction.},
  archivePrefix = {arXiv},
  eprint = {1812.04605},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepV2D-Teed_Deng-22.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepV2D-Teed_Deng-3.pdf;/Users/sunjiaming/Zotero/storage/G44F4P3Z/1812.html;/Users/sunjiaming/Zotero/storage/WI9KE8P2/1812.html},
  journal = {arXiv:1812.04605 [cs]},
  keywords = {neufu_paper},
  language = {en},
  primaryClass = {cs}
}

@article{teedRAFTRecurrentAllPairs2020,
  title = {{{RAFT}}: {{Recurrent All}}-{{Pairs Field Transforms}} for {{Optical Flow}}},
  shorttitle = {{{RAFT}}},
  author = {Teed, Zachary and Deng, Jia},
  year = {2020},
  month = mar,
  abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance, with strong cross-dataset generalization and high efficiency in inference time, training speed, and parameter count. Code is available \textbackslash url\{https://github.com/princeton-vl/RAFT\}.},
  archivePrefix = {arXiv},
  eprint = {2003.12039},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RAFT-Teed_Deng-2020.pdf;/Users/sunjiaming/Zotero/storage/7HYQB8XN/Teed and Deng - 2020 - RAFT Recurrent All-Pairs Field Transforms for Opt.pdf;/Users/sunjiaming/Zotero/storage/M6S6QCI3/2003.html},
  journal = {arXiv:2003.12039 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{teedRAFTRecurrentAllPairs2020a,
  title = {{{RAFT}}: {{Recurrent All}}-{{Pairs Field Transforms}} for {{Optical Flow}}},
  shorttitle = {{{RAFT}}},
  author = {Teed, Zachary and Deng, Jia},
  year = {2020},
  month = jul,
  abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\textbackslash\%, a 16\textbackslash\% error reduction from the best published result (6.10\textbackslash\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\textbackslash\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at \textbackslash url\{https://github.com/princeton-vl/RAFT\}.},
  archivePrefix = {arXiv},
  eprint = {2003.12039},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RAFT-Teed_Deng-22.pdf;/Users/sunjiaming/Zotero/storage/KGTASVHZ/2003.html},
  journal = {arXiv:2003.12039 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tewariStateArtNeural2020,
  title = {State of the {{Art}} on {{Neural Rendering}}},
  author = {Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and {Martin-Brualla}, Ricardo and Simon, Tomas and Saragih, Jason and Nie{\ss}ner, Matthias and Pandey, Rohit and Fanello, Sean and Wetzstein, Gordon and Zhu, Jun-Yan and Theobalt, Christian and Agrawala, Maneesh and Shechtman, Eli and Goldman, Dan B. and Zollh{\"o}fer, Michael},
  year = {2020},
  month = apr,
  abstract = {Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.},
  archivePrefix = {arXiv},
  eprint = {2004.03805},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/State of the Art on Neural Rendering-Tewari et al-2020.pdf;/Users/sunjiaming/Zotero/storage/TV2T22DL/2004.html},
  journal = {arXiv:2004.03805 [cs]},
  primaryClass = {cs}
}

@article{thiesDeferredNeuralRendering2019,
  title = {Deferred {{Neural Rendering}}: {{Image Synthesis}} Using {{Neural Textures}}},
  shorttitle = {Deferred {{Neural Rendering}}},
  author = {Thies, Justus and Zollh{\"o}fer, Michael and Nie{\ss}ner, Matthias},
  year = {2019},
  month = apr,
  abstract = {The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.},
  archivePrefix = {arXiv},
  eprint = {1904.12356},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deferred Neural Rendering-Thies et al-2019.pdf;/Users/sunjiaming/Zotero/storage/ZDB5BPDS/1904.html},
  journal = {arXiv:1904.12356 [cs]},
  primaryClass = {cs}
}

@article{thiesIGNORImageguidedNeural2020,
  title = {{{IGNOR}}: {{Image}}-Guided {{Neural Object Rendering}}},
  shorttitle = {{{IGNOR}}},
  author = {Thies, Justus and Zollh{\"o}fer, Michael and Theobalt, Christian and Stamminger, Marc and Nie{\ss}ner, Matthias},
  year = {2020},
  month = jan,
  abstract = {We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours \textbackslash\& sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on ``remembering'' object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.},
  archivePrefix = {arXiv},
  eprint = {1811.10720},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/IGNOR-Thies et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RTE6WCPJ/1811.html},
  journal = {arXiv:1811.10720 [cs]},
  primaryClass = {cs}
}

@article{thomasKPConvFlexibleDeformable2019,
  title = {{{KPConv}}: {{Flexible}} and {{Deformable Convolution}} for {{Point Clouds}}},
  shorttitle = {{{KPConv}}},
  author = {Thomas, Hugues and Qi, Charles R. and Deschaud, Jean-Emmanuel and Marcotegui, Beatriz and Goulette, Fran{\c c}ois and Guibas, Leonidas J.},
  year = {2019},
  month = apr,
  abstract = {We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.},
  archivePrefix = {arXiv},
  eprint = {1904.08889},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/KPConv-Thomas et al-2019.pdf;/Users/sunjiaming/Zotero/storage/PILEEVIC/1904.html},
  journal = {arXiv:1904.08889 [cs]},
  primaryClass = {cs}
}

@article{tianD2DKeypointExtraction2020,
  title = {{{D2D}}: {{Keypoint Extraction}} with {{Describe}} to {{Detect Approach}}},
  shorttitle = {{{D2D}}},
  author = {Tian, Yurun and Balntas, Vassileios and Ng, Tony and {Barroso-Laguna}, Axel and Demiris, Yiannis and Mikolajczyk, Krystian},
  year = {2020},
  month = may,
  abstract = {In this paper, we present a novel approach that exploits the information within the descriptor space to propose keypoint locations. Detect then describe, or detect and describe jointly are two typical strategies for extracting local descriptors. In contrast, we propose an approach that inverts this process by first describing and then detecting the keypoint locations. \% Describe-to-Detect (D2D) leverages successful descriptor models without the need for any additional training. Our method selects keypoints as salient locations with high information content which is defined by the descriptors rather than some independent operators. We perform experiments on multiple benchmarks including image matching, camera localisation, and 3D reconstruction. The results indicate that our method improves the matching performance of various descriptors and that it generalises across methods and tasks.},
  archivePrefix = {arXiv},
  eprint = {2005.13605},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/D2D-Tian et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9CHET29S/2005.html},
  journal = {arXiv:2005.13605 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{tianFCOSFullyConvolutional2019,
  title = {{{FCOS}}: {{Fully Convolutional One}}-{{Stage Object Detection}}},
  shorttitle = {{{FCOS}}},
  author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  year = {2019},
  month = apr,
  abstract = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor-box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training and significantly reduces the training memory footprint. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), our detector FCOS outperforms previous anchor-based one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks.},
  archivePrefix = {arXiv},
  eprint = {1904.01355},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FCOS-Tian et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FCOS-Tian et al-22.pdf;/Users/sunjiaming/Zotero/storage/2ULIADDL/1904.html;/Users/sunjiaming/Zotero/storage/ZV8NSXXP/1904.html},
  journal = {arXiv:1904.01355 [cs]},
  keywords = {2d detection},
  primaryClass = {cs}
}

@inproceedings{tianL2NetDeepLearning2017,
  title = {L2-{{Net}}: {{Deep Learning}} of {{Discriminative Patch Descriptor}} in {{Euclidean Space}}},
  shorttitle = {L2-{{Net}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tian, Yurun and Fan, Bin and Wu, Fuchao},
  year = {2017},
  month = jul,
  pages = {6128--6136},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.649},
  abstract = {The research focus of designing local patch descriptors has gradually shifted from handcrafted ones (e.g., SIFT) to learned ones. In this paper, we propose to learn high performance descriptor in Euclidean space via the Convolutional Neural Network (CNN). Our method is distinctive in four aspects: (i) We propose a progressive sampling strategy which enables the network to access billions of training samples in a few epochs. (ii) Derived from the basic concept of local patch matching problem, we emphasize the relative distance between descriptors. (iii) Extra supervision is imposed on the intermediate feature maps. (iv) Compactness of the descriptor is taken into account. The proposed network is named as L2-Net since the output descriptor can be matched in Euclidean space by L2 distance. L2-Net achieves state-of-the-art performance on the Brown datasets [16], Oxford dataset [18] and the newly proposed Hpatches dataset [11]. The good generalization ability shown by experiments indicates that L2-Net can serve as a direct substitution of the existing handcrafted descriptors. The pre-trained L2-Net is publicly available1.},
  file = {/Users/sunjiaming/Zotero/storage/P2R38VQ9/Tian et al. - 2017 - L2-Net Deep Learning of Discriminative Patch Desc.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@inproceedings{tianL2NetDeepLearning2017a,
  title = {L2-{{Net}}: {{Deep Learning}} of {{Discriminative Patch Descriptor}} in {{Euclidean Space}}},
  shorttitle = {L2-{{Net}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tian, Yurun and Fan, Bin and Wu, Fuchao},
  year = {2017},
  month = jul,
  pages = {6128--6136},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.649},
  abstract = {The research focus of designing local patch descriptors has gradually shifted from handcrafted ones (e.g., SIFT) to learned ones. In this paper, we propose to learn high performance descriptor in Euclidean space via the Convolutional Neural Network (CNN). Our method is distinctive in four aspects: (i) We propose a progressive sampling strategy which enables the network to access billions of training samples in a few epochs. (ii) Derived from the basic concept of local patch matching problem, we emphasize the relative distance between descriptors. (iii) Extra supervision is imposed on the intermediate feature maps. (iv) Compactness of the descriptor is taken into account. The proposed network is named as L2-Net since the output descriptor can be matched in Euclidean space by L2 distance. L2-Net achieves state-of-the-art performance on the Brown datasets [16], Oxford dataset [18] and the newly proposed Hpatches dataset [11]. The good generalization ability shown by experiments indicates that L2-Net can serve as a direct substitution of the existing handcrafted descriptors. The pre-trained L2-Net is publicly available1.},
  file = {/Users/sunjiaming/Zotero/storage/PB7SGE6F/Tian et al. - 2017 - L2-Net Deep Learning of Discriminative Patch Desc.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{tianLuckMattersUnderstanding2019,
  title = {Luck {{Matters}}: {{Understanding Training Dynamics}} of {{Deep ReLU Networks}}},
  shorttitle = {Luck {{Matters}}},
  author = {Tian, Yuandong and Jiang, Tina and Gong, Qucheng and Morcos, Ari},
  year = {2019},
  month = may,
  abstract = {We analyze the dynamics of training deep ReLU networks and their implications on generalization capability. Using a teacher-student setting, we discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks. With this relationship and the assumption of small overlapping teacher node activations, we prove that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fan-out weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. We verify our assumption by showing that the majority of BatchNorm biases of pre-trained VGG11/16 models are negative. Experiments on (1) random deep teacher networks with Gaussian inputs, (2) teacher network pre-trained on CIFAR-10 and (3) extensive ablation studies validate our multiple theoretical predictions.},
  archivePrefix = {arXiv},
  eprint = {1905.13405},
  eprinttype = {arxiv},
  journal = {arXiv:1905.13405 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{tianShapePriorDeformation2020,
  title = {Shape {{Prior Deformation}} for {{Categorical 6D Object Pose}} and {{Size Estimation}}},
  author = {Tian, Meng and Ang Jr, Marcelo H. and Lee, Gim Hee},
  year = {2020},
  month = jul,
  abstract = {We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.},
  archivePrefix = {arXiv},
  eprint = {2007.08454},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation-Tian et al-2020.pdf;/Users/sunjiaming/Zotero/storage/USP78VZH/2007.html},
  journal = {arXiv:2007.08454 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tianWhatMakesGood2020,
  title = {What Makes for Good Views for Contrastive Learning},
  author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  year = {2020},
  month = may,
  abstract = {Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we also achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification (\$73\textbackslash\%\$ top-1 linear readoff with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast},
  archivePrefix = {arXiv},
  eprint = {2005.10243},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What makes for good views for contrastive learning-Tian et al-2020.pdf;/Users/sunjiaming/Zotero/storage/KRN47FMY/2005.html},
  journal = {arXiv:2005.10243 [cs]},
  primaryClass = {cs}
}

@inproceedings{tjadenRealTimeMonocularPose2017,
  title = {Real-{{Time Monocular Pose Estimation}} of {{3D Objects Using Temporally Consistent Local Color Histograms}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tjaden, Henning and Schwanecke, Ulrich and Schomer, Elmar},
  year = {2017},
  month = oct,
  pages = {124--132},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.23},
  abstract = {We present a novel approach to 6DOF pose estimation and segmentation of rigid 3D objects using a single monocular RGB camera based on temporally consistent, local color histograms. We show that this approach outperforms previous methods in cases of cluttered backgrounds, heterogenous objects, and occlusions. The proposed histograms can be used as statistical object descriptors within a template matching strategy for pose recovery after temporary tracking loss e.g. caused by massive occlusion or if the object leaves the camera's field of view. The descriptors can be trained online within a couple of seconds moving a handheld object in front of a camera. During the training stage, our approach is already capable to recover from accidental tracking loss. We demonstrate the performance of our method in comparison to the state of the art in different challenging experiments including a popular public data set.},
  file = {/Users/sunjiaming/Zotero/storage/YVXGGDSJ/Tjaden et al. - 2017 - Real-Time Monocular Pose Estimation of 3D Objects .pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@article{tjadenRegionbasedGaussNewtonApproach2019,
  ids = {tjadenRegionbasedGaussNewtonApproach2019a},
  title = {A {{Region}}-Based {{Gauss}}-{{Newton Approach}} to {{Real}}-{{Time Monocular Multiple Object Tracking}}},
  author = {Tjaden, Henning and Schwanecke, Ulrich and Sch{\"o}mer, Elmar and Cremers, Daniel},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {1797--1812},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2018.2884990},
  abstract = {We propose an algorithm for real-time 6DOF pose tracking of rigid 3D objects using a monocular RGB camera. The key idea is to derive a region-based cost function using temporally consistent local color histograms. While such region-based cost functions are commonly optimized using first-order gradient descent techniques, we systematically derive a Gauss-Newton optimization scheme which gives rise to drastically faster convergence and highly accurate and robust tracking performance. We furthermore propose a novel complex dataset dedicated for the task of monocular object pose tracking and make it publicly available to the community. To our knowledge, it is the first to address the common and important scenario in which both the camera as well as the objects are moving simultaneously in cluttered scenes. In numerous experiments - including our own proposed dataset - we demonstrate that the proposed Gauss-Newton approach outperforms existing approaches, in particular in the presence of cluttered backgrounds, heterogeneous objects and partial occlusions.},
  archivePrefix = {arXiv},
  eprint = {1807.02087},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Region-based Gauss-Newton Approach to Real-Time Monocular Multiple Object-Tjaden et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Region-based Gauss-Newton Approach to Real-Time Monocular Multiple Object-Tjaden et al-22.pdf;/Users/sunjiaming/Zotero/storage/3VVDXMHV/1807.html;/Users/sunjiaming/Zotero/storage/T6UDTA6X/1807.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}

@article{tobinDomainRandomizationTransferring2017,
  title = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2017},
  month = mar,
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  archivePrefix = {arXiv},
  eprint = {1703.06907},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Domain Randomization for Transferring Deep Neural Networks from Simulation to-Tobin et al-2017.pdf;/Users/sunjiaming/Zotero/storage/J5IZNSPS/1703.html},
  journal = {arXiv:1703.06907 [cs]},
  primaryClass = {cs}
}

@article{tonioniLearningAdaptStereo2019,
  title = {Learning to {{Adapt}} for {{Stereo}}},
  author = {Tonioni, Alessio and Rahnama, Oscar and Joy, Thomas and Di Stefano, Luigi and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
  year = {2019},
  month = apr,
  abstract = {Real world applications of stereo depth estimation require models that are robust to dynamic variations in the environment. Even though deep learning based stereo methods are successful, they often fail to generalize to unseen variations in the environment, making them less suitable for practical applications such as autonomous driving. In this work, we introduce a ``learning-to-adapt'' framework that enables deep stereo methods to continuously adapt to new target domains in an unsupervised manner. Specifically, our approach incorporates the adaptation procedure into the learning objective to obtain a base set of parameters that are better suited for unsupervised online adaptation. To further improve the quality of the adaptation, we learn a confidence measure that effectively masks the errors introduced during the unsupervised adaptation. We evaluate our method on synthetic and real-world stereo datasets and our experiments evidence that learning-to-adapt is, indeed beneficial for online adaptation on vastly different domains.},
  archivePrefix = {arXiv},
  eprint = {1904.02957},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Adapt for Stereo-Tonioni et al-2019.pdf},
  journal = {arXiv:1904.02957 [cs]},
  keywords = {disparity},
  language = {en},
  primaryClass = {cs}
}

@article{tonioniRealtimeSelfadaptiveDeep2018,
  title = {Real-Time Self-Adaptive Deep Stereo},
  author = {Tonioni, Alessio and Tosi, Fabio and Poggi, Matteo and Mattoccia, Stefano and Di Stefano, Luigi},
  year = {2018},
  month = oct,
  abstract = {Deep convolutional neural networks trained end-to-end are the state-of-the-art methods to regress dense disparity maps from stereo pairs. These models, however, suffer from a notable decrease in accuracy when exposed to scenarios significantly different from the training set, e.g., real vs synthetic images, etc.). We argue that it is extremely unlikely to gather enough samples to achieve effective training/tuning in any target domain, thus making this setup impractical for many applications. Instead, we propose to perform unsupervised and continuous online adaptation of a deep stereo network, which allows for preserving its accuracy in any environment. However, this strategy is extremely computationally demanding and thus prevents real-time inference. We address this issue introducing a new lightweight, yet effective, deep stereo architecture, Modularly ADaptive Network (MADNet) and developing a Modular ADaptation (MAD) algorithm, which independently trains sub-portions of the network. By deploying MADNet together with MAD we introduce the first real-time self-adaptive deep stereo system enabling competitive performance on heterogeneous datasets.},
  archivePrefix = {arXiv},
  eprint = {1810.05424},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-time self-adaptive deep stereo-Tonioni et al-2018.pdf;/Users/sunjiaming/Zotero/storage/FUZV7YDD/1810.html},
  journal = {arXiv:1810.05424 [cs]},
  primaryClass = {cs}
}

@article{tretschkPatchNetsPatchBasedGeneralizable2020,
  title = {{{PatchNets}}: {{Patch}}-{{Based Generalizable Deep Implicit 3D Shape Representations}}},
  shorttitle = {{{PatchNets}}},
  author = {Tretschk, Edgar and Tewari, Ayush and Golyanik, Vladislav and Zollh{\"o}fer, Michael and Stoll, Carsten and Theobalt, Christian},
  year = {2020},
  month = aug,
  abstract = {Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.},
  archivePrefix = {arXiv},
  eprint = {2008.01639},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PatchNets-Tretschk et al-2020.pdf;/Users/sunjiaming/Zotero/storage/BGXCZR89/2008.html},
  journal = {arXiv:2008.01639 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{trevithickGRFLearningGeneral2020,
  title = {{{GRF}}: {{Learning}} a {{General Radiance Field}} for {{3D Scene Representation}} and {{Rendering}}},
  shorttitle = {{{GRF}}},
  author = {Trevithick, Alex and Yang, Bo},
  year = {2020},
  month = oct,
  abstract = {We present a simple yet powerful implicit neural function that can represent and render arbitrarily complex 3D scenes in a single network only from 2D observations. The function models 3D scenes as a general radiance field, which takes a set of posed 2D images as input, constructs an internal representation for each 3D point of the scene, and renders the corresponding appearance and geometry of any 3D point viewing from an arbitrary angle. The key to our approach is to explicitly integrate the principle of multi-view geometry to obtain the internal representations from observed 2D views, guaranteeing the learned implicit representations meaningful and multi-view consistent. In addition, we introduce an effective neural module to learn general features for each pixel in 2D images, allowing the constructed internal 3D representations to be remarkably general as well. Extensive experiments demonstrate the superiority of our approach.},
  archivePrefix = {arXiv},
  eprint = {2010.04595},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GRF-Trevithick_Yang-2020.pdf;/Users/sunjiaming/Zotero/storage/I9GMMV7R/2010.html},
  journal = {arXiv:2010.04595 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{truongGLUNetGlobalLocalUniversal2020,
  title = {{{GLU}}-{{Net}}: {{Global}}-{{Local Universal Network}} for {{Dense Flow}} and {{Correspondences}}},
  shorttitle = {{{GLU}}-{{Net}}},
  author = {Truong, Prune and Danelljan, Martin and Timofte, Radu},
  year = {2020},
  month = apr,
  abstract = {Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required. In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights. Code and trained models are available at https://github.com/PruneTruong/GLU-Net.},
  archivePrefix = {arXiv},
  eprint = {1912.05524},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GLU-Net-Truong et al-2020.pdf;/Users/sunjiaming/Zotero/storage/SS68TRYQ/1912.html},
  journal = {arXiv:1912.05524 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{truongGOCorBringingGlobally2020,
  title = {{{GOCor}}: {{Bringing Globally Optimized Correspondence Volumes}} into {{Your Neural Network}}},
  shorttitle = {{{GOCor}}},
  author = {Truong, Prune and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
  year = {2020},
  month = sep,
  abstract = {The feature correlation layer serves as a key neural network module in numerous computer vision problems that involve dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors extracted from pairs of locations in two images. However, this point-to-point feature comparison is insufficient when disambiguating multiple similar regions in an image, severely affecting the performance of the end task. We propose GOCor, a fully differentiable dense matching module, acting as a direct replacement to the feature correlation layer. The correspondence volume generated by our module is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. Moreover, our approach is capable of effectively learning spatial matching priors to resolve further matching ambiguities. We analyze our GOCor module in extensive ablative experiments. When integrated into state-of-the-art networks, our approach significantly outperforms the feature correlation layer for the tasks of geometric matching, optical flow, and dense semantic matching. The code and trained models will be made available at github.com/PruneTruong/GOCor.},
  archivePrefix = {arXiv},
  eprint = {2009.07823},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/JMNT27M6/Truong et al. - 2020 - GOCor Bringing Globally Optimized Correspondence .pdf;/Users/sunjiaming/Zotero/storage/AZKXCQNA/2009.html},
  journal = {arXiv:2009.07823 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tsutsuiMinimizingSupervisionFreespace2017,
  title = {Minimizing {{Supervision}} for {{Free}}-Space {{Segmentation}}},
  author = {Tsutsui, Satoshi and Kerola, Tommi and Saito, Shunta and Crandall, David J.},
  year = {2017},
  month = nov,
  abstract = {Identifying ``free-space,'' or safely driveable regions in the scene ahead, is a fundamental task for autonomous navigation. While this task can be addressed using semantic segmentation, the manual labor involved in creating pixelwise annotations to train the segmentation model is very costly. Although weakly supervised segmentation addresses this issue, most methods are not designed for free-space. In this paper, we observe that homogeneous texture and location are two key characteristics of free-space, and develop a novel, practical framework for free-space segmentation with minimal human supervision. Our experiments show that our framework performs better than other weakly supervised methods while using less supervision. Our work demonstrates the potential for performing free-space segmentation without tedious and costly manual annotation, which will be important for adapting autonomous driving systems to different types of vehicles and environments.},
  archivePrefix = {arXiv},
  eprint = {1711.05998},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Minimizing Supervision for Free-space Segmentation-Tsutsui et al-2017.pdf},
  journal = {arXiv:1711.05998 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{tulsianiImplicitMeshReconstruction2020,
  title = {Implicit {{Mesh Reconstruction}} from {{Unannotated Image Collections}}},
  author = {Tulsiani, Shubham and Kulkarni, Nilesh and Gupta, Abhinav},
  year = {2020},
  month = jul,
  abstract = {We present an approach to infer the 3D shape, texture, and camera pose for an object from a single RGB image, using only category-level image collections with foreground masks as supervision. We represent the shape as an image-conditioned implicit function that transforms the surface of a sphere to that of the predicted mesh, while additionally predicting the corresponding texture. To derive supervisory signal for learning, we enforce that: a) our predictions when rendered should explain the available image evidence, and b) the inferred 3D structure should be geometrically consistent with learned pixel to surface mappings. We empirically show that our approach improves over prior work that leverages similar supervision, and in fact performs competitively to methods that use stronger supervision. Finally, as our method enables learning with limited supervision, we qualitatively demonstrate its applicability over a set of about 30 object categories.},
  archivePrefix = {arXiv},
  eprint = {2007.08504},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Implicit Mesh Reconstruction from Unannotated Image Collections-Tulsiani et al-2020.pdf;/Users/sunjiaming/Zotero/storage/Q3LMMK6A/2007.html},
  journal = {arXiv:2007.08504 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tulsianiMultiviewConsistencySupervisory2018,
  title = {Multi-View {{Consistency}} as {{Supervisory Signal}} for {{Learning Shape}} and {{Pose Prediction}}},
  author = {Tulsiani, Shubham and Efros, Alexei A. and Malik, Jitendra},
  year = {2018},
  month = jan,
  abstract = {We present a framework for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as supervisory signal during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor. We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which is beyond the scope of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown.},
  archivePrefix = {arXiv},
  eprint = {1801.03910},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-view Consistency as Supervisory Signal for Learning Shape and Pose-Tulsiani et al-2018.pdf;/Users/sunjiaming/Zotero/storage/9IHC8DAD/1801.html},
  journal = {arXiv:1801.03910 [cs]},
  primaryClass = {cs}
}

@article{tulsianiMultiviewSupervisionSingleview2017,
  ids = {tulsianiMultiviewSupervisionSingleview2017a},
  title = {Multi-View {{Supervision}} for {{Single}}-View {{Reconstruction}} via {{Differentiable Ray Consistency}}},
  author = {Tulsiani, Shubham and Zhou, Tinghui and Efros, Alexei A. and Malik, Jitendra},
  year = {2017},
  month = apr,
  abstract = {We study the notion of consistency between a 3D shape and a 2D observation and propose a differentiable formulation which allows computing gradients of the 3D shape given an observation from an arbitrary view. We do so by reformulating view consistency using a differentiable ray consistency (DRC) term. We show that this formulation can be incorporated in a learning framework to leverage different types of multi-view observations e.g. foreground masks, depth, color images, semantics etc. as supervision for learning single-view 3D prediction. We present empirical analysis of our technique in a controlled setting. We also show that this approach allows us to improve over existing techniques for single-view reconstruction of objects from the PASCAL VOC dataset.},
  archivePrefix = {arXiv},
  eprint = {1704.06254},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-view Supervision for Single-view Reconstruction via Differentiable Ray-Tulsiani et al-2017.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-view Supervision for Single-view Reconstruction via Differentiable Ray-Tulsiani et al-22.pdf;/Users/sunjiaming/Zotero/storage/ASUWYI7N/1704.html;/Users/sunjiaming/Zotero/storage/CJXCGR7K/1704.html},
  journal = {arXiv:1704.06254 [cs]},
  primaryClass = {cs}
}

@article{tulsianiObjectCentricMultiViewAggregation2020,
  title = {Object-{{Centric Multi}}-{{View Aggregation}}},
  author = {Tulsiani, Shubham and Litany, Or and Qi, Charles and Guibas, Leonidas},
  year = {2020},
  month = jul,
  abstract = {We present an approach for aggregating a sparse set of views of an object in order to compute a semi-implicit 3D representation in the form of a volumetric feature grid. Key to our approach is an object-centric canonical 3D coordinate system into which views can be lifted, without explicit camera pose estimation, and then combined -- in a manner that can accommodate a variable number of views and is view order independent. We show that computing a symmetry-aware mapping from pixels to the canonical coordinate system allows us to better propagate information to unseen regions, as well as to robustly overcome pose ambiguities during inference. Our aggregate representation enables us to perform 3D inference tasks like volumetric reconstruction and novel view synthesis, and we use these tasks to demonstrate the benefits of our aggregation approach as compared to implicit or camera-centric alternatives.},
  archivePrefix = {arXiv},
  eprint = {2007.10300},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object-Centric Multi-View Aggregation-Tulsiani et al-2020.pdf;/Users/sunjiaming/Zotero/storage/N367UR35/Tulsiani et al. - 2020 - Object-Centric Multi-View Aggregation.pdf;/Users/sunjiaming/Zotero/storage/AJVNF8Y2/2007.html},
  journal = {arXiv:2007.10300 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{tulyakovPracticalDeepStereo,
  title = {Practical {{Deep Stereo}} ({{PDS}}): {{Toward}} Applications-Friendly Deep Stereo Matching},
  author = {Tulyakov, Stepan and Ivanov, Anton and Fleuret, Fran{\c c}ois},
  pages = {11},
  abstract = {End-to-end deep-learning networks recently demonstrated extremely good performance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be trained for a given disparity range.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Practical Deep Stereo (PDS)-Tulyakov et al-.pdf},
  keywords = {disparity},
  language = {en}
}

@article{tungLearningSpatialCommon2018,
  title = {Learning {{Spatial Common Sense}} with {{Geometry}}-{{Aware Recurrent Networks}}},
  author = {Tung, Hsiao-Yu Fish and Cheng, Ricson and Fragkiadaki, Katerina},
  year = {2018},
  month = dec,
  abstract = {We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to ``lift'' 2D visual features and integrate them over time into latent 3D feature maps of the scene. They are equipped with differentiable geometric operations, such as projection, unprojection, egomotion estimation and stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature space. We train the proposed architectures to predict novel image views given short frame sequences as input. Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations, and greatly outperform predictions of previous works that do not consider egomotion stabilization or a space-aware latent feature space. We train the proposed architectures to detect and segment objects in 3D, using the latent 3D feature map as input\textemdash as opposed to any input 2D video frame. The resulting detections are permanent: they continue to exist even when an object gets occluded or leaves the field of view. Our experiments suggest the proposed space-aware latent feature arrangement and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.},
  archivePrefix = {arXiv},
  eprint = {1901.00003},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Spatial Common Sense with Geometry-Aware Recurrent Networks-Tung et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Spatial Common Sense with Geometry-Aware Recurrent Networks-Tung et al-22.pdf;/Users/sunjiaming/Zotero/storage/VXYUAHKE/1901.html},
  journal = {arXiv:1901.00003 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{tyszkiewiczDISKLearningLocal2020,
  title = {{{DISK}}: {{Learning}} Local Features with Policy Gradient},
  shorttitle = {{{DISK}}},
  author = {Tyszkiewicz, Micha{\l} J. and Fua, Pascal and Trulls, Eduard},
  year = {2020},
  month = jun,
  abstract = {Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2006.13566},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/V9C8AFRP/Tyszkiewicz et al. - 2020 - DISK Learning local features with policy gradient.pdf;/Users/sunjiaming/Zotero/storage/4PEIKVSI/2006.html},
  journal = {arXiv:2006.13566 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{ulusoySemanticMultiviewStereo2017,
  title = {Semantic {{Multi}}-View {{Stereo}}: {{Jointly Estimating Objects}} and {{Voxels}}},
  shorttitle = {Semantic {{Multi}}-View {{Stereo}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ulusoy, Ali Osman and Black, Michael J. and Geiger, Andreas},
  year = {2017},
  month = jul,
  pages = {4531--4540},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.482},
  abstract = {Dense 3D reconstruction from RGB images is a highly ill-posed problem due to occlusions, textureless or reflective surfaces, as well as other challenges. We propose objectlevel shape priors to address these ambiguities. Towards this goal, we formulate a probabilistic model that integrates multi-view image evidence with 3D shape information from multiple objects. Inference in this model yields a dense 3D reconstruction of the scene as well as the existence and precise 3D pose of the objects in it. Our approach is able to recover fine details not captured in the input shapes while defaulting to the input models in occluded regions where image evidence is weak. Due to its probabilistic nature, the approach is able to cope with the approximate geometry of the 3D models as well as input shapes that are not present in the scene. We evaluate the approach quantitatively on several challenging indoor and outdoor datasets.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Semantic Multi-view Stereo-Ulusoy et al-2017.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@inproceedings{umetaniExploringGenerative3D2017,
  title = {Exploring Generative {{3D}} Shapes Using Autoencoder Networks},
  booktitle = {{{SIGGRAPH Asia}} 2017 {{Technical Briefs}} on   - {{SA}} '17},
  author = {Umetani, Nobuyuki},
  year = {2017},
  pages = {1--4},
  publisher = {{ACM Press}},
  address = {{Bangkok, Thailand}},
  doi = {10.1145/3145749.3145758},
  abstract = {We propose a new algorithm for converting unstructured triangle meshes into ones with a consistent topology for machine learning applications. We combine the orthogonal depth map computation and the shrink wrapping approach to efficiently and robustly parameterize the triangle geometry regardless of imperfections such as inverted faces, holes, and self-intersections. The converted mesh is consistently and compactly parameterized and thus is suitable for machine learning. We use an autoencoder network to extract the manifold of shapes in the same category to explore and synthesize a variety of shapes. Furthermore, we introduce a direct manipulation interface to navigate the synthesis. We demonstrate our approach with over one thousand car shapes represented in unstructured triangle meshes.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Exploring generative 3D shapes using autoencoder networks-Umetani-2017.pdf},
  isbn = {978-1-4503-5406-6},
  keywords = {shape learning},
  language = {en}
}

@article{ummenhoferDeMoNDepthMotion2017,
  title = {{{DeMoN}}: {{Depth}} and {{Motion Network}} for {{Learning Monocular Stereo}}},
  shorttitle = {{{DeMoN}}},
  author = {Ummenhofer, Benjamin and Zhou, Huizhong and Uhrig, Jonas and Mayer, Nikolaus and Ilg, Eddy and Dosovitskiy, Alexey and Brox, Thomas},
  year = {2017},
  month = jul,
  pages = {5622--5631},
  doi = {10.1109/CVPR.2017.596},
  abstract = {In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.},
  archivePrefix = {arXiv},
  eprint = {1612.02401},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeMoN-Ummenhofer et al-2017.pdf;/Users/sunjiaming/Zotero/storage/TYHS8ADX/1612.html},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,neufu_paper}
}

@article{usenkoVisualInertialMappingNonLinear2019,
  title = {Visual-{{Inertial Mapping}} with {{Non}}-{{Linear Factor Recovery}}},
  author = {Usenko, Vladyslav and Demmel, Nikolaus and Schubert, David and St{\"u}ckler, J{\"o}rg and Cremers, Daniel},
  year = {2019},
  month = apr,
  abstract = {Cameras and inertial measurement units are complementary sensors for ego-motion estimation and environment mapping. Their combination makes visual-inertial odometry (VIO) systems more accurate and robust. For globally consistent mapping, however, combining visual and inertial information is not straightforward. To estimate the motion and geometry with a set of images large baselines are required. Because of that, most systems operate on keyframes that have large time intervals between each other. Inertial data on the other hand quickly degrades with the duration of the intervals and after several seconds of integration, it typically contains only little useful information. In this paper, we propose to extract relevant information for visual-inertial mapping from visual-inertial odometry using non-linear factor recovery. We reconstruct a set of non-linear factors that make an optimal approximation of the information on the trajectory accumulated by VIO. To obtain a globally consistent map we combine these factors with loop-closing constraints using bundle adjustment. The VIO factors make the roll and pitch angles of the global map observable, and improve the robustness and the accuracy of the mapping. In experiments on a public benchmark, we demonstrate superior performance of our method over the state-of-the-art approaches.},
  archivePrefix = {arXiv},
  eprint = {1904.06504},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual-Inertial Mapping with Non-Linear Factor Recovery-Usenko et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MNM485CP/1904.html},
  journal = {arXiv:1904.06504 [cs]},
  primaryClass = {cs}
}

@article{uyDeformationAware3DModel2020,
  title = {Deformation-{{Aware 3D Model Embedding}} and {{Retrieval}}},
  author = {Uy, Mikaela Angelina and Huang, Jingwei and Sung, Minhyuk and Birdal, Tolga and Guibas, Leonidas},
  year = {2020},
  month = apr,
  abstract = {We introduce a new problem of \$\textbackslash textit\{retrieving\}\$ 3D models that are not just similar but are deformable to a given query shape. We then present a novel deep \$\textbackslash textit\{deformation-aware\}\$ embedding to solve this retrieval task. 3D model retrieval is a fundamental operation for recovering a clean and complete 3D model from a noisy and partial 3D scan. However, given a finite collection of 3D shapes, even the closest model to a query may not be a satisfactory reconstruction. This motivates us to apply 3D model deformation techniques to adapt the retrieved model so as to better fit the query. Yet, certain restrictions are enforced in most 3D deformation techniques to preserve important features of the original model that prevent a perfect fitting of the deformed model to the query. This gap between the deformed model and the query induces \$\textbackslash textit\{asymmetric\}\$ relationships among the models, which cannot be dealt with typical metric learning techniques. Thus, to retrieve the best models for fitting, we propose a novel deep embedding approach that learns the asymmetric relationships by leveraging location-dependent egocentric distance fields. We also propose two strategies for training the embedding network. We demonstrate that both of these approaches outperform other baselines in both synthetic evaluations and real 3D object reconstruction.},
  archivePrefix = {arXiv},
  eprint = {2004.01228},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deformation-Aware 3D Model Embedding and Retrieval-Uy et al-2020.pdf;/Users/sunjiaming/Zotero/storage/XBJRKER5/2004.html},
  journal = {arXiv:2004.01228 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{uyRevisitingPointCloud2019,
  title = {Revisiting {{Point Cloud Classification}}: {{A New Benchmark Dataset}} and {{Classification Model}} on {{Real}}-{{World Data}}},
  shorttitle = {Revisiting {{Point Cloud Classification}}},
  author = {Uy, Mikaela Angelina and Pham, Quang-Hieu and Hua, Binh-Son and Nguyen, Duc Thanh and Yeung, Sai-Kit},
  year = {2019},
  month = aug,
  abstract = {Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (\textasciitilde 92\%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.},
  archivePrefix = {arXiv},
  eprint = {1908.04616},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Revisiting Point Cloud Classification-Uy et al-2019.pdf;/Users/sunjiaming/Zotero/storage/HAAPAKY7/1908.html},
  journal = {arXiv:1908.04616 [cs]},
  primaryClass = {cs}
}

@article{valentinDepthMotionSmartphone2019,
  title = {Depth from Motion for Smartphone {{AR}}},
  author = {Valentin, Julien and Kowdle, Adarsh and Barron, Jonathan T. and Wadhwa, Neal and Dzitsiuk, Max and Schoenberg, Michael and Verma, Vivek and Csaszar, Ambrus and Turner, Eric and Dryanovski, Ivan and Afonso, Joao and Pascoal, Jose and Tsotsos, Konstantine and Leung, Mira and Schmidt, Mirko and Guleryuz, Onur and Khamis, Sameh and Tankovitch, Vladimir and Fanello, Sean and Izadi, Shahram and Rhemann, Christoph},
  year = {2019},
  month = jan,
  volume = {37},
  pages = {1--19},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275041},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Depth from motion for smartphone AR-Valentin et al-2019.pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {neufu_paper},
  language = {en},
  number = {6}
}

@inproceedings{valentinMeshBasedSemantic2013,
  title = {Mesh {{Based Semantic Modelling}} for {{Indoor}} and {{Outdoor Scenes}}},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Valentin, Julien P.C. and Sengupta, Sunando and Warrell, Jonathan and Shahrokni, Ali and Torr, Philip H.S.},
  year = {2013},
  month = jun,
  pages = {2067--2074},
  publisher = {{IEEE}},
  address = {{Portland, OR, USA}},
  doi = {10.1109/CVPR.2013.269},
  abstract = {Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a significant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.},
  file = {/Users/sunjiaming/Zotero/storage/G8CUTF7S/Valentin et al. - 2013 - Mesh Based Semantic Modelling for Indoor and Outdo.pdf},
  isbn = {978-0-7695-4989-7},
  language = {en}
}

@article{valentinSemanticPaintInteractive3D2015,
  title = {{{SemanticPaint}}: {{Interactive 3D Labeling}} and {{Learning}} at Your {{Fingertips}}},
  shorttitle = {{{SemanticPaint}}},
  author = {Valentin, Julien and Torr, Philip and Vineet, Vibhav and Cheng, Ming-Ming and Kim, David and Shotton, Jamie and Kohli, Pushmeet and Nie{\ss}ner, Matthias and Criminisi, Antonio and Izadi, Shahram},
  year = {2015},
  month = nov,
  volume = {34},
  pages = {1--17},
  issn = {07300301},
  doi = {10.1145/2751556},
  file = {/Users/sunjiaming/Zotero/storage/8JHDBBXJ/Valentin et al. - 2015 - SemanticPaint Interactive 3D Labeling and Learnin.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {5}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Attention Is All You Need-Vaswani et al-2017.pdf;/Users/sunjiaming/Zotero/storage/MDTUKW97/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{vijayanarasimhanSfMNetLearningStructure2017,
  title = {{{SfM}}-{{Net}}: {{Learning}} of {{Structure}} and {{Motion}} from {{Video}}},
  shorttitle = {{{SfM}}-{{Net}}},
  author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
  year = {2017},
  month = apr,
  abstract = {We propose SfM-Net, a geometry-aware neural network for motion estimation in videos that decomposes frame-to-frame pixel motion in terms of scene and object depth, camera motion and 3D object rotations and translations. Given a sequence of frames, SfM-Net predicts depth, segmentation, camera and rigid object motions, converts those into a dense frame-to-frame motion field (optical flow), differentiably warps frames in time to match pixels and back-propagates. The model can be trained with various degrees of supervision: 1) self-supervised by the re-projection photometric error (completely unsupervised), 2) supervised by ego-motion (camera motion), or 3) supervised by depth (e.g., as provided by RGBD sensors). SfM-Net extracts meaningful depth estimates and successfully estimates frame-to-frame camera rotations and translations. It often successfully segments the moving objects in the scene, even though such supervision is never provided.},
  archivePrefix = {arXiv},
  eprint = {1704.07804},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/PIMA9HBW/Vijayanarasimhan et al. - 2017 - SfM-Net Learning of Structure and Motion from Vid.pdf;/Users/sunjiaming/Zotero/storage/MNBB5HWY/1704.html},
  journal = {arXiv:1704.07804 [cs]},
  primaryClass = {cs}
}

@article{vogiatzisVideobasedRealtimeMultiview2011,
  title = {Video-Based, Real-Time Multi-View Stereo},
  author = {Vogiatzis, George and Hern{\'a}ndez, Carlos},
  year = {2011},
  month = jun,
  volume = {29},
  pages = {434--441},
  issn = {02628856},
  doi = {10.1016/j.imavis.2011.01.006},
  abstract = {We investigate the problem of obtaining a dense reconstruction in real-time, from a live video stream. In recent years, Multi-view stereo (MVS) has received considerable attention and a number of methods have been proposed. However, most methods operate under the assumption of a relatively sparse set of still images as input and unlimited computation time. Video based MVS has received less attention despite the fact that video sequences offer significant benefits in terms of usability of MVS systems. In this paper we propose a novel video based MVS algorithm that is suitable for real-time, interactive 3d modeling with a hand-held camera. The key idea is a per-pixel, probabilistic depth estimation scheme that updates posterior depth distributions with every new frame. The current implementation is capable of updating 15 million distributions per second. We evaluate the proposed method against the state-of-the-art real-time MVS method and show improvement in terms of accuracy.},
  file = {/Users/sunjiaming/Zotero/storage/NAYH3X3Q/Vogiatzis and Hernández - 2011 - Video-based, real-time multi-view stereo.pdf},
  journal = {Image and Vision Computing},
  keywords = {neufu_paper},
  language = {en},
  number = {7}
}

@misc{VolumeRenderingRay,
  title = {Volume {{Rendering With Ray Casting}}},
  file = {/Users/sunjiaming/Zotero/storage/CJ69E76R/ray-cast.html},
  howpublished = {https://web.cs.wpi.edu/\textasciitilde matt/courses/cs563/talks/powwie/p1/ray-cast.htm}
}

@article{vonstumbergGNNetGaussNewtonLoss2019,
  title = {{{GN}}-{{Net}}: {{The Gauss}}-{{Newton Loss}} for {{Multi}}-{{Weather Relocalization}}},
  shorttitle = {{{GN}}-{{Net}}},
  author = {{von Stumberg}, Lukas and Wenzel, Patrick and Khan, Qadeer and Cremers, Daniel},
  year = {2019},
  month = apr,
  abstract = {Direct SLAM methods have shown exceptional performance on odometry tasks. However, they are susceptible to dynamic lighting and weather changes while also suffering from a bad initialization on large baselines. To overcome this, we propose GN-Net: a network optimized with the novel Gauss-Newton loss for training weather invariant deep features, tailored for direct image alignment. Our network can be trained with pixel correspondences between images even from different sequences. Experiments on both simulated and real-world datasets demonstrate that our approach is more robust against bad initialization, variations in day-time, and weather changes thereby outperforming state-of-the-art direct and indirect methods. Furthermore, we release an evaluation benchmark for relocalization tracking against different types of weather. Our benchmark is available at {$<$}a ref="https://vision.in.tum.de/gn-net" target="\_blank"{$>$}this URL{$<$}/a{$>$}.},
  archivePrefix = {arXiv},
  eprint = {1904.11932},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GN-Net-von Stumberg et al-22.pdf;/Users/sunjiaming/Zotero/storage/VBHSZJZV/gn-net-supplementary.pdf;/Users/sunjiaming/Zotero/storage/SW3PMKPQ/1904.html},
  journal = {arXiv:1904.11932 [cs]},
  primaryClass = {cs}
}

@article{vonstumbergLMRelocLevenbergMarquardtBased2020,
  title = {{{LM}}-{{Reloc}}: {{Levenberg}}-{{Marquardt Based Direct Visual Relocalization}}},
  shorttitle = {{{LM}}-{{Reloc}}},
  author = {{von Stumberg}, Lukas and Wenzel, Patrick and Yang, Nan and Cremers, Daniel},
  year = {2020},
  month = oct,
  abstract = {We present LM-Reloc -- a novel approach for visual relocalization based on direct image alignment. In contrast to prior works that tackle the problem with a feature-based formulation, the proposed method does not rely on feature matching and RANSAC. Hence, the method can utilize not only corners but any region of the image with gradients. In particular, we propose a loss formulation inspired by the classical Levenberg-Marquardt algorithm to train LM-Net. The learned features significantly improve the robustness of direct image alignment, especially for relocalization across different conditions. To further improve the robustness of LM-Net against large image baselines, we propose a pose estimation network, CorrPoseNet, which regresses the relative pose to bootstrap the direct image alignment. Evaluations on the CARLA and Oxford RobotCar relocalization tracking benchmark show that our approach delivers more accurate results than previous state-of-the-art methods while being comparable in terms of robustness.},
  archivePrefix = {arXiv},
  eprint = {2010.06323},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LM-Reloc-von Stumberg et al-2020.pdf;/Users/sunjiaming/Zotero/storage/493H3635/2010.html},
  journal = {arXiv:2010.06323 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{voSpatiotemporalBundleAdjustment,
  title = {Spatiotemporal {{Bundle Adjustment}} for {{Dynamic 3D Human Reconstruction}} in the {{Wild}}},
  author = {Vo, Minh and Sheikh, Yaser and Narasimhan, Srinivasa G},
  pages = {14},
  abstract = {Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene. The triangulation constraint, however, is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not designed to estimate the temporal alignment between cameras. We present a spatiotemporal bundle adjustment framework that jointly optimizes four coupled sub-problems: estimating camera intrinsics and extrinsics, triangulating 3D static points, as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points. Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline, validated on a large motion capture corpus of human subjects. We devise an incremental reconstruction and alignment algorithm to strictly enforce the motion prior during the spatiotemporal bundle adjustment. This algorithm is further made more efficient by a divide and conquer scheme with little loss in accuracy. We apply this algorithm to reconstruct 3D motion trajectories of human bodies in a dynamic event captured by uncalibrated and unsynchronized video streams in the wild. To make the reconstruction visually more interpretable, we fit a statistical human body model to the video streams. This fitting is constrained by the same motion prior, the 3D trajectory of the dynamic points, and other semantic cues extracted from the images. Because the videos are aligned with sub-frame precision, we reconstruct 3D motion at much higher temporal resolution than the input videos.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Spatiotemporal Bundle Adjustment for Dynamic 3D Human Reconstruction in the Wild-Vo et al-.pdf},
  language = {en}
}

@article{vuontelaAudiospatialVisuospatialWorking2003,
  title = {Audiospatial and {{Visuospatial Working Memory}} in 6\textendash 13 {{Year Old School Children}}},
  author = {Vuontela, Virve and Steenari, Maija-Riikka and Carlson, Synn{\"o}ve and Koivisto, Juha and Fj{\"a}llberg, Mika and Aronen, Eeva T.},
  year = {2003},
  month = jan,
  volume = {10},
  pages = {74--81},
  issn = {1072-0502},
  doi = {10.1101/lm.53503},
  abstract = {The neural processes subserving working memory, and brain structures underlying this system, continue to develop during childhood. We investigated the effects of age and gender on audiospatial and visuospatial working memory in a nonclinical sample of school-aged children using n-back tasks. The results showed that auditory and visual working memory performance improves with age, suggesting functional maturation of underlying cognitive processes and brain areas. The gender differences found in the performance of working memory tasks suggest a larger degree of immaturity in boys than girls at the age period of 6\textendash 10 yr. The differences observed between the mastering of auditory and visual working memory tasks may indicate that visual working memory reaches functional maturity earlier than the corresponding auditory system.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Audiospatial and Visuospatial Working Memory in 6–13 Year Old School Children-Vuontela et al-2003.pdf},
  journal = {Learning \& Memory},
  number = {1},
  pmcid = {PMC196650},
  pmid = {12551966}
}

@article{vyasFastTransformersClustered2020,
  title = {Fast {{Transformers}} with {{Clustered Attention}}},
  author = {Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = jul,
  abstract = {Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance.},
  archivePrefix = {arXiv},
  eprint = {2007.04825},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/LIQ7XG6Z/Vyas et al. - 2020 - Fast Transformers with Clustered Attention.pdf;/Users/sunjiaming/Zotero/storage/JLXXMBKF/2007.html},
  journal = {arXiv:2007.04825 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{waechterLetThereBe2014,
  title = {Let {{There Be Color}}! {{Large}}-{{Scale Texturing}} of {{3D Reconstructions}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Waechter, Michael and Moehrle, Nils and Goesele, Michael},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8693},
  pages = {836--850},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10602-1_54},
  file = {/Users/sunjiaming/Zotero/storage/B6Q8BS39/Waechter et al. - 2014 - Let There Be Color! Large-Scale Texturing of 3D Re.pdf},
  isbn = {978-3-319-10601-4 978-3-319-10602-1},
  language = {en}
}

@article{wagstaffLimitationsRepresentingFunctions2019,
  title = {On the {{Limitations}} of {{Representing Functions}} on {{Sets}}},
  author = {Wagstaff, Edward and Fuchs, Fabian B. and Engelcke, Martin and Posner, Ingmar and Osborne, Michael},
  year = {2019},
  month = jan,
  abstract = {Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.},
  archivePrefix = {arXiv},
  eprint = {1901.09006},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On the Limitations of Representing Functions on Sets-Wagstaff et al-2019.pdf},
  journal = {arXiv:1901.09006 [cs, stat]},
  keywords = {3d feature learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{waldControlledEnvironments3D2020,
  title = {Beyond {{Controlled Environments}}: {{3D Camera Re}}-{{Localization}} in {{Changing Indoor Scenes}}},
  shorttitle = {Beyond {{Controlled Environments}}},
  author = {Wald, Johanna and Sattler, Torsten and Golodetz, Stuart and Cavallari, Tommaso and Tombari, Federico},
  year = {2020},
  month = aug,
  abstract = {Long-term camera re-localization is an important task with numerous computer vision and robotics applications. Whilst various outdoor benchmarks exist that target lighting, weather and seasonal changes, far less attention has been paid to appearance changes that occur indoors. This has led to a mismatch between popular indoor benchmarks, which focus on static scenes, and indoor environments that are of interest for many real-world applications. In this paper, we adapt 3RScan - a recently introduced indoor RGB-D dataset designed for object instance re-localization - to create RIO10, a new long-term camera re-localization benchmark focused on indoor scenes. We propose new metrics for evaluating camera re-localization and explore how state-of-the-art camera re-localizers perform according to these metrics. We also examine in detail how different types of scene change affect the performance of different methods, based on novel ways of detecting such changes in a given RGB-D frame. Our results clearly show that long-term indoor re-localization is an unsolved problem. Our benchmark and tools are publicly available at waldjohannau.github.io/RIO10},
  archivePrefix = {arXiv},
  eprint = {2008.02004},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Beyond Controlled Environments-Wald et al-2020.pdf;/Users/sunjiaming/Zotero/storage/ISE9TDDS/2008.html},
  journal = {arXiv:2008.02004 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{waldRIO3DObject2019,
  title = {{{RIO}}: {{3D Object Instance Re}}-{{Localization}} in {{Changing Indoor Environments}}},
  shorttitle = {{{RIO}}},
  author = {Wald, Johanna and Avetisyan, Armen and Navab, Nassir and Tombari, Federico and Nie{\ss}ner, Matthias},
  year = {2019},
  month = aug,
  abstract = {In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58\%.},
  archivePrefix = {arXiv},
  eprint = {1908.06109},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RIO-Wald et al-2019.pdf;/Users/sunjiaming/Zotero/storage/KXQWYPWL/1908.html},
  journal = {arXiv:1908.06109 [cs]},
  primaryClass = {cs}
}

@article{waldropNewsFeatureWhat2019,
  title = {News {{Feature}}: {{What}} Are the Limits of Deep Learning?},
  shorttitle = {News {{Feature}}},
  author = {Waldrop, M. Mitchell},
  year = {2019},
  month = jan,
  volume = {116},
  pages = {1074--1077},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1821594116},
  abstract = {Author Information M. Mitchell Waldrop, Science Writer},
  copyright = {\textcopyright{} 2019 . Published under the PNAS license.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/News Feature-Waldrop-2019.pdf;/Users/sunjiaming/Zotero/storage/N8XU8358/1074.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {learning},
  language = {en},
  number = {4},
  pmid = {30670601}
}

@article{wang3DN3DDeformation2019,
  title = {{{3DN}}: {{3D Deformation Network}}},
  shorttitle = {{{3DN}}},
  author = {Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
  year = {2019},
  month = mar,
  abstract = {Applications in virtual and augmented reality create a demand for rapid creation and easy access to large sets of 3D models. An effective way to address this demand is to edit or deform existing 3D models based on a reference, e.g., a 2D image which is very easy to acquire. Given such a source 3D model and a target which can be a 2D image, 3D model, or a point cloud acquired as a depth scan, we introduce 3DN, an end-to-end network that deforms the source model to resemble the target. Our method infers per-vertex offset displacements while keeping the mesh connectivity of the source model fixed. We present a training strategy which uses a novel differentiable operation, mesh sampling operator, to generalize our method across source and target models with varying mesh densities. Mesh sampling operator can be seamlessly integrated into the network to handle meshes with different topologies. Qualitative and quantitative results show that our method generates higher quality results compared to the state-of-the art learning-based methods for 3D shape generation. Code is available at github.com/laughtervv/3DN.},
  archivePrefix = {arXiv},
  eprint = {1903.03322},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3DN-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/VZ9KD4WQ/1903.html},
  journal = {arXiv:1903.03322 [cs]},
  primaryClass = {cs}
}

@article{wang6PACKCategorylevel6D2019,
  title = {6-{{PACK}}: {{Category}}-Level {{6D Pose Tracker}} with {{Anchor}}-{{Based Keypoints}}},
  shorttitle = {6-{{PACK}}},
  author = {Wang, Chen and {Mart{\'i}n-Mart{\'i}n}, Roberto and Xu, Danfei and Lv, Jun and Lu, Cewu and {Fei-Fei}, Li and Savarese, Silvio and Zhu, Yuke},
  year = {2019},
  month = oct,
  abstract = {We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real-time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.},
  archivePrefix = {arXiv},
  eprint = {1910.10750},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/6-PACK-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MZKCZCEL/1910.html},
  journal = {arXiv:1910.10750 [cs]},
  primaryClass = {cs}
}

@article{wangAdaptiveOCNN2018,
  title = {Adaptive {{O}}-{{CNN}}},
  author = {Wang, Peng-Shuai and Sun, Chun-Yu and Liu, Yang P. and Tong, Xin},
  year = {2018},
  doi = {10.1145/3272127.3275050},
  abstract = {We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding. Different from volumetric-based or octree-based CNN methods that represent a 3D shape with voxels in the same resolution, our method represents a 3D shape adaptively with octants at different levels and models the 3D shape within each octant with a planar patch. Based on this adaptive patch-based representation, we propose an Adaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The Adaptive O-CNN encoder takes the planar patch normal and displacement as input and performs 3D convolutions only at the octants at each level, while the Adaptive O-CNN decoder infers the shape occupancy and subdivision status of octants at each level and estimates the best plane normal and displacement for each leaf octant. As a general framework for 3D shape analysis and generation, the Adaptive O-CNN not only reduces the memory and computational cost, but also offers better shape generation capability than the existing 3D-CNN approaches. We validate Adaptive O-CNN in terms of efficiency and effectiveness on different shape analysis and generation tasks, including shape classification, 3D autoencoding, shape prediction from a single image, and shape completion for noisy and incomplete point clouds.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Adaptive O-CNN-Wang et al-2018.pdf},
  journal = {SIGGRAPH Asia '18}
}

@article{wangAssociativelySegmentingInstances2019,
  title = {Associatively {{Segmenting Instances}} and {{Semantics}} in {{Point Clouds}}},
  author = {Wang, Xinlong and Liu, Shu and Shen, Xiaoyong and Shen, Chunhua and Jia, Jiaya},
  year = {2019},
  month = feb,
  abstract = {A 3D point cloud describes the real scene precisely and intuitively.To date how to segment diversified elements in such an informative 3D scene is rarely discussed. In this paper, we first introduce a simple and flexible framework to segment instances and semantics in point clouds simultaneously. Then, we propose two approaches which make the two tasks take advantage of each other, leading to a win-win situation. Specifically, we make instance segmentation benefit from semantic segmentation through learning semantic-aware point-level instance embedding. Meanwhile, semantic features of the points belonging to the same instance are fused together to make more accurate per-point semantic predictions. Our method largely outperforms the state-of-the-art method in 3D instance segmentation along with a significant improvement in 3D semantic segmentation. Code has been made available at: https://github.com/WXinlong/ASIS.},
  archivePrefix = {arXiv},
  eprint = {1902.09852},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Associatively Segmenting Instances and Semantics in Point Clouds-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QDRJTJIK/1902.html},
  journal = {arXiv:1902.09852 [cs]},
  primaryClass = {cs}
}

@article{wangAxialDeepLabStandAloneAxialAttention2020,
  title = {Axial-{{DeepLab}}: {{Stand}}-{{Alone Axial}}-{{Attention}} for {{Panoptic Segmentation}}},
  shorttitle = {Axial-{{DeepLab}}},
  author = {Wang, Huiyu and Zhu, Yukun and Green, Bradley and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  year = {2020},
  month = aug,
  abstract = {Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8\% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.},
  archivePrefix = {arXiv},
  eprint = {2003.07853},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Axial-DeepLab-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NJMFD54J/2003.html},
  journal = {arXiv:2003.07853 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{wangCoherentPointDrift2019,
  title = {Coherent {{Point Drift Networks}}: {{Unsupervised Learning}} of {{Non}}-{{Rigid Point Set Registration}}},
  shorttitle = {Coherent {{Point Drift Networks}}},
  author = {Wang, Lingjing and Fang, Yi},
  year = {2019},
  month = jun,
  abstract = {Given new pairs of source and target point sets, standard point set registration methods often repeatedly conduct the independent iterative search of desired geometric transformation to align the source point set with the target one. This limits their use in applications to handle the real-time point set registration with large volume dataset. This paper presents a novel method, named coherent point drift networks (CPD-Net), for unsupervised learning of geometric transformation towards real-time non-rigid point set registration. In contrast to previous efforts (e.g. coherent point drift), CPD-Net can learn displacement field function to estimate geometric transformation from a training dataset, consequently, to predict the desired geometric transformation for the alignment of previously unseen pairs without any additional iterative optimization process. Furthermore, CPD-Net leverages the power of deep neural network to fit an arbitrary function, that adaptively accommodates different levels of complexity of the desired geometric transformation. Particularly, CPD-Net is proved with a theoretical guarantee to learn a continuous displacement vector function that could further avoid imposing additional parametric smoothness constraint as in previous works. Our experiments verify CPD-Net's impressive performance for non-rigid point set registration on various 2D/3D datasets, even in presence of significant displacement noise, outliers, and missing points. Our code is availabel at https://github.com/nyummvc/CPD-Net.},
  archivePrefix = {arXiv},
  eprint = {1906.03039},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Coherent Point Drift Networks-Wang_Fang-2019.pdf;/Users/sunjiaming/Zotero/storage/C2SEML42/1906.html},
  journal = {arXiv:1906.03039 [cs]},
  primaryClass = {cs}
}

@article{wangDeepClosestPoint2019,
  title = {Deep {{Closest Point}}: {{Learning Representations}} for {{Point Cloud Registration}}},
  shorttitle = {Deep {{Closest Point}}},
  author = {Wang, Yue and Solomon, Justin M.},
  year = {2019},
  month = may,
  abstract = {Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer, to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.},
  archivePrefix = {arXiv},
  eprint = {1905.03304},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Closest Point-Wang_Solomon-2019.pdf;/Users/sunjiaming/Zotero/storage/8VBWY483/1905.html},
  journal = {arXiv:1905.03304 [cs]},
  primaryClass = {cs}
}

@article{wangDeepNRSfM3D2020,
  title = {Deep {{NRSfM}}++: {{Towards 3D Reconstruction}} in the {{Wild}}},
  shorttitle = {Deep {{NRSfM}}++},
  author = {Wang, Chaoyang and Lin, Chen-Hsuan and Lucey, Simon},
  year = {2020},
  month = jan,
  abstract = {The recovery of 3D shape and pose solely from 2D landmarks stemming from a large ensemble of images can be viewed as a non-rigid structure from motion (NRSfM) problem. To date, however, the application of NRSfM to problems in the wild has been problematic. Classical NRSfM approaches do not scale to large numbers of images and can only handle certain types of 3D structure (e.g. low-rank). A recent breakthrough in this problem has allowed for the reconstruction of a substantially broader set of 3D structures, dramatically expanding the approach's importance to many problems in computer vision. However, the approach is still limited in that (i) it cannot handle missing/occluded points, and (ii) it is applicable only to weak-perspective camera models. In this paper, we present Deep NRSfM++, an approach to allow NRSfM to be truly applicable in the wild by offering up innovative solutions to the above two issues. Furthermore, we demonstrate state-of-the-art performance across numerous benchmarks, even against recent methods based on deep neural networks.},
  archivePrefix = {arXiv},
  eprint = {2001.10090},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep NRSfM++-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/8I8PWTRC/2001.html},
  journal = {arXiv:2001.10090 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{wangDenseFusion6DObject2019,
  title = {{{DenseFusion}}: {{6D Object Pose Estimation}} by {{Iterative Dense Fusion}}},
  shorttitle = {{{DenseFusion}}},
  author = {Wang, Chen and Xu, Danfei and Zhu, Yuke and {Mart{\'i}n-Mart{\'i}n}, Roberto and Lu, Cewu and {Fei-Fei}, Li and Savarese, Silvio},
  year = {2019},
  month = jan,
  abstract = {A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.},
  archivePrefix = {arXiv},
  eprint = {1901.04780},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DenseFusion-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/GZTXUTNQ/1901.html},
  journal = {arXiv:1901.04780 [cs]},
  primaryClass = {cs}
}

@article{wangDepthawareCNNRGBD2018,
  title = {Depth-Aware {{CNN}} for {{RGB}}-{{D Segmentation}}},
  author = {Wang, Weiyue and Neumann, Ulrich},
  year = {2018},
  month = mar,
  abstract = {Convolutional neural networks (CNN) are limited by the lack of capability to handle geometric information due to the fixed grid kernel structure. The availability of depth data enables progress in RGB-D semantic segmentation with CNNs. State-of-the-art methods either use depth as additional images or process spatial information in 3D volumes or point clouds. These methods suffer from high computation and memory cost. To address these issues, we present Depth-aware CNN by introducing two intuitive, flexible and effective operations: depth-aware convolution and depth-aware average pooling. By leveraging depth similarity between pixels in the process of information propagation, geometry is seamlessly incorporated into CNN. Without introducing any additional parameters, both operators can be easily integrated into existing CNNs. Extensive experiments and ablation studies on challenging RGB-D semantic segmentation benchmarks validate the effectiveness and flexibility of our approach.},
  archivePrefix = {arXiv},
  eprint = {1803.06791},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Depth-aware CNN for RGB-D Segmentation-Wang_Neumann-2018.pdf;/Users/sunjiaming/Zotero/storage/6XA2D4KZ/1803.html},
  journal = {arXiv:1803.06791 [cs]},
  primaryClass = {cs}
}

@article{wangDirectShapePhotometricAlignment2019,
  title = {{{DirectShape}}: {{Photometric Alignment}} of {{Shape Priors}} for {{Visual Vehicle Pose}} and {{Shape Estimation}}},
  shorttitle = {{{DirectShape}}},
  author = {Wang, Rui and Yang, Nan and Stueckler, Joerg and Cremers, Daniel},
  year = {2019},
  month = apr,
  abstract = {3D scene understanding from images is a challenging problem which is encountered in robotics, augmented reality and autonomous driving scenarios. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from stereo images of road scenes. Unlike previous work that relies on geometric alignment of shapes with dense stereo reconstructions, our approach works directly on images and infers shape and pose efficiently through combined photometric and silhouette alignment of 3D shape priors with a stereo image. We use a shape prior that represents cars in a low-dimensional linear embedding of volumetric signed distance functions. For efficiently measuring the consistency with both alignment terms, we propose an adaptive sparse point selection scheme. In experiments, we demonstrate superior performance of our method in pose estimation and shape reconstruction over a state-of-the-art approach that uses geometric alignment with dense stereo reconstructions. Our approach can also boost the performance of deep-learning based approaches to 3D object detection as a refinement method. We demonstrate that our method significantly improves accuracy for several recent detection approaches.},
  archivePrefix = {arXiv},
  eprint = {1904.10097},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DirectShape-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/Y27WE5IW/1904.html},
  journal = {arXiv:1904.10097 [cs]},
  primaryClass = {cs}
}

@article{wangDynamicGraphCNN2018,
  title = {Dynamic {{Graph CNN}} for {{Learning}} on {{Point Clouds}}},
  author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  year = {2018},
  month = jan,
  abstract = {Point clouds provide a flexible and scalable geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. Hence, the design of intelligent computational models that act directly on point clouds is critical, especially when efficiency considerations or noise preclude the possibility of expensive denoising and meshing procedures. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv is differentiable and can be plugged into existing architectures. Compared to existing modules operating largely in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked or recurrently applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. Beyond proposing this module, we provide extensive evaluation and analysis revealing that EdgeConv captures and exploits fine-grained geometric properties of point clouds. The proposed approach achieves state-of-the-art performance on standard benchmarks including ModelNet40 and S3DIS.},
  archivePrefix = {arXiv},
  eprint = {1801.07829},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dynamic Graph CNN for Learning on Point Clouds-Wang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/I3A84A2H/1801.html},
  journal = {arXiv:1801.07829 [cs]},
  keywords = {graph convolution},
  primaryClass = {cs}
}

@article{wangFastOnlineObject2018,
  title = {Fast {{Online Object Tracking}} and {{Segmentation}}: {{A Unifying Approach}}},
  shorttitle = {Fast {{Online Object Tracking}} and {{Segmentation}}},
  author = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H. S.},
  year = {2018},
  month = dec,
  abstract = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/\textasciitilde qwang/SiamMask.},
  archivePrefix = {arXiv},
  eprint = {1812.05050},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fast Online Object Tracking and Segmentation-Wang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/WK98HA6G/Wang et al. - 2018 - Fast Online Object Tracking and Segmentation A Un.pdf;/Users/sunjiaming/Zotero/storage/SCX953LM/1812.html},
  journal = {arXiv:1812.05050 [cs]},
  primaryClass = {cs}
}

@article{wangFrustratinglySimpleFewShot2020,
  title = {Frustratingly {{Simple Few}}-{{Shot Object Detection}}},
  author = {Wang, Xin and Huang, Thomas E. and Darrell, Trevor and Gonzalez, Joseph E. and Yu, Fisher},
  year = {2020},
  month = mar,
  abstract = {Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2\textasciitilde 20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.},
  archivePrefix = {arXiv},
  eprint = {2003.06957},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Frustratingly Simple Few-Shot Object Detection-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/H5MD5TQN/2003.html},
  journal = {arXiv:2003.06957 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{wangFrustumConvNetSliding2019,
  title = {Frustum {{ConvNet}}: {{Sliding Frustums}} to {{Aggregate Local Point}}-{{Wise Features}} for {{Amodal 3D Object Detection}}},
  shorttitle = {Frustum {{ConvNet}}},
  author = {Wang, Zhixin and Jia, Kui},
  year = {2019},
  month = mar,
  abstract = {In this work, we propose a novel method termed Frustum ConvNet (F-ConvNet) for amodal 3D object detection from point clouds. Given 2D region proposals in a RGB image, our method first generates a sequence of frustums for each region proposal, and uses the obtained frustums to group local points. F-ConvNet aggregates point-wise features as frustumlevel feature vectors, and arrays these feature vectors as a feature map for use of its subsequent component of fully convolutional network (FCN), which spatially fuses frustumlevel features and supports an end-to-end and continuous estimation of oriented boxes in the 3D space. We also propose component variants of L-ConvNet, including a FCN variant that extracts multi-resolution frustum features, and a refined use of L-ConvNet over a reduced 3D space. Careful ablation studies verify the efficacy of these component variants. LConvNet assumes no prior knowledge of the working 3D environment, and is thus dataset-agnostic. We present experiments on both the indoor SUN-RGBD and outdoor KITTI datasets. LConvNet outperforms all existing methods on SUN-RGBD, and at the time of submission it outperforms all published works on the KITTI benchmark. We will make the code of L-ConvNet publicly available.},
  archivePrefix = {arXiv},
  eprint = {1903.01864},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Frustum ConvNet-Wang_Jia-2019.pdf;/Users/sunjiaming/Zotero/storage/R5L8QYNY/1903.html},
  journal = {arXiv:1903.01864 [cs]},
  keywords = {3d detection},
  primaryClass = {cs}
}

@article{wangFusingBirdView2017,
  title = {Fusing {{Bird View LIDAR Point Cloud}} and {{Front View Camera Image}} for {{Deep Object Detection}}},
  author = {Wang, Zining and Zhan, Wei and Tomizuka, Masayoshi},
  year = {2017},
  month = nov,
  abstract = {We propose a new method for fusing LIDAR point cloud and camera-captured images in deep convolutional neural networks (CNN). The proposed method constructs a new layer called sparse non-homogeneous pooling layer to transform features between bird's eye view and front view. The sparse point cloud is used to construct the mapping between the two views. The pooling layer allows efficient fusion of the multi-view features at any stage of the network. This is favorable for 3D object detection using camera-LIDAR fusion for autonomous driving. A corresponding one-stage detector is designed and tested on the KITTI bird's eye view object detection dataset, which produces 3D bounding boxes from the bird's eye view map. The fusion method shows significant improvement on both speed and accuracy of the pedestrian detection over other fusion-based object detection networks.},
  archivePrefix = {arXiv},
  eprint = {1711.06703},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Fusing Bird View LIDAR Point Cloud and Front View Camera Image for Deep Object-Wang et al-2017.pdf},
  journal = {arXiv:1711.06703 [cs]},
  keywords = {3d detection},
  language = {en},
  primaryClass = {cs}
}

@article{wangGeneralizingMonocular3D2019,
  title = {Generalizing {{Monocular 3D Human Pose Estimation}} in the {{Wild}}},
  author = {Wang, Luyang and Chen, Yan and Guo, Zhenhua and Qian, Keyuan and Lin, Mude and Li, Hongsheng and Ren, Jimmy S.},
  year = {2019},
  month = apr,
  abstract = {The availability of the large-scale labeled 3D poses in the Human3.6M dataset plays an important role in advancing the algorithms for 3D human pose estimation from a still image. We observe that recent innovation in this area mainly focuses on new techniques that explicitly address the generalization issue when using this dataset, because this database is constructed in a highly controlled environment with limited human subjects and background variations. Despite such efforts, we can show that the results of the current methods are still error-prone especially when tested against the images taken in-the-wild. In this paper, we aim to tackle this problem from a different perspective. We propose a principled approach to generate high quality 3D pose ground truth given any in-the-wild image with a person inside. We achieve this by first devising a novel stereo inspired neural network to directly map any 2D pose to high quality 3D counterpart. We then perform a carefully designed geometric searching scheme to further refine the joints. Based on this scheme, we build a large-scale dataset with 400,000 in-the-wild images and their corresponding 3D pose ground truth. This enables the training of a high quality neural network model, without specialized training scheme and auxiliary loss function, which performs favorably against the state-of-the-art 3D pose estimation methods. We also evaluate the generalization ability of our model both quantitatively and qualitatively. Results show that our approach convincingly outperforms the previous methods. We make our dataset and code publicly available.},
  archivePrefix = {arXiv},
  eprint = {1904.05512},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Generalizing Monocular 3D Human Pose Estimation in the Wild-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MJGF5PTE/1904.html},
  journal = {arXiv:1904.05512 [cs]},
  primaryClass = {cs}
}

@article{wangLearningCorrespondenceCycleConsistency2019,
  title = {Learning {{Correspondence}} from the {{Cycle}}-{{Consistency}} of {{Time}}},
  author = {Wang, Xiaolong and Jabri, Allan and Efros, Alexei A.},
  year = {2019},
  month = mar,
  abstract = {We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods.},
  archivePrefix = {arXiv},
  eprint = {1903.07593},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Correspondence from the Cycle-Consistency of Time-Wang et al-2019.pdf},
  journal = {arXiv:1903.07593 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{wangLearningDepthMonocular2017,
  title = {Learning {{Depth}} from {{Monocular Videos}} Using {{Direct Methods}}},
  author = {Wang, Chaoyang and Buenaposada, Jose Miguel and Zhu, Rui and Lucey, Simon},
  year = {2017},
  month = nov,
  abstract = {The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.},
  archivePrefix = {arXiv},
  eprint = {1712.00175},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Depth from Monocular Videos using Direct Methods-Wang et al-2017.pdf},
  journal = {arXiv:1712.00175 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{wangLearningDepthMonocular2018,
  title = {Learning {{Depth}} from {{Monocular Videos Using Direct Methods}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Chaoyang and Buenaposada, Jose Miguel and Zhu, Rui and Lucey, Simon},
  year = {2018},
  month = jun,
  pages = {2022--2030},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00216},
  abstract = {The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.},
  file = {/Users/sunjiaming/Zotero/storage/2HRLMCGS/Wang et al. - 2018 - Learning Depth from Monocular Videos Using Direct .pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{wangLearningFeatureDescriptors2020,
  title = {Learning {{Feature Descriptors}} Using {{Camera Pose Supervision}}},
  author = {Wang, Qianqian and Zhou, Xiaowei and Hariharan, Bharath and Snavely, Noah},
  year = {2020},
  month = apr,
  abstract = {Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. Though trained with weak supervision, our learned descriptors outperform even prior fully-supervised methods and achieve state-of-the-art performance on a variety of geometric tasks.},
  archivePrefix = {arXiv},
  eprint = {2004.13324},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Feature Descriptors using Camera Pose Supervision-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MULHJ3KZ/2004.html},
  journal = {arXiv:2004.13324 [cs]},
  primaryClass = {cs}
}

@article{wangMVDepthNetRealtimeMultiview2018,
  title = {{{MVDepthNet}}: {{Real}}-Time {{Multiview Depth Estimation Neural Network}}},
  shorttitle = {{{MVDepthNet}}},
  author = {Wang, Kaixuan and Shen, Shaojie},
  year = {2018},
  month = jul,
  abstract = {Although deep neural networks have been widely applied to computer vision problems, extending them into multiview depth estimation is non-trivial. In this paper, we present MVDepthNet, a convolutional network to solve the depth estimation problem given several image-pose pairs from a localized monocular camera in neighbor viewpoints. Multiview observations are encoded in a cost volume and then combined with the reference image to estimate the depth map using an encoder-decoder network. By encoding the information from multiview observations into the cost volume, our method achieves real-time performance and the flexibility of traditional methods that can be applied regardless of the camera intrinsic parameters and the number of images. Geometric data augmentation is used to train MVDepthNet. We further apply MVDepthNet in a monocular dense mapping system that continuously estimates depth maps using a single localized moving camera. Experiments show that our method can generate depth maps efficiently and precisely.},
  archivePrefix = {arXiv},
  eprint = {1807.08563},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MVDepthNet-Wang_Shen-2018.pdf;/Users/sunjiaming/Zotero/storage/GDVWZJM4/1807.html},
  journal = {arXiv:1807.08563 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{wangNormalizedObjectCoordinate2019,
  title = {Normalized {{Object Coordinate Space}} for {{Category}}-{{Level 6D Object Pose}} and {{Size Estimation}}},
  author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
  year = {2019},
  month = jan,
  abstract = {The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to ``instance-level'' 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time.},
  archivePrefix = {arXiv},
  eprint = {1901.02970},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/UGTIXLXA/Wang et al. - 2019 - Normalized Object Coordinate Space for Category-Le.pdf;/Users/sunjiaming/Zotero/storage/JU99CSHG/1901.html},
  journal = {arXiv:1901.02970 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{wangObjectInstanceAnnotation,
  title = {Object {{Instance Annotation}} with {{Deep Extreme Level Set Evolution}}},
  author = {Wang, Zian and Acuna, David and Ling, Huan and Kar, Amlan and Fidler, Sanja},
  pages = {9},
  abstract = {In this paper, we tackle the task of interactive object segmentation. We revive the old ideas on level set segmentation which framed object annotation as curve evolution. Carefully designed energy functions ensured that the curve was well aligned with image boundaries, and generally ``well behaved". The Level Set Method can handle objects with complex shapes and topological changes such as merging and splitting, thus able to deal with occluded objects and objects with holes. We propose Deep Extreme Level Set Evolution that combines powerful CNN models with level set optimization in an end-to-end fashion. Our method learns to predict evolution parameters conditioned on the image and evolves the predicted initial contour to produce the final result. We make our model interactive by incorporating user clicks on the extreme boundary points, following DEXTR [29]. We show that our approach significantly outperforms DEXTR on the static Cityscapes dataset [16] and the video segmentation benchmark DAVIS [34], and performs on par on PASCAL [19] and SBD [20].},
  file = {/Users/sunjiaming/Zotero/storage/TQXRYWDQ/Wang et al. - Object Instance Annotation with Deep Extreme Level.pdf},
  language = {en}
}

@article{wangPillarbasedObjectDetection2020,
  title = {Pillar-Based {{Object Detection}} for {{Autonomous Driving}}},
  author = {Wang, Yue and Fathi, Alireza and Kundu, Abhijit and Ross, David and Pantofaru, Caroline and Funkhouser, Tom and Solomon, Justin},
  year = {2020},
  month = jul,
  abstract = {We present a simple and flexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to fix the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multi-view feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the final prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while significantly improving upon state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {2007.10323},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pillar-based Object Detection for Autonomous Driving-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/54QZNIV6/2007.html},
  journal = {arXiv:2007.10323 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{wangPillarbasedObjectDetection2020a,
  title = {Pillar-Based {{Object Detection}} for {{Autonomous Driving}}},
  author = {Wang, Yue and Fathi, Alireza and Kundu, Abhijit and Ross, David and Pantofaru, Caroline and Funkhouser, Thomas and Solomon, Justin},
  year = {2020},
  month = jul,
  abstract = {We present a simple and flexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to fix the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multi-view feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the final prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while significantly improving upon state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {2007.10323},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pillar-based Object Detection for Autonomous Driving-Wang et al-22.pdf;/Users/sunjiaming/Zotero/storage/KRF4SGCX/2007.html},
  journal = {arXiv:2007.10323 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{wangPixel2MeshGenerating3D2018,
  title = {{{Pixel2Mesh}}: {{Generating 3D Mesh Models}} from {{Single RGB Images}}},
  shorttitle = {{{Pixel2Mesh}}},
  author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
  year = {2018},
  month = aug,
  abstract = {We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1804.01654},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pixel2Mesh-Wang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/VB8KFHWU/1804.html},
  journal = {arXiv:1804.01654 [cs]},
  primaryClass = {cs}
}

@article{wangPRNetSelfSupervisedLearning2019,
  title = {{{PRNet}}: {{Self}}-{{Supervised Learning}} for {{Partial}}-to-{{Partial Registration}}},
  shorttitle = {{{PRNet}}},
  author = {Wang, Yue and Solomon, Justin M.},
  year = {2019},
  month = oct,
  abstract = {We present a simple, flexible, and general framework titled Partial Registration Network (PRNet), for partial-to-partial point cloud registration. Inspired by recently-proposed learning-based methods for registration, we use deep networks to tackle non-convexity of the alignment and partial correspondence problems. While previous learning-based methods assume the entire shape is visible, PRNet is suitable for partial-to-partial registration, outperforming PointNetLK, DCP, and non-learning methods on synthetic data. PRNet is self-supervised, jointly learning an appropriate geometric representation, a keypoint detector that finds points in common between partial views, and keypoint-to-keypoint correspondences. We show PRNet predicts keypoints and correspondences consistently across views and objects. Furthermore, the learned representation is transferable to classification.},
  archivePrefix = {arXiv},
  eprint = {1910.12240},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PRNet-Wang_Solomon-2019.pdf;/Users/sunjiaming/Zotero/storage/EAPMJI2F/1910.html},
  journal = {arXiv:1910.12240 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wangPseudoLiDARVisualDepth2018,
  title = {Pseudo-{{LiDAR}} from {{Visual Depth Estimation}}: {{Bridging}} the {{Gap}} in {{3D Object Detection}} for {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR}} from {{Visual Depth Estimation}}},
  author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  year = {2018},
  month = dec,
  abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that data representation (rather than its quality) accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within 30m range from the previous state-of-the-art of 22\% to an unprecedented 74\%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo image based approaches.},
  archivePrefix = {arXiv},
  eprint = {1812.07179},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Pseudo-LiDAR from Visual Depth Estimation-Wang et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Pseudo-LiDAR from Visual Depth Estimation-Wang et al-22.pdf},
  journal = {arXiv:1812.07179 [cs]},
  keywords = {3d detection},
  language = {en},
  primaryClass = {cs}
}

@article{wangSelf6DSelfSupervisedMonocular2020,
  title = {{{Self6D}}: {{Self}}-{{Supervised Monocular 6D Object Pose Estimation}}},
  shorttitle = {{{Self6D}}},
  author = {Wang, Gu and Manhardt, Fabian and Shao, Jianzhun and Ji, Xiangyang and Navab, Nassir and Tombari, Federico},
  year = {2020},
  month = apr,
  abstract = {Estimating the 6D object pose is a fundamental problem in computer vision. Convolutional Neural Networks (CNNs) have recently proven to be capable of predicting reliable 6D pose estimates even from monocular images. Nonetheless, CNNs are identified as being extremely data-driven, yet, acquiring adequate annotations is oftentimes very time-consuming and labor intensive. To overcome this shortcoming, we propose the idea of monocular 6D pose estimation by means of self-supervised learning, which eradicates the need for real data with annotations. After training our proposed network fully supervised with synthetic RGB data, we leverage recent advances in neural rendering to further self-supervise the model on unannotated real RGB-D data, seeking for a visually and geometrically optimal alignment. Extensive evaluations demonstrate that our proposed self-supervision is able to significantly enhance the model's original performance, outperforming all other methods relying on synthetic data or employing elaborate techniques from the domain adaptation realm.},
  archivePrefix = {arXiv},
  eprint = {2004.06468},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Self6D-Wang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/3Z3FCCNI/2004.html},
  journal = {arXiv:2004.06468 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{wangSGPNSimilarityGroup2017,
  title = {{{SGPN}}: {{Similarity Group Proposal Network}} for {{3D Point Cloud Instance Segmentation}}},
  shorttitle = {{{SGPN}}},
  author = {Wang, Weiyue and Yu, Ronald and Huang, Qiangui and Neumann, Ulrich},
  year = {2017},
  month = nov,
  abstract = {We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point. To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.},
  archivePrefix = {arXiv},
  eprint = {1711.08588},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Wang et al_2017_SGPN.pdf;/Users/sunjiaming/Zotero/storage/FDF9SI78/1711.html},
  journal = {arXiv:1711.08588 [cs]},
  primaryClass = {cs}
}

@article{wangSteklovSpectralGeometry2018,
  title = {Steklov {{Spectral Geometry}} for {{Extrinsic Shape Analysis}}},
  author = {Wang, Yu and {Ben-Chen}, Mirela and Polterovich, Iosif and Solomon, Justin},
  year = {2018},
  month = dec,
  volume = {38},
  pages = {7:1--7:21},
  issn = {0730-0301},
  doi = {10.1145/3152156},
  abstract = {We propose using the Dirichlet-to-Neumann operator as an extrinsic alternative to the Laplacian for spectral geometry processing and shape analysis. Intrinsic approaches, usually based on the Laplace\textendash Beltrami operator, cannot capture the spatial embedding of a shape up to rigid motion, and many previous extrinsic methods lack theoretical justification. Instead, we consider the Steklov eigenvalue problem, computing the spectrum of the Dirichlet-to-Neumann operator of a surface bounding a volume. A remarkable property of this operator is that it completely encodes volumetric geometry. We use the boundary element method (BEM) to discretize the operator, accelerated by hierarchical numerical schemes and preconditioning; this pipeline allows us to solve eigenvalue and linear problems on large-scale meshes despite the density of the Dirichlet-to-Neumann discretization. We further demonstrate that our operators naturally fit into existing frameworks for geometry processing, making a shift from intrinsic to extrinsic geometry as simple as substituting the Laplace\textendash Beltrami operator with the Dirichlet-to-Neumann operator.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Steklov Spectral Geometry for Extrinsic Shape Analysis-Wang et al-2018.pdf},
  journal = {ACM Trans. Graph.},
  number = {1}
}

@article{wangWebStereoVideo2019,
  title = {Web {{Stereo Video Supervision}} for {{Depth Prediction}} from {{Dynamic Scenes}}},
  author = {Wang, Chaoyang and Lucey, Simon and Perazzi, Federico and Wang, Oliver},
  year = {2019},
  month = apr,
  abstract = {We present a fully data-driven method to compute depth from diverse monocular video sequences that contain large amounts of non-rigid objects, e.g., people. In order to learn reconstruction cues for non-rigid scenes, we introduce a new dataset consisting of stereo videos scraped in-the-wild. This dataset has a wide variety of scene types, and features large amounts of nonrigid objects, especially people. From this, we compute disparity maps to be used as supervision to train our approach. We propose a loss function that allows us to generate a depth prediction even with unknown camera intrinsics and stereo baselines in the dataset. We validate the use of large amounts of Internet video by evaluating our method on existing video datasets with depth supervision, including SINTEL, and KITTI, and show that our approach generalizes better to natural scenes.},
  archivePrefix = {arXiv},
  eprint = {1904.11112},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2UARKZGW/1904.html},
  journal = {arXiv:1904.11112 [cs]},
  primaryClass = {cs}
}

@article{wangWhatMakesTraining2019,
  title = {What {{Makes Training Multi}}-{{Modal Networks Hard}}?},
  author = {Wang, Weiyao and Tran, Du and Feiszli, Matt},
  year = {2019},
  month = may,
  abstract = {Consider end-to-end training of a multi-modal vs. a single-modal network on a task with multiple input modalities: the multi-modal network receives more information, so it should match or outperform its single-modal counterpart. In our experiments, however, we observe the opposite: the best single-modal network always outperforms the multi-modal network. This observation is consistent across different combinations of modalities and on different tasks and benchmarks. This paper identifies two main causes for this performance drop: first, multi-modal networks are often prone to overfitting due to increased capacity. Second, different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is sub-optimal. We address these two problems with a technique we call Gradient Blending, which computes an optimal blend of modalities based on their overfitting behavior. We demonstrate that Gradient Blending outperforms widely-used baselines for avoiding overfitting and achieves state-of-the-art accuracy on various tasks including fine-grained sport classification, human action recognition, and acoustic event detection.},
  archivePrefix = {arXiv},
  eprint = {1905.12681},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/What Makes Training Multi-Modal Networks Hard-Wang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/NBQCJMC3/1905.html},
  journal = {arXiv:1905.12681 [cs]},
  primaryClass = {cs}
}

@article{watsonLearningStereoSingle2020,
  title = {Learning {{Stereo}} from {{Single Images}}},
  author = {Watson, Jamie and Mac Aodha, Oisin and Turmukhambetov, Daniyar and Brostow, Gabriel J. and Firman, Michael},
  year = {2020},
  month = aug,
  abstract = {Supervised deep networks are among the best methods for finding correspondences in stereo image pairs. Like all supervised approaches, these networks require ground truth data during training. However, collecting large quantities of accurate dense correspondence data is very challenging. We propose that it is unnecessary to have such a high reliance on ground truth depths or even corresponding stereo pairs. Inspired by recent progress in monocular depth estimation, we generate plausible disparity maps from single images. In turn, we use those flawed disparity maps in a carefully designed pipeline to generate stereo training pairs. Training in this manner makes it possible to convert any collection of single RGB images into stereo training data. This results in a significant reduction in human effort, with no need to collect real depths or to hand-design synthetic data. We can consequently train a stereo matching network from scratch on datasets like COCO, which were previously hard to exploit for stereo. Through extensive experiments we show that our approach outperforms stereo networks trained with standard synthetic datasets, when evaluated on KITTI, ETH3D, and Middlebury.},
  archivePrefix = {arXiv},
  eprint = {2008.01484},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Stereo from Single Images-Watson et al-2020.pdf;/Users/sunjiaming/Zotero/storage/8ZQFPZF7/2008.html},
  journal = {arXiv:2008.01484 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{wederRoutedFusionLearningRealtime2020,
  title = {{{RoutedFusion}}: {{Learning Real}}-Time {{Depth Map Fusion}}},
  shorttitle = {{{RoutedFusion}}},
  author = {Weder, Silvan and Sch{\"o}nberger, Johannes and Pollefeys, Marc and Oswald, Martin R.},
  year = {2020},
  month = jan,
  abstract = {The efficient fusion of depth maps is a key part of most state-of-the-art 3D reconstruction methods. Besides requiring high accuracy, these depth fusion methods need to be scalable and real-time capable. To this end, we present a novel real-time capable machine learning-based method for depth map fusion. Similar to the seminal depth map fusion approach by Curless and Levoy, we only update a local group of voxels to ensure real-time capability. Instead of a simple linear fusion of depth information, we propose a neural network that predicts non-linear updates to better account for typical fusion errors. Our network is composed of a 2D depth routing network and a 3D depth fusion network which efficiently handle sensor-specific noise and outliers. This is especially useful for surface edges and thin objects for which the original approach suffers from thickening artifacts. Our method outperforms the traditional fusion approach and related learned approaches on both synthetic and real data. We demonstrate the performance of our method in reconstructing fine geometric details from noise and outlier contaminated data on various scenes},
  archivePrefix = {arXiv},
  eprint = {2001.04388},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RoutedFusion-Weder et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EY49XKTU/2001.html},
  journal = {arXiv:2001.04388 [cs]},
  keywords = {neufu_paper},
  primaryClass = {cs}
}

@article{weiDeepSFMStructureMotion2019,
  title = {{{DeepSFM}}: {{Structure From Motion Via Deep Bundle Adjustment}}},
  shorttitle = {{{DeepSFM}}},
  author = {Wei, Xingkui and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Xue, Xiangyang},
  year = {2019},
  month = dec,
  abstract = {Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network.In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both.In each cost volume, we encode not only photo-metric consistency across multiple input images, but also geometric consistency to ensure that depths from multiple views agree with each other.The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology.Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.},
  archivePrefix = {arXiv},
  eprint = {1912.09697},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepSFM-Wei et al-2019.pdf;/Users/sunjiaming/Zotero/storage/ZGT2HFB5/1912.html},
  journal = {arXiv:1912.09697 [cs]},
  primaryClass = {cs}
}

@article{weiDenseHumanBody2015,
  title = {Dense {{Human Body Correspondences Using Convolutional Networks}}},
  author = {Wei, Lingyu and Huang, Qixing and Ceylan, Duygu and Vouga, Etienne and Li, Hao},
  year = {2015},
  month = nov,
  abstract = {We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods that require full watertight 3D geometry.},
  archivePrefix = {arXiv},
  eprint = {1511.05904},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Dense Human Body Correspondences Using Convolutional Networks-Wei et al-2015.pdf},
  journal = {arXiv:1511.05904 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{weiLearningInferSemantic2020,
  title = {Learning to {{Infer Semantic Parameters}} for {{3D Shape Editing}}},
  author = {Wei, Fangyin and Sizikova, Elena and Sud, Avneesh and Rusinkiewicz, Szymon and Funkhouser, Thomas},
  year = {2020},
  month = nov,
  abstract = {Many applications in 3D shape design and augmentation require the ability to make specific edits to an object's semantic parameters (e.g., the pose of a person's arm or the length of an airplane's wing) while preserving as much existing details as possible. We propose to learn a deep network that infers the semantic parameters of an input shape and then allows the user to manipulate those parameters. The network is trained jointly on shapes from an auxiliary synthetic template and unlabeled realistic models, ensuring robustness to shape variability while relieving the need to label realistic exemplars. At testing time, edits within the parameter space drive deformations to be applied to the original shape, which provides semantically-meaningful manipulation while preserving the details. This is in contrast to prior methods that either use autoencoders with a limited latent-space dimensionality, failing to preserve arbitrary detail, or drive deformations with purely-geometric controls, such as cages, losing the ability to update local part regions. Experiments with datasets of chairs, airplanes, and human bodies demonstrate that our method produces more natural edits than prior work.},
  archivePrefix = {arXiv},
  eprint = {2011.04755},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Infer Semantic Parameters for 3D Shape Editing-Wei et al-2020.pdf;/Users/sunjiaming/Zotero/storage/7SAWT8LD/2011.html},
  journal = {arXiv:2011.04755 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{weiler3DSteerableCNNs,
  title = {{{3D Steerable CNNs}}: {{Learning Rotationally Equivariant Features}} in {{Volumetric Data}}},
  author = {Weiler, Maurice and Boomsma, Wouter and Geiger, Mario and Welling, Max and Cohen, Taco},
  pages = {12},
  abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/3D Steerable CNNs-Weiler et al-.pdf},
  language = {en}
}

@article{weinzaepfelVisualLocalizationLearning,
  title = {Visual {{Localization}} by {{Learning Objects}}-of-{{Interest Dense Match Regression}}},
  author = {Weinzaepfel, Philippe and Csurka, Gabriela and Cabon, Yohann and Humenberger, Martin},
  pages = {10},
  abstract = {We introduce a novel CNN-based approach for visual localization from a single RGB image that relies on densely matching a set of Objects-of-Interest (OOIs). In this paper, we focus on planar objects which are highly descriptive in an environment, such as paintings in museums or logos and storefronts in malls or airports. For each OOI, we define a reference image for which 3D world coordinates are available. Given a query image, our CNN model detects the OOIs, segments them and finds a dense set of 2D-2D matches between each detected OOI and its corresponding reference image. Given these 2D-2D matches, together with the 3D world coordinates of each reference image, we obtain a set of 2D-3D matches from which solving a Perspective-n-Point problem gives a pose estimate. We show that 2D-3D matches for reference images, as well as OOI annotations can be obtained for all training images from a single instance annotation per OOI by leveraging Structure-from-Motion reconstruction. We introduce a novel synthetic dataset, VirtualGallery, which targets challenges such as varying lighting conditions and different occlusion levels. Our results show that our method achieves high precision and is robust to these challenges. We also experiment using the Baidu localization dataset captured in a shopping mall. Our approach is the first deep regressionbased method to scale to such a larger environment.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual Localization by Learning Objects-of-Interest Dense Match Regression-Weinzaepfel et al-2.pdf},
  language = {en}
}

@article{weinzaepfelVisualLocalizationLearninga,
  title = {Visual {{Localization}} by {{Learning Objects}}-of-{{Interest Dense Match Regression}}},
  author = {Weinzaepfel, Philippe and Csurka, Gabriela and Cabon, Yohann and Humenberger, Martin},
  pages = {10},
  abstract = {We introduce a novel CNN-based approach for visual localization from a single RGB image that relies on densely matching a set of Objects-of-Interest (OOIs). In this paper, we focus on planar objects which are highly descriptive in an environment, such as paintings in museums or logos and storefronts in malls or airports. For each OOI, we define a reference image for which 3D world coordinates are available. Given a query image, our CNN model detects the OOIs, segments them and finds a dense set of 2D-2D matches between each detected OOI and its corresponding reference image. Given these 2D-2D matches, together with the 3D world coordinates of each reference image, we obtain a set of 2D-3D matches from which solving a Perspective-n-Point problem gives a pose estimate. We show that 2D-3D matches for reference images, as well as OOI annotations can be obtained for all training images from a single instance annotation per OOI by leveraging Structure-from-Motion reconstruction. We introduce a novel synthetic dataset, VirtualGallery, which targets challenges such as varying lighting conditions and different occlusion levels. Our results show that our method achieves high precision and is robust to these challenges. We also experiment using the Baidu localization dataset captured in a shopping mall. Our approach is the first deep regressionbased method to scale to such a larger environment.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Visual Localization by Learning Objects-of-Interest Dense Match Regression-Weinzaepfel et al-.pdf},
  language = {en}
}

@article{wengBaseline3DMultiObject2019,
  title = {A {{Baseline}} for {{3D Multi}}-{{Object Tracking}}},
  author = {Weng, Xinshuo and Kitani, Kris},
  year = {2019},
  month = jul,
  abstract = {3D multi-object tracking (MOT) is an essential component technology for many real-time applications such as autonomous driving or assistive robotics. However, recent works for 3D MOT tend to focus more on developing accurate systems giving less regard to computational cost and system complexity. In contrast, this work proposes a simple yet accurate real-time baseline 3D MOT system. We use an off-the-shelf 3D object detector to obtain oriented 3D bounding boxes from the LiDAR point cloud. Then, a combination of 3D Kalman filter and Hungarian algorithm is used for state estimation and data association. Although our baseline system is a straightforward combination of standard methods, we obtain the state-of-the-art results. To evaluate our baseline system, we propose a new 3D MOT extension to the official KITTI 2D MOT evaluation along with two new metrics. Our proposed baseline method for 3D MOT establishes new state-of-the-art performance on 3D MOT for KITTI, improving the 3D MOTA from 72.23 of prior art to 76.47. Surprisingly, by projecting our 3D tracking results to the 2D image plane and compare against published 2D MOT methods, our system places 2nd on the official KITTI leaderboard. Also, our proposed 3D MOT method runs at a rate of 214.7 FPS, 65 times faster than the state-of-the-art 2D MOT system. Our code is publicly available at https://github.com/xinshuoweng/AB3DMOT},
  archivePrefix = {arXiv},
  eprint = {1907.03961},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/A Baseline for 3D Multi-Object Tracking-Weng_Kitani-22.pdf;/Users/sunjiaming/Zotero/storage/IYWWXVRE/1907.html},
  journal = {arXiv:1907.03961 [cs]},
  primaryClass = {cs}
}

@article{wengJoint3DTracking2020,
  title = {Joint {{3D Tracking}} and {{Forecasting}} with {{Graph Neural Network}} and {{Diversity Sampling}}},
  author = {Weng, Xinshuo and Yuan, Ye and Kitani, Kris},
  year = {2020},
  month = mar,
  abstract = {3D multi-object tracking (MOT) and trajectory forecasting are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneficial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. To evaluate this hypothesis, we propose a unified solution for 3D MOT and trajectory forecasting which also incorporates two additional novel computational units. First, we propose a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which multiple agents interact with one another. The GNN is able to model complex hierarchical interactions, improve the discriminative feature learning for MOT association, and provide socially-aware context for trajectory forecasting. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating many duplicate trajectory samples. We evaluate on the KITTI and nuScenes datasets, showing that our unified method with feature interaction and diversity sampling achieves new state-of-the-art performance on both 3D MOT and trajectory forecasting. Our code will be made available at https://github.com/xinshuoweng/GNNTrkForecast.},
  archivePrefix = {arXiv},
  eprint = {2003.07847},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint 3D Tracking and Forecasting with Graph Neural Network and Diversity-Weng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EBNRFR7C/2003.html},
  journal = {arXiv:2003.07847 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{wenPixel2MeshMultiView3D2019,
  title = {{{Pixel2Mesh}}++: {{Multi}}-{{View 3D Mesh Generation}} via {{Deformation}}},
  shorttitle = {{{Pixel2Mesh}}++},
  author = {Wen, Chao and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei},
  year = {2019},
  month = aug,
  abstract = {We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.},
  archivePrefix = {arXiv},
  eprint = {1908.01491},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pixel2Mesh++-Wen et al-2019.pdf;/Users/sunjiaming/Zotero/storage/GEQ3LDYG/1908.html},
  journal = {arXiv:1908.01491 [cs]},
  primaryClass = {cs}
}

@article{wenzel4SeasonsCrossSeasonDataset2020,
  title = {{{4Seasons}}: {{A Cross}}-{{Season Dataset}} for {{Multi}}-{{Weather SLAM}} in {{Autonomous Driving}}},
  shorttitle = {{{4Seasons}}},
  author = {Wenzel, Patrick and Wang, Rui and Yang, Nan and Cheng, Qing and Khan, Qadeer and {von Stumberg}, Lukas and Zeller, Niclas and Cremers, Daniel},
  year = {2020},
  month = sep,
  abstract = {We present a novel dataset covering seasonal and challenging perceptual conditions for autonomous driving. Among others, it enables research on visual odometry, global place recognition, and map-based re-localization tracking. The data was collected in different scenarios and under a wide variety of weather conditions and illuminations, including day and night. This resulted in more than 350 km of recordings in nine different environments ranging from multi-level parking garage over urban (including tunnels) to countryside and highway. We provide globally consistent reference poses with up-to centimeter accuracy obtained from the fusion of direct stereo visual-inertial odometry with RTK-GNSS. The full dataset is available at www.4seasons-dataset.com.},
  archivePrefix = {arXiv},
  eprint = {2009.06364},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/4Seasons-Wenzel et al-2020.pdf;/Users/sunjiaming/Zotero/storage/QLXTU73W/2009.html},
  journal = {arXiv:2009.06364 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{whelanElasticFusionRealtimeDense2016,
  title = {{{ElasticFusion}}: {{Real}}-Time Dense {{SLAM}} and Light Source Estimation},
  shorttitle = {{{ElasticFusion}}},
  author = {Whelan, Thomas and {Salas-Moreno}, Renato F and Glocker, Ben and Davison, Andrew J and Leutenegger, Stefan},
  year = {2016},
  month = dec,
  volume = {35},
  pages = {1697--1716},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364916669237},
  abstract = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments and beyond explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. This is accomplished by using dense frame-tomodel camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency. In the spirit of improving map quality as well as tracking accuracy and robustness, we furthermore explore a novel approach to real-time discrete light source detection. This technique is capable of detecting numerous light sources in indoor environments in real-time as a user handheld camera explores the scene. Absolutely no prior information about the scene or number of light sources is required. By making a small set of simple assumptions about the appearance properties of the scene our method can incrementally estimate both the quantity and location of multiple light sources in the environment in an online fashion. Our results demonstrate that our technique functions well in many different environments and lighting configurations. We show that this enables (a) more realistic augmented reality (AR) rendering; (b) a richer understanding of the scene beyond pure geometry and; (c) more accurate and robust photometric tracking.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ElasticFusion-Whelan et al-2016.pdf},
  journal = {The International Journal of Robotics Research},
  keywords = {reconstruction},
  language = {en},
  number = {14}
}

@article{whelanRealtimeLargescaleDense2015,
  title = {Real-Time Large-Scale Dense {{RGB}}-{{D SLAM}} with Volumetric Fusion},
  author = {Whelan, Thomas and Kaess, Michael and Johannsson, Hordur and Fallon, Maurice and Leonard, John J. and McDonald, John},
  year = {2015},
  month = apr,
  volume = {34},
  pages = {598--626},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364914551008},
  abstract = {We present a new SLAM system capable of producing high quality globally consistent surface reconstructions over hundreds of metres in real-time with only a low-cost commodity RGB-D sensor. By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds. In this paper we highlight three key techniques associated with applying a volumetric fusion-based mapping system to the SLAM problem in real-time. First, the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every frame volumetric fusion of depth maps to function over an unbounded spatial region. Second, overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric and photometric camera pose constraints. Third, efficiently updating the dense map according to place recognition and subsequent loop closure constraints by the use of an ``as-rigid-as-possible'' space deformation. We present results on a wide variety of aspects of the system and show through evaluation on de facto standard RGB-D benchmarks that our system performs strongly in terms of trajectory estimation, map quality and computational performance in comparison to other state-of-the-art systems.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Real-time large-scale dense RGB-D SLAM with volumetric fusion-Whelan et al-2015.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {4-5}
}

@article{widyaStructurefromMotionUsingDense2018,
  title = {Structure-from-{{Motion}} Using {{Dense CNN Features}} with {{Keypoint Relocalization}}},
  author = {Widya, Aji Resindra and Torii, Akihiko and Okutomi, Masatoshi},
  year = {2018},
  month = may,
  abstract = {Structure from Motion (SfM) using imagery that involves extreme appearance changes is yet a challenging task due to a loss of feature repeatability. Using feature correspondences obtained by matching densely extracted convolutional neural network (CNN) features significantly improves the SfM reconstruction capability. However, the reconstruction accuracy is limited by the spatial resolution of the extracted CNN features which is not even pixel-level accuracy in the existing approach. Providing dense feature matches with precise keypoint positions is not trivial because of memory limitation and computational burden of dense features. To achieve accurate SfM reconstruction with highly repeatable dense features, we propose an SfM pipeline that uses dense CNN features with relocalization of keypoint position that can efficiently and accurately provide pixel-level feature correspondences. Then, we demonstrate on the Aachen Day-Night dataset that the proposed SfM using dense CNN features with the keypoint relocalization outperforms a state-of-the-art SfM (COLMAP using RootSIFT) by a large margin.},
  archivePrefix = {arXiv},
  eprint = {1805.03879},
  eprinttype = {arxiv},
  journal = {arXiv:1805.03879 [cs]},
  primaryClass = {cs}
}

@article{wilesD2DLearningFind2020,
  title = {{{D2D}}: {{Learning}} to Find Good Correspondences for Image Matching and Manipulation},
  shorttitle = {{{D2D}}},
  author = {Wiles, Olivia and Ehrhardt, Sebastien and Zisserman, Andrew},
  year = {2020},
  month = jul,
  abstract = {We propose a new approach to determining correspondences between image pairs under large changes in illumination, viewpoint, context, and material. While most approaches seek to extract a set of reliably detectable regions in each image which are then compared (sparse-to-sparse) using increasingly complicated or specialized pipelines, we propose a simple approach for matching all points between the images (dense-to-dense) and subsequently selecting the best matches. The two key parts of our approach are: (i) to condition the learned features on both images, and (ii) to learn a distinctiveness score which is used to choose the best matches at test time. We demonstrate that our model can be used to achieve state of the art or competitive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization.},
  archivePrefix = {arXiv},
  eprint = {2007.08480},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/D2D-Wiles et al-2020.pdf;/Users/sunjiaming/Zotero/storage/IW9NUVLK/2007.html},
  journal = {arXiv:2007.08480 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{williamsDeepGeometricPrior2019,
  title = {Deep {{Geometric Prior}} for {{Surface Reconstruction}}},
  author = {Williams, Francis and Schneider, Teseo and Silva, Claudio and Zorin, Denis and Bruna, Joan and Panozzo, Daniele},
  year = {2019},
  month = apr,
  abstract = {The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.},
  archivePrefix = {arXiv},
  eprint = {1811.10943},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Geometric Prior for Surface Reconstruction-Williams et al-2019.pdf;/Users/sunjiaming/Zotero/storage/5CXTKAGA/1811.html},
  journal = {arXiv:1811.10943 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{williamsNeuralSplinesFitting2020,
  title = {Neural {{Splines}}: {{Fitting 3D Surfaces}} with {{Infinitely}}-{{Wide Neural Networks}}},
  shorttitle = {Neural {{Splines}}},
  author = {Williams, Francis and Trager, Matthew and Bruna, Joan and Zorin, Denis},
  year = {2020},
  month = jun,
  abstract = {We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming Screened Poisson Surface Reconstruction and modern neural network based techniques. Because our approach is based on a simple kernel formulation, it is fast to run and easy to analyze. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with our kernel biases toward smooth interpolants. Finally, we formulate Screened Poisson Surface Reconstruction as a kernel method and derive an analytic expression for its norm in the corresponding RKHS.},
  archivePrefix = {arXiv},
  eprint = {2006.13782},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Splines-Williams et al-2020.pdf;/Users/sunjiaming/Zotero/storage/KGDV9AF8/2006.html},
  journal = {arXiv:2006.13782 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  primaryClass = {cs}
}

@article{wojkeSimpleOnlineRealtime2017,
  title = {Simple {{Online}} and {{Realtime Tracking}} with a {{Deep Association Metric}}},
  author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  year = {2017},
  month = mar,
  abstract = {Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
  archivePrefix = {arXiv},
  eprint = {1703.07402},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Simple Online and Realtime Tracking with a Deep Association Metric-Wojke et al-2017.pdf;/Users/sunjiaming/Zotero/storage/9EQAGR2N/1703.html},
  journal = {arXiv:1703.07402 [cs]},
  keywords = {2d tracking},
  primaryClass = {cs}
}

@article{wolcottRobustLIDARLocalization2017,
  title = {Robust {{LIDAR}} Localization Using Multiresolution {{Gaussian}} Mixture Maps for Autonomous Driving},
  author = {Wolcott, Ryan W and Eustice, Ryan M},
  year = {2017},
  month = mar,
  volume = {36},
  pages = {292--319},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364917696568},
  abstract = {This paper reports on a fast multiresolution scan matcher for local vehicle localization of self-driving cars. State-ofthe-art approaches to vehicle localization rely on observing road surface reflectivity with a three-dimensional (3D) light detection and ranging (LIDAR) scanner to achieve centimeter-level accuracy. However, these approaches can often fail when faced with adverse weather conditions that obscure the view of the road paint (e.g., puddles and snowdrifts), poor road surface texture, or when road appearance degrades over time. we present a generic probabilistic method for localizing an autonomous vehicle equipped with a 3D LIDAR scanner. This proposed algorithm models the world as a mixture of several Gaussians, characterizing the z-height and reflectivity distribution of the environment\textemdash which we rasterize to facilitate fast and exact multiresolution inference. Results are shown on a collection of datasets totaling over 500 km of road data covering highway, rural, residential, and urban roadways, in which we demonstrate our method to be robust through heavy snowfall and roadway repavements.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Robust LIDAR localization using multiresolution Gaussian mixture maps for-Wolcott_Eustice-2017.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {3}
}

@article{wongIdentifyingUnknownInstances2019,
  title = {Identifying {{Unknown Instances}} for {{Autonomous Driving}}},
  author = {Wong, Kelvin and Wang, Shenlong and Ren, Mengye and Liang, Ming and Urtasun, Raquel},
  year = {2019},
  month = oct,
  abstract = {In the past few years, we have seen great progress in perception algorithms, particular through the use of deep learning. However, most existing approaches focus on a few categories of interest, which represent only a small fraction of the potential categories that robots need to handle in the real-world. Thus, identifying objects from unknown classes remains a challenging yet crucial task. In this paper, we develop a novel open-set instance segmentation algorithm for point clouds which can segment objects from both known and unknown classes in a holistic way. Our method uses a deep convolutional neural network to project points into a category-agnostic embedding space in which they can be clustered into instances irrespective of their semantics. Experiments on two large-scale self-driving datasets validate the effectiveness of our proposed method.},
  archivePrefix = {arXiv},
  eprint = {1910.11296},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Identifying Unknown Instances for Autonomous Driving-Wong et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AFU4H522/1910.html},
  journal = {arXiv:1910.11296 [cs]},
  primaryClass = {cs}
}

@article{wongVOICEDDepthCompletion2019,
  title = {{{VOICED}}: {{Depth Completion}} from {{Inertial Odometry}} and {{Vision}}},
  shorttitle = {{{VOICED}}},
  author = {Wong, Alex and Fei, Xiaohan and Soatto, Stefano},
  year = {2019},
  month = may,
  abstract = {We describe a method to infer dense depth from camera motion and sparse depth as estimated using a visual-inertial odometry system. Unlike other scenarios using point clouds from lidar or structured light sensors, we have few hundreds to few thousand points, insufficient to inform the topology of the scene. Our method first constructs a piecewise planar scaffolding of the scene, and then uses it to infer dense depth using the image along with the sparse points. We use a predictive cross-modal criterion, akin to `self-supervision,' measuring photometric consistency across time, forward-backward pose consistency, and geometric compatibility with the sparse point cloud. We also launch the first visual-inertial + depth dataset, which we hope will foster additional exploration into combining the complementary strengths of visual and inertial sensors. To compare our method to prior work, we adopt the unsupervised KITTI depth completion benchmark, and show state-of-the-art performance on it.},
  archivePrefix = {arXiv},
  eprint = {1905.08616},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/VOICED-Wong et al-2019.pdf;/Users/sunjiaming/Zotero/storage/FUJZW874/1905.html},
  journal = {arXiv:1905.08616 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  year = {2018},
  month = mar,
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems \textemdash{} BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archivePrefix = {arXiv},
  eprint = {1803.08494},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Group Normalization-Wu_He-2018.pdf},
  journal = {arXiv:1803.08494 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{wuInteractiveShapeCosegmentation2014,
  title = {Interactive Shape Co-Segmentation via Label Propagation},
  author = {Wu, Zizhao and Shou, Ruyang and Wang, Yunhai and Liu, Xinguo},
  year = {2014},
  month = feb,
  volume = {38},
  pages = {248--254},
  issn = {0097-8493},
  doi = {10.1016/j.cag.2013.11.009},
  abstract = {In this paper, we present an interactive approach for shape co-segmentation via label propagation. Our intuitive approach is able to produce error-free results and is very effective at handling out-of-sample data. Specifically, we start by over-segmenting a set of shapes into primitive patches. Then, we allow the users to assign labels to some patches and propagate the label information from these patches to the unlabeled ones. We iterate the last two steps until the error-free consistent segmentations are obtained. Additionally, we provide an inductive extension of our framework, which effectively addresses the out-of-sample data. The experimental results demonstrate the effectiveness of our approach.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Interactive shape co-segmentation via label propagation-Wu et al-2014.pdf},
  journal = {Computers \& Graphics},
  language = {en}
}

@article{wuPointConvDeepConvolutional2018,
  title = {{{PointConv}}: {{Deep Convolutional Networks}} on {{3D Point Clouds}}},
  shorttitle = {{{PointConv}}},
  author = {Wu, Wenxuan and Qi, Zhongang and Fuxin, Li},
  year = {2018},
  month = nov,
  abstract = {Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and density functions through kernel density estimation. The most important contribution of this work is a novel reformulation proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.},
  archivePrefix = {arXiv},
  eprint = {1811.07246},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointConv-Wu et al-2018.pdf;/Users/sunjiaming/Zotero/storage/77LKDEIZ/1811.html},
  journal = {arXiv:1811.07246 [cs]},
  primaryClass = {cs}
}

@article{wuPointPWCNetCoarsetoFineNetwork2019,
  title = {{{PointPWC}}-{{Net}}: {{A Coarse}}-to-{{Fine Network}} for {{Supervised}} and {{Self}}-{{Supervised Scene Flow Estimation}} on {{3D Point Clouds}}},
  shorttitle = {{{PointPWC}}-{{Net}}},
  author = {Wu, Wenxuan and Wang, Zhiyuan and Li, Zhuwen and Liu, Wei and Fuxin, Li},
  year = {2019},
  month = nov,
  abstract = {We propose a novel end-to-end deep scene flow model, called PointPWC-Net, on 3D point clouds in a coarse-to-fine fashion. Flow computed at the coarse level is upsampled and warped to a finer level, enabling the algorithm to accommodate for large motion without a prohibitive search space. We introduce novel cost volume, upsampling, and warping layers to efficiently handle 3D point cloud data. Unlike traditional cost volumes that require exhaustively computing all the cost values on a high-dimensional grid, our point-based formulation discretizes the cost volume onto input 3D points, and a PointConv operation efficiently computes convolutions on the cost volume. Experiment results on FlyingThings3D outperform the state-of-the-art by a large margin. We further explore novel self-supervised losses to train our model and achieve comparable results to state-of-the-art trained with supervised loss. Without any fine-tuning, our method also shows great generalization ability on KITTI Scene Flow 2015 dataset, outperforming all previous methods.},
  archivePrefix = {arXiv},
  eprint = {1911.12408},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointPWC-Net-Wu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/82EQB3LC/1911.html},
  journal = {arXiv:1911.12408 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{wuSCFusionRealtimeIncremental2020,
  title = {{{SCFusion}}: {{Real}}-Time {{Incremental Scene Reconstruction}} with {{Semantic Completion}}},
  shorttitle = {{{SCFusion}}},
  author = {Wu, Shun-Cheng and Tateno, Keisuke and Navab, Nassir and Tombari, Federico},
  year = {2020},
  month = oct,
  abstract = {Real-time scene reconstruction from depth data inevitably suffers from occlusion, thus leading to incomplete 3D models. Partial reconstructions, in turn, limit the performance of algorithms that leverage them for applications in the context of, e.g., augmented reality, robotic navigation, and 3D mapping. Most methods address this issue by predicting the missing geometry as an offline optimization, thus being incompatible with real-time applications. We propose a framework that ameliorates this issue by performing scene reconstruction and semantic scene completion jointly in an incremental and real-time manner, based on an input sequence of depth maps. Our framework relies on a novel neural architecture designed to process occupancy maps and leverages voxel states to accurately and efficiently fuse semantic completion with the 3D global model. We evaluate the proposed approach quantitatively and qualitatively, demonstrating that our method can obtain accurate 3D semantic scene completion in real-time.},
  archivePrefix = {arXiv},
  eprint = {2010.13662},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SCFusion-Wu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/7W3Q857D/2010.html},
  journal = {arXiv:2010.13662 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{wuUnsupervisedLearningProbably2020,
  title = {Unsupervised {{Learning}} of {{Probably Symmetric Deformable 3D Objects}} from {{Images}} in the {{Wild}}},
  author = {Wu, Shangzhe and Rupprecht, Christian and Vedaldi, Andrea},
  year = {2020},
  month = mar,
  abstract = {We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.},
  archivePrefix = {arXiv},
  eprint = {1911.11130},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images-Wu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NQ756DVL/1911.html},
  journal = {arXiv:1911.11130 [cs]},
  primaryClass = {cs}
}

@article{xiangGenerating3DAdversarial2018,
  title = {Generating {{3D Adversarial Point Clouds}}},
  author = {Xiang, Chong and Qi, Charles R. and Li, Bo},
  year = {2018},
  month = sep,
  abstract = {Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classification dataset. Overall, our attack algorithms achieve a success rate higher than 99\% for all targeted attacks},
  archivePrefix = {arXiv},
  eprint = {1809.07016},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Generating 3D Adversarial Point Clouds-Xiang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/MV935Y5X/1809.html},
  journal = {arXiv:1809.07016 [cs]},
  primaryClass = {cs}
}

@article{xiangLearningRGBDFeature2020,
  title = {Learning {{RGB}}-{{D Feature Embeddings}} for {{Unseen Object Instance Segmentation}}},
  author = {Xiang, Yu and Xie, Christopher and Mousavian, Arsalan and Fox, Dieter},
  year = {2020},
  month = jul,
  abstract = {Segmenting unseen objects in cluttered scenes is an important skill that robots need to acquire in order to perform tasks in new environments. In this work, we propose a new method for unseen object instance segmentation by learning RGB-D feature embeddings from synthetic data. A metric learning loss function is utilized to learn to produce pixel-wise feature embeddings such that pixels from the same object are close to each other and pixels from different objects are separated in the embedding space. With the learned feature embeddings, a mean shift clustering algorithm can be applied to discover and segment unseen objects. We further improve the segmentation accuracy with a new two-stage clustering algorithm. Our method demonstrates that non-photorealistic synthetic RGB and depth images can be used to learn feature embeddings that transfer well to real-world images for unseen object instance segmentation.},
  archivePrefix = {arXiv},
  eprint = {2007.15157},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation-Xiang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9DXI86WM/2007.html},
  journal = {arXiv:2007.15157 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{xiangMonocularTotalCapture2018,
  title = {Monocular {{Total Capture}}: {{Posing Face}}, {{Body}}, and {{Hands}} in the {{Wild}}},
  shorttitle = {Monocular {{Total Capture}}},
  author = {Xiang, Donglai and Joo, Hanbyul and Sheikh, Yaser},
  year = {2018},
  month = dec,
  abstract = {We present the first method to capture the 3D total motion of a target person from a monocular view input. Given an image or a monocular video, our method reconstructs the motion from body, face, and fingers represented by a 3D deformable mesh model. We use an efficient representation called 3D Part Orientation Fields (POFs), to encode the 3D orientations of all body parts in the common 2D image space. POFs are predicted by a Fully Convolutional Network (FCN), along with the joint confidence maps. To train our network, we collect a new 3D human motion dataset capturing diverse total body motion of 40 subjects in a multiview system. We leverage a 3D deformable human model to reconstruct total body pose from the CNN outputs by exploiting the pose and shape prior in the model. We also present a texture-based tracking method to obtain temporally coherent motion capture output. We perform thorough quantitative evaluations including comparison with the existing body-specific and hand-specific methods, and performance analysis on camera viewpoint and human pose changes. Finally, we demonstrate the results of our total body motion capture on various challenging in-the-wild videos. Our code and newly collected human motion dataset will be publicly shared.},
  archivePrefix = {arXiv},
  eprint = {1812.01598},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Monocular Total Capture-Xiang et al-2018.pdf;/Users/sunjiaming/Zotero/storage/JM7TC4NA/1812.html},
  journal = {arXiv:1812.01598 [cs]},
  primaryClass = {cs}
}

@incollection{xiangObjectNet3DLargeScale2016,
  title = {{{ObjectNet3D}}: {{A Large Scale Database}} for {{3D Object Recognition}}},
  shorttitle = {{{ObjectNet3D}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Xiang, Yu and Kim, Wonhui and Chen, Wei and Ji, Jingwei and Choy, Christopher and Su, Hao and Mottaghi, Roozbeh and Guibas, Leonidas and Savarese, Silvio},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9912},
  pages = {160--176},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46484-8_10},
  abstract = {We contribute a large scale database for 3D object recognition, named ObjectNet3D, that consists of 100 categories, 90,127 images, 201,888 objects in these images and 44,147 3D shapes. Objects in the 2D images in our database are aligned with the 3D shapes, and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object. Consequently, our database is useful for recognizing the 3D pose and 3D shape of objects from 2D images. We also provide baseline experiments on four tasks: region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval, which can serve as baselines for future research using our database. Our database is available online at http://cvgl.stanford.edu/projects/objectnet3d.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ObjectNet3D-Xiang et al-2016.pdf},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  keywords = {dataset},
  language = {en}
}

@article{xiangPoseCNNConvolutionalNeural2017,
  title = {{{PoseCNN}}: {{A Convolutional Neural Network}} for {{6D Object Pose Estimation}} in {{Cluttered Scenes}}},
  shorttitle = {{{PoseCNN}}},
  author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
  year = {2017},
  month = nov,
  abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.},
  archivePrefix = {arXiv},
  eprint = {1711.00199},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PoseCNN-Xiang et al-2017.pdf;/Users/sunjiaming/Zotero/storage/JFMCC3MA/1711.html},
  journal = {arXiv:1711.00199 [cs]},
  primaryClass = {cs}
}

@article{xiangSAPIENSimulAtedPartbased2020,
  title = {{{SAPIEN}}: {{A SimulAted Part}}-Based {{Interactive ENvironment}}},
  shorttitle = {{{SAPIEN}}},
  author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
  year = {2020},
  month = mar,
  abstract = {Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment.},
  archivePrefix = {arXiv},
  eprint = {2003.08515},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SAPIEN-Xiang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/I2DYQQGL/2003.html},
  journal = {arXiv:2003.08515 [cs]},
  primaryClass = {cs}
}

@article{xianUprightNetGeometryAwareCamera2019,
  title = {{{UprightNet}}: {{Geometry}}-{{Aware Camera Orientation Estimation}} from {{Single Images}}},
  shorttitle = {{{UprightNet}}},
  author = {Xian, Wenqi and Li, Zhengqi and Fisher, Matthew and Eisenmann, Jonathan and Shechtman, Eli and Snavely, Noah},
  year = {2019},
  month = aug,
  abstract = {We introduce UprightNet, a learning-based approach for estimating 2DoF camera orientation from a single RGB image of an indoor scene. Unlike recent methods that leverage deep learning to perform black-box regression from image to orientation parameters, we propose an end-to-end framework that incorporates explicit geometric reasoning. In particular, we design a network that predicts two representations of scene geometry, in both the local camera and global reference coordinate systems, and solves for the camera orientation as the rotation that best aligns these two predictions via a differentiable least squares module. This network can be trained end-to-end, and can be supervised with both ground truth camera poses and intermediate representations of surface geometry. We evaluate UprightNet on the single-image camera orientation task on synthetic and real datasets, and show significant improvements over prior state-of-the-art approaches.},
  archivePrefix = {arXiv},
  eprint = {1908.07070},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/UprightNet-Xian et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AH9UQT4M/1908.html},
  journal = {arXiv:1908.07070 [cs]},
  primaryClass = {cs}
}

@article{xiaoVideoObjectDetection2018,
  title = {Video {{Object Detection}} with an {{Aligned Spatial}}-{{Temporal Memory}}},
  author = {Xiao, Fanyi and Lee, Yong Jae},
  year = {2018},
  month = jul,
  abstract = {We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM's design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.},
  archivePrefix = {arXiv},
  eprint = {1712.06317},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Video Object Detection with an Aligned Spatial-Temporal Memory-Xiao_Lee-2018.pdf;/Users/sunjiaming/Zotero/storage/TYCYAWT9/1712.html},
  journal = {arXiv:1712.06317 [cs]},
  primaryClass = {cs}
}

@article{xieFineRegistration3D2015,
  title = {Fine Registration of {{3D}} Point Clouds Fusing Structural and Photometric Information Using an {{RGB}}-{{D}} Camera},
  author = {Xie, Jun and Hsu, Yu-Feng and Feris, Rogerio Schmidt and Sun, Ming-Ting},
  year = {2015},
  month = oct,
  volume = {32},
  pages = {194--204},
  issn = {1047-3203},
  doi = {10.1016/j.jvcir.2015.08.007},
  abstract = {We address the problem of accurate and efficient alignment of 3D point clouds captured by an RGB-D (Kinect-style) camera from different viewpoints. While the Iterative Closest Point (ICP) algorithm has been widely used for dense point cloud matching, it is limited in its ability to produce accurate results in challenging scenarios involving objects that lack structural features and have significant camera view changes. In this paper, we introduce a new cost function with dynamic weights for the ICP algorithm to tackle this problem. It balances the significance of structural and photometric features with dynamically adjusted weights to improve the error minimization process. Our algorithm also includes a novel outlier rejection method, which adopts adaptive thresholding at each ICP iteration, using both the structural information of the object and the spatial distances of sparse SIFT feature pairs. The effectiveness of our proposed approach is demonstrated by experimental results from various challenging scenarios. We obtained superior registration accuracy than related previous methods, at the same time maintaining low computational requirements.},
  file = {/Users/sunjiaming/Zotero/storage/DHQIS432/S1047320315001509.html},
  journal = {Journal of Visual Communication and Image Representation}
}

@article{xiePolarMaskSingleShot2019,
  title = {{{PolarMask}}: {{Single Shot Instance Segmentation}} with {{Polar Representation}}},
  shorttitle = {{{PolarMask}}},
  author = {Xie, Enze and Sun, Peize and Song, Xiaoge and Wang, Wenhai and Liu, Xuebo and Liang, Ding and Shen, Chunhua and Luo, Ping},
  year = {2019},
  month = oct,
  abstract = {In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used as a mask prediction module for instance segmentation, by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9\% in mask mAP with single-model and single-scale training/testing on challenging COCO dataset. For the first time, we demonstrate a much simpler and flexible instance segmentation framework achieving competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation tasks. Code is available at: github.com/xieenze/PolarMask.},
  archivePrefix = {arXiv},
  eprint = {1909.13226},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PolarMask-Xie et al-2019.pdf;/Users/sunjiaming/Zotero/storage/84UHUQNF/1909.html},
  journal = {arXiv:1909.13226 [cs]},
  primaryClass = {cs}
}

@article{xieVideoDepthEstimation2020,
  title = {Video {{Depth Estimation}} by {{Fusing Flow}}-to-{{Depth Proposals}}},
  author = {Xie, Jiaxin and Lei, Chenyang and Li, Zhuwen and Li, Li Erran and Chen, Qifeng},
  year = {2020},
  month = mar,
  abstract = {Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present an approach with a differentiable flow-to-depth layer for video depth estimation. The model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera pose, our flow-to-depth layer generates depth proposals and the corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize depth proposals and their confidence maps inferred from different adjacent frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to improve the results further. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable cross dataset generalization capability: our model trained on KITTI still performs well on the unseen Waymo dataset.},
  archivePrefix = {arXiv},
  eprint = {1912.12874},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Video Depth Estimation by Fusing Flow-to-Depth Proposals-Xie et al-2020.pdf;/Users/sunjiaming/Zotero/storage/IJRPAHBN/1912.html},
  journal = {arXiv:1912.12874 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{xingDeformableGeneratorNetwork2018,
  title = {Deformable {{Generator Network}}: {{Unsupervised Disentanglement}} of {{Appearance}} and {{Geometry}}},
  shorttitle = {Deformable {{Generator Network}}},
  author = {Xing, Xianglei and Gao, Ruiqi and Han, Tian and Zhu, Song-Chun and Wu, Ying Nian},
  year = {2018},
  month = jun,
  abstract = {We propose a deformable generator model to disentangle the appearance and geometric information from images into two independent latent vectors. The appearance generator produces the appearance information, including color, illumination, identity or category, of an image. The geometric generator produces displacement of the coordinates of each pixel and performs geometric warping, such as stretching and rotation, on the appearance generator to obtain the final synthesized image. The proposed model can learn both representations from image data in an unsupervised manner. The learned geometric generator can be conveniently transferred to the other image datasets to facilitate downstream AI tasks.},
  archivePrefix = {arXiv},
  eprint = {1806.06298},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Xing et al_2018_Deformable Generator Network.pdf;/Users/sunjiaming/Zotero/storage/7B6LN45K/1806.html},
  journal = {arXiv:1806.06298 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xingUnsupervisedDisentanglingAppearance,
  title = {Unsupervised {{Disentangling}} of {{Appearance}} and {{Geometry}} by {{Deformable Generator Network}}},
  author = {Xing, Xianglei and Han, Tian and Gao, Ruiqi and Zhu, Song-Chun and Wu, Ying Nian},
  pages = {10},
  abstract = {We present a deformable generator model to disentangle the appearance and geometric information in purely unsupervised manner. The appearance generator models the appearance related information, including color, illumination, identity or category, of an image, while the geometric generator performs geometric related warping, such as rotation and stretching, through generating displacement of the coordinate of each pixel to obtain the final image. Two generators act upon independent latent factors to extract disentangled appearance and geometric information from images. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric information can be well disentangled, and the learned geometric generator can be conveniently transferred to other image datasets to facilitate knowledge transfer tasks.},
  file = {/Users/sunjiaming/Zotero/storage/3UWPLDBX/Xing et al. - Unsupervised Disentangling of Appearance and Geome.pdf},
  language = {en}
}

@article{xiongDeformableFilterConvolution2019,
  title = {Deformable {{Filter Convolution}} for {{Point Cloud Reasoning}}},
  author = {Xiong, Yuwen and Ren, Mengye and Liao, Renjie and Wong, Kelvin and Urtasun, Raquel},
  year = {2019},
  month = jul,
  abstract = {Point clouds are the native output of many real-world 3D sensors. To borrow the success of 2D convolutional network architectures, a majority of popular 3D perception models voxelize the points, which can result in a loss of local geometric details that cannot be recovered. In this paper, we propose a novel learnable convolution layer for processing 3D point cloud data directly. Instead of discretizing points into fixed voxels, we deform our learnable 3D filters to match with the point cloud shape. We propose to combine voxelized backbone networks with our deformable filter layer at 1) the network input stream and 2) the output prediction layers to enhance point level reasoning. We obtain state-of-the-art results on LiDAR semantic segmentation and producing a significant gain in performance on LiDAR object detection.},
  archivePrefix = {arXiv},
  eprint = {1907.13079},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deformable Filter Convolution for Point Cloud Reasoning-Xiong et al-2019.pdf;/Users/sunjiaming/Zotero/storage/2ZF2PM84/1907.html},
  journal = {arXiv:1907.13079 [cs]},
  primaryClass = {cs}
}

@article{xiongUPSNetUnifiedPanoptic2019,
  title = {{{UPSNet}}: {{A Unified Panoptic Segmentation Network}}},
  shorttitle = {{{UPSNet}}},
  author = {Xiong, Yuwen and Liao, Renjie and Zhao, Hengshuang and Hu, Rui and Bai, Min and Yumer, Ersin and Urtasun, Raquel},
  year = {2019},
  month = jan,
  abstract = {In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves state-of-theart performance with much faster inference.},
  archivePrefix = {arXiv},
  eprint = {1901.03784},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/UPSNet-Xiong et al-2019.pdf},
  journal = {arXiv:1901.03784 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{xuAccurateOpticalFlow2017,
  title = {Accurate {{Optical Flow}} via {{Direct Cost Volume Processing}}},
  author = {Xu, Jia and Ranftl, Ren{\'e} and Koltun, Vladlen},
  year = {2017},
  month = apr,
  abstract = {We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1704.07325},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Accurate Optical Flow via Direct Cost Volume Processing-Xu et al-2017.pdf},
  journal = {arXiv:1704.07325 [cs]},
  keywords = {flow},
  language = {en},
  primaryClass = {cs}
}

@article{xuDeepMOTDifferentiableFramework2019,
  title = {{{DeepMOT}}: {{A Differentiable Framework}} for {{Training Multiple Object Trackers}}},
  shorttitle = {{{DeepMOT}}},
  author = {Xu, Yihong and Ban, Yutong and {Alameda-Pineda}, Xavier and Horaud, Radu},
  year = {2019},
  month = jun,
  abstract = {Multiple Object Tracking accuracy and precision (MOTA and MOTP) are two standard and widely-used metrics to assess the quality of multiple object trackers. They are specifically designed to encode the challenges and difficulties of tracking multiple objects. To directly optimize a tracker based on MOTA and MOTP is difficult, since both the metrics are strongly rely on the Hungarian algorithm, which are non-differentiable. We propose a differentiable proxy for the MOTA and MOTP, thus allowing to train a deep multiple-object tracker by directly optimizing (a proxy of) the standard MOT metrics. The proposed approximation is based on a bidirectional recurrent network that inputs the object-to-hypothesis distance matrix and outputs the optimal hypothesis-to-object association, thus emulating the Hungarian algorithm. Followed by a differentiable module, the estimated association is used to compute the MOTA and MOTP. The experimental study demonstrates the benefits of this differentiable framework on two recent deep trackers over the MOT17 dataset. Moreover, the code is publicly available from https://gitlab.inria.fr/yixu/deepmot.},
  archivePrefix = {arXiv},
  eprint = {1906.06618},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeepMOT-Xu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AZW7P7IP/1906.html},
  journal = {arXiv:1906.06618 [cs]},
  primaryClass = {cs}
}

@article{xuDISNDeepImplicit2019,
  title = {{{DISN}}: {{Deep Implicit Surface Network}} for {{High}}-Quality {{Single}}-View {{3D Reconstruction}}},
  shorttitle = {{{DISN}}},
  author = {Xu, Qiangeng and Wang, Weiyue and Ceylan, Duygu and Mech, Radomir and Neumann, Ulrich},
  year = {2019},
  month = may,
  abstract = {Reconstructing 3D shapes from single-view images has been a long-standing research problem and has attracted a lot of attention. In this paper, we present DISN, a Deep Implicit Surface Network that generates a high-quality 3D shape given an input image by predicting the underlying signed distance field. In addition to utilizing global image features, DISN also predicts the local image patch each 3D point sample projects onto and extracts local features from the patch. Combining global and local features significantly improves the accuracy of the predicted signed distance field. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at github.com/laughtervv/DISN.},
  archivePrefix = {arXiv},
  eprint = {1905.10711},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DISN-Xu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LEQX34AD/1905.html},
  journal = {arXiv:1905.10711 [cs]},
  primaryClass = {cs}
}

@article{xueBLVDBuildingLargescale2019,
  title = {{{BLVD}}: {{Building A Large}}-Scale {{5D Semantics Benchmark}} for {{Autonomous Driving}}},
  shorttitle = {{{BLVD}}},
  author = {Xue, Jianru and Fang, Jianwu and Li, Tao and Zhang, Bohua and Zhang, Pu and Ye, Zhen and Dou, Jian},
  year = {2019},
  month = mar,
  abstract = {In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249,129 3D annotations, 4,902 independent individuals for tracking with the length of overall 214,922 points, 6,004 valid fragments for 5D interactive event recognition, and 4,900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/BLVD-Xue et al-2019.pdf;/Users/sunjiaming/Zotero/storage/6M83G2ZM/1903.html},
  keywords = {dataset},
  language = {en}
}

@article{xueHolisticallyAttractedWireframeParsing2020,
  title = {Holistically-{{Attracted Wireframe Parsing}}},
  author = {Xue, Nan and Wu, Tianfu and Bai, Song and Wang, Fu-Dong and Xia, Gui-Song and Zhang, Liangpei and Torr, Philip H. S.},
  year = {2020},
  month = mar,
  abstract = {This paper presents a fast and parsimonious parsing method to accurately and robustly detect a vectorized wireframe in an input image with a single forward pass. The proposed method is end-to-end trainable, consisting of three components: (i) line segment and junction proposal generation, (ii) line segment and junction matching, and (iii) line segment and junction verification. For computing line segment proposals, a novel exact dual representation is proposed which exploits a parsimonious geometric reparameterization for line segments and forms a holistic 4-dimensional attraction field map for an input image. Junctions can be treated as the "basins" in the attraction field. The proposed method is thus called Holistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed method is tested on two benchmarks, the Wireframe dataset, and the YorkUrban dataset. On both benchmarks, it obtains state-of-the-art performance in terms of accuracy and efficiency. For example, on the Wireframe dataset, compared to the previous state-of-the-art method L-CNN, it improves the challenging mean structural average precision (msAP) by a large margin (\$2.8\textbackslash\%\$ absolute improvements) and achieves 29.5 FPS on single GPU (\$89\textbackslash\%\$ relative improvement). A systematic ablation study is performed to further justify the proposed method.},
  archivePrefix = {arXiv},
  eprint = {2003.01663},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Holistically-Attracted Wireframe Parsing-Xue et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9J8549YI/2003.html},
  journal = {arXiv:2003.01663 [cs]},
  primaryClass = {cs}
}

@article{xuGeometryAwareVideoObject2019,
  title = {Geometry-{{Aware Video Object Detection}} for {{Static Cameras}}},
  author = {Xu, Dan and Xie, Weidi and Zisserman, Andrew},
  year = {2019},
  month = sep,
  abstract = {In this paper we propose a geometry-aware model for video object detection. Specifically, we consider the setting that cameras can be well approximated as static, e.g. in video surveillance scenarios, and scene pseudo depth maps can therefore be inferred easily from the object scale on the image plane. We make the following contributions: First, we extend the recent anchor-free detector (CornerNet [17]) to video object detections. In order to exploit the spatial-temporal information while maintaining high efficiency, the proposed model accepts video clips as input, and only makes predictions for the starting and the ending frames, i.e. heatmaps of object bounding box corners and the corresponding embeddings for grouping. Second, to tackle the challenge from scale variations in object detection, scene geometry information, e.g. derived depth maps, is explicitly incorporated into deep networks for multi-scale feature selection and for the network prediction. Third, we validate the proposed architectures on an autonomous driving dataset generated from the Carla simulator [5], and on a real dataset for human detection (DukeMTMC dataset [28]). When comparing with the existing competitive single-stage or two-stage detectors, the proposed geometry-aware spatio-temporal network achieves significantly better results.},
  archivePrefix = {arXiv},
  eprint = {1909.03140},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Geometry-Aware Video Object Detection for Static Cameras-Xu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/G657YE79/1909.html},
  journal = {arXiv:1909.03140 [cs]},
  primaryClass = {cs}
}

@article{xuMIDFusionOctreebasedObjectLevel2018,
  title = {{{MID}}-{{Fusion}}: {{Octree}}-Based {{Object}}-{{Level Multi}}-{{Instance Dynamic SLAM}}},
  shorttitle = {{{MID}}-{{Fusion}}},
  author = {Xu, Binbin and Li, Wenbin and Tzoumanikas, Dimos and Bloesch, Michael and Davison, Andrew and Leutenegger, Stefan},
  year = {2018},
  month = dec,
  abstract = {We propose a new multi-instance dynamic RGB-D SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and refine mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the first system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.},
  archivePrefix = {arXiv},
  eprint = {1812.07976},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MID-Fusion-Xu et al-2018.pdf;/Users/sunjiaming/Zotero/storage/4KNSD9WP/1812.html},
  journal = {arXiv:1812.07976 [cs]},
  keywords = {reconstruction},
  primaryClass = {cs}
}

@inproceedings{xuMultilevelFusionBased2018,
  title = {Multi-Level {{Fusion Based 3D Object Detection}} from {{Monocular Images}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Bin and Chen, Zhenzhong},
  year = {2018},
  month = jun,
  pages = {2345--2353},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00249},
  abstract = {In this paper, we present an end-to-end multi-level fusion based framework for 3D object detection from a single monocular image. The whole network is composed of two parts: one for 2D region proposal generation and another for simultaneously predictions of objects' 2D locations, orientations, dimensions, and 3D locations. With the help of a stand-alone module to estimate the disparity and compute the 3D point cloud, we introduce the multi-level fusion scheme. First, we encode the disparity information with a front view feature representation and fuse it with the RGB image to enhance the input. Second, features extracted from the original input and the point cloud are combined to boost the object detection. For 3D localization, we introduce an extra stream to predict the location information from point cloud directly and add it to the aforementioned location prediction. The proposed algorithm can directly output both 2D and 3D object detection results in an endto-end fashion with only a single RGB image as the input. The experimental results on the challenging KITTI benchmark demonstrate that our algorithm significantly outperforms monocular state-of-the-art methods.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multi-level Fusion Based 3D Object Detection from Monocular Images-Xu_Chen-2018.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{xuPlanarPriorAssisted2019,
  title = {Planar {{Prior Assisted PatchMatch Multi}}-{{View Stereo}}},
  author = {Xu, Qingshan and Tao, Wenbing},
  year = {2019},
  month = dec,
  abstract = {The completeness of 3D models is still a challenging problem in multi-view stereo (MVS) due to the unreliable photometric consistency in low-textured areas. Since low-textured areas usually exhibit strong planarity, planar models are advantageous to the depth estimation of low-textured areas. On the other hand, PatchMatch multi-view stereo is very efficient for its sampling and propagation scheme. By taking advantage of planar models and PatchMatch multi-view stereo, we propose a planar prior assisted PatchMatch multi-view stereo framework in this paper. In detail, we utilize a probabilistic graphical model to embed planar models into PatchMatch multi-view stereo and contribute a novel multi-view aggregated matching cost. This novel cost takes both photometric consistency and planar compatibility into consideration, making it suited for the depth estimation of both non-planar and planar regions. Experimental results demonstrate that our method can efficiently recover the depth information of extremely low-textured areas, thus obtaining high complete 3D models and achieving state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {1912.11744},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/XRQ6SCT2/Xu and Tao - 2019 - Planar Prior Assisted PatchMatch Multi-View Stereo.pdf;/Users/sunjiaming/Zotero/storage/SPU2KBIQ/1912.html},
  journal = {arXiv:1912.11744 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{xuPointFusionDeepSensor2018,
  title = {{{PointFusion}}: {{Deep Sensor Fusion}} for {{3D Bounding Box Estimation}}},
  shorttitle = {{{PointFusion}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xu, Danfei and Anguelov, Dragomir and Jain, Ashesh},
  year = {2018},
  month = jun,
  pages = {244--253},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00033},
  abstract = {We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multistage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and applicationagnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PointFusion-Xu et al-2018.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{xuRGBDIndividualSegmentation2019,
  title = {{{RGB}}-{{D Individual Segmentation}}},
  author = {Xu, Wenqiang and Fu, Yanjun and Luo, Yuchen and Liu, Chang and Lu, Cewu},
  year = {2019},
  month = oct,
  abstract = {Fine-grained recognition task deals with sub-category classification problem, which is important for real-world applications. In this work, we are particularly interested in the segmentation task on the \textbackslash emph\{finest-grained\} level, which is specifically named "individual segmentation". In other words, the individual-level category has no sub-category under it. Segmentation problem in the individual level reveals some new properties, limited training data for single individual object, unknown background, and difficulty for the use of depth. To address these new problems, we propose a "Context Less-Aware" (CoLA) pipeline, which produces RGB-D object-predominated images that have less background context, and enables a scale-aware training and testing with 3D information. Extensive experiments show that the proposed CoLA strategy largely outperforms baseline methods on YCB-Video dataset and our proposed Supermarket-10K dataset. Code, trained model and new dataset will be published with this paper.},
  archivePrefix = {arXiv},
  eprint = {1910.07641},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RGB-D Individual Segmentation-Xu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4PWVSTHP/1910.html},
  journal = {arXiv:1910.07641 [cs]},
  primaryClass = {cs}
}

@article{xuZoomNetPartAwareAdaptive2020,
  title = {{{ZoomNet}}: {{Part}}-{{Aware Adaptive Zooming Neural Network}} for {{3D Object Detection}}},
  shorttitle = {{{ZoomNet}}},
  author = {Xu, Zhenbo and Zhang, Wei and Ye, Xiaoqing and Tan, Xiao and Yang, Wei and Wen, Shilei and Ding, Errui and Meng, Ajin and Huang, Liusheng},
  year = {2020},
  month = mar,
  abstract = {3D object detection is an essential task in autonomous driving and robotics. Though great progress has been made, challenges remain in estimating 3D pose for distant and occluded objects. In this paper, we present a novel framework named ZoomNet for stereo imagery-based 3D detection. The pipeline of ZoomNet begins with an ordinary 2D object detection model which is used to obtain pairs of left-right bounding boxes. To further exploit the abundant texture cues in RGB images for more accurate disparity estimation, we introduce a conceptually straight-forward module -- adaptive zooming, which simultaneously resizes 2D instance bounding boxes to a unified resolution and adjusts the camera intrinsic parameters accordingly. In this way, we are able to estimate higher-quality disparity maps from the resized box images then construct dense point clouds for both nearby and distant objects. Moreover, we introduce to learn part locations as complementary features to improve the resistance against occlusion and put forward the 3D fitting score to better estimate the 3D detection quality. Extensive experiments on the popular KITTI 3D detection dataset indicate ZoomNet surpasses all previous state-of-the-art methods by large margins (improved by 9.4\% on APbv (IoU=0.7) over pseudo-LiDAR). Ablation study also demonstrates that our adaptive zooming strategy brings an improvement of over 10\% on AP3d (IoU=0.7). In addition, since the official KITTI benchmark lacks fine-grained annotations like pixel-wise part locations, we also present our KFG dataset by augmenting KITTI with detailed instance-wise annotations including pixel-wise part location, pixel-wise disparity, etc.. Both the KFG dataset and our codes will be publicly available at https://github.com/detectRecog/ZoomNet.},
  archivePrefix = {arXiv},
  eprint = {2003.00529},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/ZoomNet-Xu et al-2020.pdf;/Users/sunjiaming/Zotero/storage/T9BXSZN4/2003.html},
  journal = {arXiv:2003.00529 [cs]},
  primaryClass = {cs}
}

@incollection{yanDenseHybridRecurrent2020,
  title = {Dense {{Hybrid Recurrent Multi}}-View {{Stereo Net}} with {{Dynamic Consistency Checking}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Yan, Jianfeng and Wei, Zizhuang and Yi, Hongwei and Ding, Mingyu and Zhang, Runze and Chen, Yisong and Wang, Guoping and Tai, Yu-Wing},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12349},
  pages = {674--689},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58548-8_39},
  abstract = {In this paper, we propose an efficient and effective dense hybrid recurrent multi-view stereo net with dynamic consistency checking, namely D2HC-RMVSNet, for accurate dense point cloud reconstruction. Our novel hybrid recurrent multi-view stereo net consists of two core modules: 1) a light DRENet (Dense Reception Expanded) module to extract dense feature maps of original size with multi-scale context information, 2) a HU-LSTM (Hybrid U-LSTM) to regularize 3D matching volume into predicted depth map, which efficiently aggregates different scale information by coupling LSTM and U-Net architecture. To further improve the accuracy and completeness of reconstructed point clouds, we leverage a dynamic consistency checking strategy instead of prefixed parameters and strategies widely adopted in existing methods for dense point cloud reconstruction. In doing so, we dynamically aggregate geometric consistency matching error among all the views. Our method ranks 1st on the complex outdoor Tanks and Temples benchmark over all the methods. Extensive experiments on the in-door DTU dataset show our method exhibits competitive performance to the state-of-theart method while dramatically reduces memory consumption, which costs only 19.4\% of R-MVSNet memory consumption. The codebase is available at https://github.com/yhw-yhw/D2HC-RMVSNet.},
  file = {/Users/sunjiaming/Zotero/storage/55CXED2L/Yan et al. - 2020 - Dense Hybrid Recurrent Multi-view Stereo Net with .pdf},
  isbn = {978-3-030-58547-1 978-3-030-58548-8},
  language = {en}
}

@article{yang3DSSDPointbased3D2020,
  title = {{{3DSSD}}: {{Point}}-Based {{3D Single Stage Object Detector}}},
  shorttitle = {{{3DSSD}}},
  author = {Yang, Zetong and Sun, Yanan and Liu, Shu and Jia, Jiaya},
  year = {2020},
  month = feb,
  abstract = {Currently, there have been many kinds of voxel-based 3D single stage detectors, while point-based single stage methods are still underexplored. In this paper, we first present a lightweight and effective point-based 3D single stage object detector, named 3DSSD, achieving a good balance between accuracy and efficiency. In this paradigm, all upsampling layers and refinement stage, which are indispensable in all existing point-based methods, are abandoned to reduce the large computation cost. We novelly propose a fusion sampling strategy in downsampling process to make detection on less representative points feasible. A delicate box prediction network including a candidate generation layer, an anchor-free regression head with a 3D center-ness assignment strategy is designed to meet with our demand of accuracy and speed. Our paradigm is an elegant single stage anchor-free framework, showing great superiority to other existing methods. We evaluate 3DSSD on widely used KITTI dataset and more challenging nuScenes dataset. Our method outperforms all state-of-the-art voxel-based single stage methods by a large margin, and has comparable performance to two stage point-based methods as well, with inference speed more than 25 FPS, 2x faster than former state-of-the-art point-based methods.},
  archivePrefix = {arXiv},
  eprint = {2002.10187},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3DSSD-Yang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/RLZHXS8Y/2002.html},
  journal = {arXiv:2002.10187 [cs]},
  primaryClass = {cs}
}

@article{yangConditionalPriorNetworks,
  title = {Conditional {{Prior Networks}} for {{Optical Flow}}},
  author = {Yang, Yanchao and Soatto, Stefano},
  pages = {17},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Conditional Prior Networks for Optical Flow-Yang_Soatto-.pdf},
  language = {en}
}

@article{yangCubeSLAMMonocular3D2018,
  title = {{{CubeSLAM}}: {{Monocular 3D Object Detection}} and {{SLAM}} without {{Prior Models}}},
  shorttitle = {{{CubeSLAM}}},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2018},
  month = jun,
  abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM without prior object model, and demonstrate that the two aspects can benefit each other. For 3D detection, we generate high quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected to align with image edges. Experiments on SUN RGBD and KITTI shows the efficiency and accuracy over existing approaches. Then in the second part, multi-view bundle adjustment with novel measurement functions is proposed to jointly optimize camera poses, objects and points, utilizing single view detection results. Objects can provide more geometric constraints and scale consistency compared to points. On the collected and public TUM and KITTI odometry datasets, we achieve better pose estimation accuracy over the state-of-the-art monocular SLAM while also improve the 3D object detection accuracy at the same time.},
  archivePrefix = {arXiv},
  eprint = {1806.00557},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/CubeSLAM-Yang_Scherer-2018.pdf},
  journal = {arXiv:1806.00557 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangCubeSLAMMonocular3D2018a,
  title = {{{CubeSLAM}}: {{Monocular 3D Object SLAM}}},
  shorttitle = {{{CubeSLAM}}},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2018},
  month = jun,
  doi = {10.1109/TRO.2019.2909168},
  abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM in both static and dynamic environments, and demonstrate that the two parts can improve each other. Firstly for single image object detection, we generate high-quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Secondly, multi-view bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the state-of-the-art monocular camera pose estimation and at the same time, improves the 3D object detection accuracy.},
  archivePrefix = {arXiv},
  eprint = {1806.00557},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/CubeSLAM-Yang_Scherer-22.pdf;/Users/sunjiaming/Zotero/storage/FTWLGYBR/1806.html},
  journal = {arXiv:1806.00557 [cs]},
  primaryClass = {cs}
}

@article{yangD3VODeepDepth2020,
  title = {{{D3VO}}: {{Deep Depth}}, {{Deep Pose}} and {{Deep Uncertainty}} for {{Monocular Visual Odometry}}},
  shorttitle = {{{D3VO}}},
  author = {Yang, Nan and {von Stumberg}, Lukas and Wang, Rui and Cremers, Daniel},
  year = {2020},
  month = mar,
  abstract = {We propose D3VO as a novel framework for monocular visual odometry that exploits deep networks on three levels -- deep depth, pose and uncertainty estimation. We first propose a novel self-supervised monocular depth estimation network trained on stereo videos without any external supervision. In particular, it aligns the training image pairs into similar lighting condition with predictive brightness transformation parameters. Besides, we model the photometric uncertainties of pixels on the input images, which improves the depth estimation accuracy and provides a learned weighting function for the photometric residuals in direct (feature-less) visual odometry. Evaluation results show that the proposed network outperforms state-of-the-art self-supervised depth estimation networks. D3VO tightly incorporates the predicted depth, pose and uncertainty into a direct visual odometry method to boost both the front-end tracking as well as the back-end non-linear optimization. We evaluate D3VO in terms of monocular visual odometry on both the KITTI odometry benchmark and the EuRoC MAV dataset. The results show that D3VO outperforms state-of-the-art traditional monocular VO methods by a large margin. It also achieves comparable results to state-of-the-art stereo/LiDAR odometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC MAV, while using only a single camera.},
  archivePrefix = {arXiv},
  eprint = {2003.01060},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/D3VO-Yang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/CHPUEW2N/2003.html},
  journal = {arXiv:2003.01060 [cs]},
  primaryClass = {cs}
}

@article{yangDeeperLabSingleShotImage2019,
  title = {{{DeeperLab}}: {{Single}}-{{Shot Image Parser}}},
  shorttitle = {{{DeeperLab}}},
  author = {Yang, Tien-Ju and Collins, Maxwell D. and Zhu, Yukun and Hwang, Jyh-Jing and Liu, Ting and Zhang, Xiao and Sze, Vivienne and Papandreou, George and Chen, Liang-Chieh},
  year = {2019},
  month = feb,
  abstract = {We present a single-shot, bottom-up approach for whole image parsing. Whole image parsing, also known as Panoptic Segmentation, generalizes the tasks of semantic segmentation for 'stuff' classes and instance segmentation for 'thing' classes, assigning both semantic and instance labels to every pixel in an image. Recent approaches to whole image parsing typically employ separate standalone modules for the constituent semantic and instance segmentation tasks and require multiple passes of inference. Instead, the proposed DeeperLab image parser performs whole image parsing with a significantly simpler, fully convolutional approach that jointly addresses the semantic and instance segmentation tasks in a single-shot manner, resulting in a streamlined system that better lends itself to fast processing. For quantitative evaluation, we use both the instance-based Panoptic Quality (PQ) metric and the proposed region-based Parsing Covering (PC) metric, which better captures the image parsing quality on 'stuff' classes and larger object instances. We report experimental results on the challenging Mapillary Vistas dataset, in which our single model achieves 31.95\% (val) / 31.6\% PQ (test) and 55.26\% PC (val) with 3 frames per second (fps) on GPU or near real-time speed (22.6 fps on GPU) with reduced accuracy.},
  archivePrefix = {arXiv},
  eprint = {1902.05093},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DeeperLab-Yang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/H26MJSIU/1902.html},
  journal = {arXiv:1902.05093 [cs]},
  keywords = {2d segmentation},
  primaryClass = {cs}
}

@article{yangDenseRepPointsRepresenting2019,
  title = {Dense {{RepPoints}}: {{Representing Visual Objects}} with {{Dense Point Sets}}},
  shorttitle = {Dense {{RepPoints}}},
  author = {Yang, Ze and Xu, Yinghao and Xue, Han and Zhang, Zheng and Urtasun, Raquel and Wang, Liwei and Lin, Stephen and Hu, Han},
  year = {2019},
  month = dec,
  abstract = {We present an object representation, called \textbackslash textbf\{Dense RepPoints\}, for flexible and detailed modeling of object appearance and geometry. In contrast to the coarse geometric localization and feature extraction of bounding boxes, Dense RepPoints adaptively distributes a dense set of points to semantically and geometrically significant positions on an object, providing informative cues for object analysis. Techniques are developed to address challenges related to supervised training for dense point sets from image segments annotations and making this extensive representation computationally practical. In addition, the versatility of this representation is exploited to model object structure over multiple levels of granularity. Dense RepPoints significantly improves performance on geometrically-oriented visual understanding tasks, including a \$1.6\$ AP gain in object detection on the challenging COCO benchmark.},
  archivePrefix = {arXiv},
  eprint = {1912.11473},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Dense RepPoints-Yang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QJV64M5G/1912.html},
  journal = {arXiv:1912.11473 [cs]},
  primaryClass = {cs}
}

@inproceedings{yangDirectMonocularOdometry2017,
  title = {Direct Monocular Odometry Using Points and Lines},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2017},
  month = may,
  pages = {3871--3877},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/ICRA.2017.7989446},
  abstract = {Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-theart monocular odometry methods. In some challenging textureless environments, our algorithm reduces the state estimation error over 50\%.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Direct monocular odometry using points and lines-Yang_Scherer-2017.pdf},
  isbn = {978-1-5090-4633-1},
  language = {en}
}

@article{yangExtremeRelativePose2018,
  title = {Extreme {{Relative Pose Estimation}} for {{RGB}}-{{D Scans}} via {{Scene Completion}}},
  author = {Yang, Zhenpei and Pan, Jeffrey Z. and Luo, Linjie and Zhou, Xiaowei and Grauman, Kristen and Huang, Qixing},
  year = {2018},
  month = dec,
  abstract = {Estimating the relative rigid pose between two RGB-D scans of the same underlying environment is a fundamental problem in computer vision, robotics, and computer graphics. Most existing approaches allow only limited maximum relative pose changes since they require considerable overlap between the input scans. We introduce a novel deep neural network that extends the scope to extreme relative poses, with little or even no overlap between the input scans. The key idea is to infer more complete scene information about the underlying environment and match on the completed scans. In particular, instead of only performing scene completion from each individual scan, our approach alternates between relative pose estimation and scene completion. This allows us to perform scene completion by utilizing information from both input scans at late iterations, resulting in better results for both scene completion and relative pose estimation. Experimental results on benchmark datasets show that our approach leads to considerable improvements over state-of-the-art approaches for relative pose estimation. In particular, our approach provides encouraging relative pose estimates even between non-overlapping scans.},
  archivePrefix = {arXiv},
  eprint = {1901.00063},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Extreme Relative Pose Estimation for RGB-D Scans via Scene Completion-Yang et al-2018.pdf},
  journal = {arXiv:1901.00063 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangIPODIntensivePointbased2018,
  title = {{{IPOD}}: {{Intensive Point}}-Based {{Object Detector}} for {{Point Cloud}}},
  shorttitle = {{{IPOD}}},
  author = {Yang, Zetong and Sun, Yanan and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},
  year = {2018},
  month = dec,
  abstract = {We present a novel 3D object detection framework, named IPOD, based on raw point cloud. It seeds object proposal for each point, which is the basic element. This paradigm provides us with high recall and high fidelity of information, leading to a suitable way to process point cloud data. We design an end-to-end trainable architecture, where features of all points within a proposal are extracted from the backbone network and achieve a proposal feature for final bounding inference. These features with both context information and precise point cloud coordinates yield improved performance. We conduct experiments on KITTI dataset, evaluating our performance in terms of 3D object detection, Bird's Eye View (BEV) detection and 2D object detection. Our method accomplishes new state-of-the-art , showing great advantage on the hard set.},
  archivePrefix = {arXiv},
  eprint = {1812.05276},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/IPOD-Yang et al-2018.pdf},
  journal = {arXiv:1812.05276 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangLearningObjectBounding2019a,
  ids = {yangLearningObjectBounding2019},
  title = {Learning {{Object Bounding Boxes}} for {{3D Instance Segmentation}} on {{Point Clouds}}},
  author = {Yang, Bo and Wang, Jianan and Clark, Ronald and Hu, Qingyong and Wang, Sen and Markham, Andrew and Trigoni, Niki},
  year = {2019},
  month = jun,
  abstract = {We propose a novel, conceptually simple and general framework for instance segmentation on 3D point clouds. Our method, called 3D-BoNet, follows the simple design philosophy of per-point multilayer perceptrons (MLPs). The framework directly regresses 3D bounding boxes for all instances in a point cloud, while simultaneously predicting a point-level mask for each instance. It consists of a backbone network followed by two parallel network branches for 1) bounding box regression and 2) point mask prediction. 3D-BoNet is single-stage, anchor-free and end-to-end trainable. Moreover, it is remarkably computationally efficient as, unlike existing approaches, it does not require any post-processing steps such as non-maximum suppression, feature sampling, clustering or voting. Extensive experiments show that our approach surpasses existing work on both ScanNet and S3DIS datasets while being approximately 10x more computationally efficient. Comprehensive ablation studies demonstrate the effectiveness of our design.},
  archivePrefix = {arXiv},
  eprint = {1906.01140},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds-Yang et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds-Yang et al-22.pdf;/Users/sunjiaming/Zotero/storage/FKLBHKXV/1906.html;/Users/sunjiaming/Zotero/storage/QDF6AH3I/1906.html},
  journal = {arXiv:1906.01140 [cs]},
  primaryClass = {cs}
}

@article{yangMetaAnchorLearningDetect2018,
  title = {{{MetaAnchor}}: {{Learning}} to {{Detect Objects}} with {{Customized Anchors}}},
  shorttitle = {{{MetaAnchor}}},
  author = {Yang, Tong and Zhang, Xiangyu and Li, Zeming and Zhang, Wenqiang and Sun, Jian},
  year = {2018},
  month = jul,
  abstract = {We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios.},
  archivePrefix = {arXiv},
  eprint = {1807.00980},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/MetaAnchor-Yang et al-2018.pdf},
  journal = {arXiv:1807.00980 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangMobile3DReconRealtimeMonocular2020,
  title = {{{Mobile3DRecon}}: {{Real}}-Time {{Monocular 3D Reconstruction}} on a {{Mobile Phone}}},
  shorttitle = {{{Mobile3DRecon}}},
  author = {Yang, Xingbin and Zhou, Liyang and Jiang, Hanqing and Tang, Zhongliang and Wang, Yuanbo and Bao, Hujun and Zhang, Guofeng},
  year = {2020},
  pages = {1--1},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2020.3023634},
  abstract = {We present a real-time monocular 3D reconstruction system on a mobile phone, called Mobile3DRecon. Using an embedded monocular camera, our system provides an online mesh generation capability on back end together with real-time 6DoF pose tracking on front end for users to achieve realistic AR effects and interactions on mobile phones. Unlike most existing state-of-the-art systems which produce only point cloud based 3D models online or surface mesh offline, we propose a novel online incremental mesh generation approach to achieve fast online dense surface mesh reconstruction to satisfy the demand of real-time AR applications. For each keyframe of 6DoF tracking, we perform a robust monocular depth estimation, with a multi-view semi-global matching method followed by a depth refinement post-processing. The proposed mesh generation module incrementally fuses each estimated keyframe depth map to an online dense surface mesh, which is useful for achieving realistic AR effects such as occlusions and collisions. We verify our real-time reconstruction results on two mid-range mobile platforms. The experiments with quantitative and qualitative evaluation demonstrate the effectiveness of the proposed monocular 3D reconstruction system, which can handle the occlusions and collisions between virtual objects and real scenes to achieve realistic AR effects.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Mobile3DRecon-Yang et al-2020.pdf},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  language = {en}
}

@article{yangMonocularObjectPlane2018,
  title = {Monocular {{Object}} and {{Plane SLAM}} in {{Structured Environments}}},
  author = {Yang, Shichao and Scherer, Sebastian},
  year = {2018},
  month = sep,
  abstract = {We present a monocular Simultaneous Localization and Mapping (SLAM) using high level object and plane landmarks, in addition to points. The resulting map is denser, more compact and meaningful compared to point only SLAM. We first propose a high order graphical model to jointly infer the 3D object and layout planes from single image considering occlusions and semantic constraints. The extracted cuboid object and layout planes are further optimized in a unified SLAM framework. Objects and planes can provide more semantic constraints such as Manhattan and object supporting relationships compared to points. Experiments on various public and collected datasets including ICL NUIM and TUM mono show that our algorithm can improve camera localization accuracy compared to state-of-the-art SLAM and also generate dense maps in many structured environments.},
  archivePrefix = {arXiv},
  eprint = {1809.03415},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Monocular Object and Plane SLAM in Structured Environments-Yang_Scherer-2018.pdf},
  journal = {arXiv:1809.03415 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangPIXORRealTime3D,
  title = {{{PIXOR}}: {{Real}}-{{Time 3D Object Detection From Point Clouds}}},
  author = {Yang, Bin and Luo, Wenjie and Urtasun, Raquel},
  pages = {9},
  abstract = {We address the problem of real-time 3D object detection from point clouds in the context of autonomous driving. Speed is critical as detection is a necessary component for safety. Existing approaches are, however, expensive in computation due to high dimensionality of point clouds. We utilize the 3D data more efficiently by representing the scene from the Bird's Eye View (BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs oriented 3D object estimates decoded from pixel-wise neural network predictions. The input representation, network architecture, and model optimization are specially designed to balance high accuracy and real-time efficiency. We validate PIXOR on two datasets: the KITTI BEV object detection benchmark, and a large-scale 3D vehicle detection benchmark. In both datasets we show that the proposed detector surpasses other state-of-the-art methods notably in terms of Average Precision (AP), while still runs at 10 FPS.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/PIXOR-Yang et al-.pdf},
  language = {en}
}

@inproceedings{yangRealtimeMonocularDense2017,
  title = {Real-Time Monocular Dense Mapping on Aerial Robots Using Visual-Inertial Fusion},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Yang, Zhenfei and Gao, Fei and Shen, Shaojie},
  year = {2017},
  month = may,
  pages = {4552--4559},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/ICRA.2017.7989529},
  abstract = {In this work, we present a solution to real-time monocular dense mapping. A tightly-coupled visual-inertial localization module is designed to provide metric and highaccuracy odometry. A motion stereo algorithm is proposed to take the video input from one camera to produce local depth measurements with semi-global regularization. The local measurements are then integrated into a global map for noise filtering and map refinement. The global map obtained is able to support navigation and obstacle avoidance for aerial robots through our indoor and outdoor experimental verification. Our system runs at 10Hz on an Nvidia Jetson TX1 by properly distributing computation to CPU and GPU. Through onboard experiments, we demonstrate its ability to close the perceptionaction loop for autonomous aerial robots. We release our implementation as open-source software1.},
  file = {/Users/sunjiaming/Zotero/storage/GRMIV8FM/Yang et al. - 2017 - Real-time monocular dense mapping on aerial robots.pdf},
  isbn = {978-1-5090-4633-1},
  keywords = {neufu_paper},
  language = {en}
}

@article{yangRepPointsPointSet2019,
  ids = {yangRepPointsPointSet2019a},
  title = {{{RepPoints}}: {{Point Set Representation}} for {{Object Detection}}},
  shorttitle = {{{RepPoints}}},
  author = {Yang, Ze and Liu, Shaohui and Hu, Han and Wang, Liwei and Lin, Stephen},
  year = {2019},
  month = apr,
  abstract = {Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present \textbackslash textbf\{RepPoints\} (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints, implemented without multi-scale training and testing, can be as effective as state-of-the-art anchor-based detection methods, with 42.8 AP and 65.0 \$AP\_\{50\}\$ on the COCO test-dev detection benchmark.},
  archivePrefix = {arXiv},
  eprint = {1904.11490},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RepPoints-Yang et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/RepPoints-Yang et al-22.pdf;/Users/sunjiaming/Zotero/storage/HXKCF5A6/1904.html;/Users/sunjiaming/Zotero/storage/MMLZSX9C/1904.html},
  journal = {arXiv:1904.11490 [cs]},
  primaryClass = {cs}
}

@article{yangSegStereoExploitingSemantic2018,
  title = {{{SegStereo}}: {{Exploiting Semantic Information}} for {{Disparity Estimation}}},
  shorttitle = {{{SegStereo}}},
  author = {Yang, Guorun and Zhao, Hengshuang and Shi, Jianping and Deng, Zhidong and Jia, Jiaya},
  year = {2018},
  month = jul,
  abstract = {Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves stateof-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets.},
  archivePrefix = {arXiv},
  eprint = {1807.11699},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SegStereo-Yang et al-2018.pdf},
  journal = {arXiv:1807.11699 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yangSTDSparsetoDense3D2019,
  title = {{{STD}}: {{Sparse}}-to-{{Dense 3D Object Detector}} for {{Point Cloud}}},
  shorttitle = {{{STD}}},
  author = {Yang, Zetong and Sun, Yanan and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya},
  year = {2019},
  month = jul,
  abstract = {We present a new two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point cloud as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a high recall with less computation compared with prior works. Then, PointsPool is applied for generating proposal features by transforming their interior point features from sparse expression to compact representation, which saves even more computation time. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method in terms of 3D object and Bird's Eye View (BEV) detection. Our method outperforms other state-of-the-arts by a large margin, especially on the hard set, with inference speed more than 10 FPS.},
  archivePrefix = {arXiv},
  eprint = {1907.10471},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/STD-Yang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/RLV8T5PR/1907.html},
  journal = {arXiv:1907.10471 [cs]},
  primaryClass = {cs}
}

@article{yangSurfelGANSynthesizingRealistic2020,
  title = {{{SurfelGAN}}: {{Synthesizing Realistic Sensor Data}} for {{Autonomous Driving}}},
  shorttitle = {{{SurfelGAN}}},
  author = {Yang, Zhenpei and Chai, Yuning and Anguelov, Dragomir and Zhou, Yin and Sun, Pei and Erhan, Dumitru and Rafferty, Sean and Kretzschmar, Henrik},
  year = {2020},
  month = may,
  abstract = {Autonomous driving system development is critically dependent on the ability to replay complex and diverse traffic scenarios in simulation. In such scenarios, the ability to accurately simulate the vehicle sensors such as cameras, lidar or radar is essential. However, current sensor simulators leverage gaming engines such as Unreal or Unity, requiring manual creation of environments, objects and material properties. Such approaches have limited scalability and fail to produce realistic approximations of camera, lidar, and radar data without significant additional work. In this paper, we present a simple yet effective approach to generate realistic scenario sensor data, based only on a limited amount of lidar and camera data collected by an autonomous vehicle. Our approach uses texture-mapped surfels to efficiently reconstruct the scene from an initial vehicle pass or set of passes, preserving rich information about object 3D geometry and appearance, as well as the scene conditions. We then leverage a SurfelGAN network to reconstruct realistic camera images for novel positions and orientations of the self-driving vehicle and moving objects in the scene. We demonstrate our approach on the Waymo Open Dataset and show that it can synthesize realistic camera data for simulated scenarios. We also create a novel dataset that contains cases in which two self-driving vehicles observe the same scene at the same time. We use this dataset to provide additional evaluation and demonstrate the usefulness of our SurfelGAN model.},
  archivePrefix = {arXiv},
  eprint = {2005.03844},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SurfelGAN-Yang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/SFWGLY9J/2005.html},
  journal = {arXiv:2005.03844 [cs]},
  primaryClass = {cs}
}

@article{yangUR2KiDUnifyingRetrieval2020,
  title = {{{UR2KiD}}: {{Unifying Retrieval}}, {{Keypoint Detection}}, and {{Keypoint Description}} without {{Local Correspondence Supervision}}},
  shorttitle = {{{UR2KiD}}},
  author = {Yang, Tsun-Yi and Nguyen, Duy-Kien and Heijnen, Huub and Balntas, Vassileios},
  year = {2020},
  month = jan,
  abstract = {In this paper, we explore how three related tasks, namely keypoint detection, description, and image retrieval can be jointly tackled using a single unified framework, which is trained without the need of training data with point to point correspondences. By leveraging diverse information from sequential layers of a standard ResNet-based architecture, we are able to extract keypoints and descriptors that encode local information using generic techniques such as local activation norms, channel grouping and dropping, and self-distillation. Subsequently, global information for image retrieval is encoded in an end-to-end pipeline, based on pooling of the aforementioned local responses. In contrast to previous methods in local matching, our method does not depend on pointwise/pixelwise correspondences, and requires no such supervision at all i.e. no depth-maps from an SfM model nor manually created synthetic affine transformations. We illustrate that this simple and direct paradigm, is able to achieve very competitive results against the state-of-the-art methods in various challenging benchmark conditions such as viewpoint changes, scale changes, and day-night shifting localization.},
  archivePrefix = {arXiv},
  eprint = {2001.07252},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/UR2KiD-Yang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/CLD6TQM2/2001.html},
  journal = {arXiv:2001.07252 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@incollection{yangVolumetricCorrespondenceNetworks2019,
  title = {Volumetric {{Correspondence Networks}} for {{Optical Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yang, Gengshan and Ramanan, Deva},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {794--805},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Volumetric Correspondence Networks for Optical Flow-Yang_Ramanan-2019.pdf;/Users/sunjiaming/Zotero/storage/N9ZADXBA/8367-volumetric-correspondence-networks-for-optical-flow.html}
}

@article{yanSECONDSparselyEmbedded2018,
  title = {{{SECOND}}: {{Sparsely Embedded Convolutional Detection}}},
  shorttitle = {{{SECOND}}},
  author = {Yan, Yan and Mao, Yuxing and Li, Bo},
  year = {2018},
  month = oct,
  volume = {18},
  pages = {3337},
  issn = {1424-8220},
  doi = {10.3390/s18103337},
  abstract = {LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SECOND-Yan et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/SECOND-Yan et al-22.pdf},
  journal = {Sensors},
  language = {en},
  number = {10}
}

@article{yaoBlendedMVSLargescaleDataset2019,
  title = {{{BlendedMVS}}: {{A Large}}-Scale {{Dataset}} for {{Generalized Multi}}-View {{Stereo Networks}}},
  shorttitle = {{{BlendedMVS}}},
  author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Zhang, Jingyang and Ren, Yufan and Zhou, Lei and Fang, Tian and Quan, Long},
  year = {2019},
  month = nov,
  abstract = {While deep learning has recently achieved great success on multi-view stereo (MVS), limited training data makes the trained model hard to be generalized to unseen scenarios. Compared with other computer vision tasks, it is rather difficult to collect a large-scale MVS dataset as it requires expensive active scanners and labor-intensive process to obtain ground truth 3D structures. In this paper, we introduce BlendedMVS, a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. To create the dataset, we apply a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, we render these mesh models to color images and depth maps. The rendered color images are further blended with the input images to generate photo-realistic blended images as the training input. Our dataset contains over 17k high-resolution images covering a variety of scenes, including cities, architectures, sculptures and small objects. Extensive experiments demonstrate that BlendedMVS endows the trained model with significantly better generalization ability compared with other MVS datasets. The entire dataset with pretrained models will be made publicly available at https://github.com/YoYo000/BlendedMVS.},
  archivePrefix = {arXiv},
  eprint = {1911.10127},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/BlendedMVS-Yao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/3XB3IVQB/1911.html},
  journal = {arXiv:1911.10127 [cs]},
  primaryClass = {cs}
}

@article{yaoMVSNetDepthInference2018,
  title = {{{MVSNet}}: {{Depth Inference}} for {{Unstructured Multi}}-View {{Stereo}}},
  shorttitle = {{{MVSNet}}},
  author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Fang, Tian and Quan, Long},
  year = {2018},
  month = apr,
  abstract = {We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.},
  archivePrefix = {arXiv},
  eprint = {1804.02505},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/MVSNet-Yao et al-2018.pdf},
  journal = {arXiv:1804.02505 [cs]},
  keywords = {neufu_paper},
  language = {en},
  primaryClass = {cs}
}

@article{yaoRecurrentMVSNetHighresolution2019,
  title = {Recurrent {{MVSNet}} for {{High}}-Resolution {{Multi}}-View {{Stereo Depth Inference}}},
  author = {Yao, Yao and Luo, Zixin and Li, Shiwei and Shen, Tianwei and Fang, Tian and Quan, Long},
  year = {2019},
  month = feb,
  abstract = {Deep learning has recently demonstrated its excellent performance for multi-view stereo (MVS). However, one major limitation of current learned MVS approaches is the scalability: the memory-consuming cost volume regularization makes the learned MVS hard to be applied to high-resolution scenes. In this paper, we introduce a scalable multi-view stereo framework based on the recurrent neural network. Instead of regularizing the entire 3D cost volume in one go, the proposed Recurrent Multi-view Stereo Network (R-MVSNet) sequentially regularizes the 2D cost maps along the depth direction via the gated recurrent unit (GRU). This reduces dramatically the memory consumption and makes high-resolution reconstruction feasible. We first show the state-of-the-art performance achieved by the proposed R-MVSNet on the recent MVS benchmarks. Then, we further demonstrate the scalability of the proposed method on several large-scale scenarios, where previous learned approaches often fail due to the memory constraint. Code is available at https://github.com/YoYo000/MVSNet.},
  archivePrefix = {arXiv},
  eprint = {1902.10556},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference-Yao et al-2019.pdf;/Users/sunjiaming/Zotero/storage/4KF5DTKD/1902.html},
  journal = {arXiv:1902.10556 [cs]},
  primaryClass = {cs}
}

@article{yarivMultiviewNeuralSurface2020,
  title = {Multiview {{Neural Surface Reconstruction}} with {{Implicit Lighting}} and {{Material}}},
  author = {Yariv, Lior and Kasten, Yoni and Moran, Dror and Galun, Meirav and Atzmon, Matan and Basri, Ronen and Lipman, Yaron},
  year = {2020},
  month = jun,
  abstract = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
  archivePrefix = {arXiv},
  eprint = {2003.09852},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Multiview Neural Surface Reconstruction with Implicit Lighting and Material-Yariv et al-2020.pdf;/Users/sunjiaming/Zotero/storage/VBJQUYYA/2003.html},
  journal = {arXiv:2003.09852 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,neufu_paper},
  primaryClass = {cs}
}

@article{yarivUniversalDifferentiableRenderer2020,
  title = {Universal {{Differentiable Renderer}} for {{Implicit Neural Representations}}},
  author = {Yariv, Lior and Atzmon, Matan and Lipman, Yaron},
  year = {2020},
  month = mar,
  abstract = {The goal of this work is to learn implicit 3D shape representation with 2D supervision (i.e., a collection of images). To that end we introduce the Universal Differentiable Renderer (UDR) a neural network architecture that can provably approximate reflected light from an implicit neural representation of a 3D surface, under a wide set of reflectance properties and lighting conditions. Experimenting with the task of multiview 3D reconstruction, we find our model to improve upon the baselines in the accuracy of the reconstructed 3D geometry and rendering from unseen viewing directions.},
  archivePrefix = {arXiv},
  eprint = {2003.09852},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Universal Differentiable Renderer for Implicit Neural Representations-Yariv et al-2020.pdf;/Users/sunjiaming/Zotero/storage/KI7Q6V3L/2003.html},
  journal = {arXiv:2003.09852 [cs]},
  primaryClass = {cs}
}

@article{yew3DFeatNetWeaklySupervised2018,
  title = {{{3DFeat}}-{{Net}}: {{Weakly Supervised Local 3D Features}} for {{Point Cloud Registration}}},
  shorttitle = {{{3DFeat}}-{{Net}}},
  author = {Yew, Zi Jian and Lee, Gim Hee},
  year = {2018},
  month = jul,
  abstract = {In this paper, we propose the 3DFeat-Net which learns both 3D feature detector and descriptor for point cloud matching using weak supervision. Unlike many existing works, we do not require manual annotation of matching point clusters. Instead, we leverage on alignment and attention mechanisms to learn feature correspondences from GPS/INS tagged 3D point clouds without explicitly specifying them. We create training and benchmark outdoor Lidar datasets, and experiments show that 3DFeat-Net obtains state-of-the-art performance on these gravity-aligned datasets.},
  archivePrefix = {arXiv},
  eprint = {1807.09413},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3DFeat-Net-Yew_Lee-2018.pdf;/Users/sunjiaming/Zotero/storage/2HC4NSRW/1807.html},
  journal = {arXiv:1807.09413 [cs]},
  keywords = {3d registration},
  primaryClass = {cs}
}

@article{yiGSPNGenerativeShape2018,
  title = {{{GSPN}}: {{Generative Shape Proposal Network}} for {{3D Instance Segmentation}} in {{Point Cloud}}},
  shorttitle = {{{GSPN}}},
  author = {Yi, Li and Zhao, Wang and Wang, He and Sung, Minhyuk and Guibas, Leonidas},
  year = {2018},
  month = dec,
  abstract = {We introduce a novel 3D object proposal approach named Generative Shape Proposal Network (GSPN) for instance segmentation in point cloud data. Instead of treating object proposal as a direct bounding box regression problem, we take an analysis-by-synthesis strategy and generate proposals by reconstructing shapes from noisy observations in a scene. We incorporate GSPN into a novel 3D instance segmentation framework named Region-based PointNet (R-PointNet) which allows flexible proposal refinement and instance segmentation generation. We achieve state-of-the-art performance on several 3D instance segmentation tasks. The success of GSPN largely comes from its emphasis on geometric understandings during object proposal, which greatly reducing proposals with low objectness.},
  archivePrefix = {arXiv},
  eprint = {1812.03320},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GSPN-Yi et al-2018.pdf;/Users/sunjiaming/Zotero/storage/VGFTUSXI/1812.html},
  journal = {arXiv:1812.03320 [cs]},
  primaryClass = {cs}
}

@article{yiLargeScale3DShape2017,
  title = {Large-{{Scale 3D Shape Reconstruction}} and {{Segmentation}} from {{ShapeNet Core55}}},
  author = {Yi, Li and Shao, Lin and Savva, Manolis and Huang, Haibin and Zhou, Yang and Wang, Qirui and Graham, Benjamin and Engelcke, Martin and Klokov, Roman and Lempitsky, Victor and Gan, Yuan and Wang, Pengyu and Liu, Kun and Yu, Fenggen and Shui, Panpan and Hu, Bingyang and Zhang, Yan and Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Jeong, Minki and Choi, Jaehoon and Kim, Changick and Geetchandra, Angom and Murthy, Narasimha and Ramu, Bhargava and Manda, Bharadwaj and Ramanathan, M. and Kumar, Gautam and Preetham, P. and Srivastava, Siddharth and Bhugra, Swati and Lall, Brejesh and Haene, Christian and Tulsiani, Shubham and Malik, Jitendra and Lafer, Jared and Jones, Ramsey and Li, Siyuan and Lu, Jie and Jin, Shi and Yu, Jingyi and Huang, Qixing and Kalogerakis, Evangelos and Savarese, Silvio and Hanrahan, Pat and Funkhouser, Thomas and Su, Hao and Guibas, Leonidas},
  year = {2017},
  month = oct,
  abstract = {We introduce a large-scale 3D shape understanding benchmark using data and annotation from ShapeNet 3D object database. The benchmark consists of two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. Ten teams have participated in the challenge and the best performing teams have outperformed state-of-the-art approaches on both tasks. A few novel deep learning architectures have been proposed on various 3D representations on both tasks. We report the techniques used by each team and the corresponding performances. In addition, we summarize the major discoveries from the reported results and possible trends for the future work in the field.},
  archivePrefix = {arXiv},
  eprint = {1710.06104},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55-Yi et al-2017.pdf;/Users/sunjiaming/Zotero/storage/VD3WYVUD/1710.html},
  journal = {arXiv:1710.06104 [cs]},
  primaryClass = {cs}
}

@article{yiLearningFindGood2018,
  title = {Learning to {{Find Good Correspondences}}},
  author = {Yi, Kwang Moo and Trulls, Eduard and Ono, Yuki and Lepetit, Vincent and Salzmann, Mathieu and Fua, Pascal},
  year = {2018},
  month = may,
  abstract = {We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while imbuing it with global information, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data.},
  archivePrefix = {arXiv},
  eprint = {1711.05971},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Find Good Correspondences-Yi et al-22.pdf;/Users/sunjiaming/Zotero/storage/47V4TSSZ/1711.html},
  journal = {arXiv:1711.05971 [cs]},
  primaryClass = {cs}
}

@article{yinCenterbased3DObject2020,
  title = {Center-Based {{3D Object Detection}} and {{Tracking}}},
  author = {Yin, Tianwei and Zhou, Xingyi and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2020},
  month = jun,
  abstract = {Three-dimensional objects are commonly represented as 3D boxes in a point-cloud. This representation mimics the well-studied image-based 2D bounding-box detection but comes with additional challenges. Objects in a 3D world do not follow any particular orientation, and box-based detectors have difficulties enumerating all orientations or fitting an axis-aligned bounding box to rotated objects. In this paper, we instead propose to represent, detect, and track 3D objects as points. We use a keypoint detector to find centers of objects and simply regress to other attributes, including 3D size, 3D orientation, and velocity. In our center-based framework, 3D object tracking simplifies to greedy closest-point matching. The resulting detection and tracking algorithm is simple, efficient, and effective. On the nuScenes dataset, our point-based representations perform \$3\$-\$4\$ mAP higher than the box-based counterparts for 3D detection, and 6 AMOTA higher for 3D tracking. Our real-time model runs end-to-end 3D detection and tracking at \$30\$ FPS with \$54.2\$ AMOTA and \$48.3\$ mAP while the best single model achieves \$60.3\$ mAP for 3D detection and \$63.8\$ AMOTA for 3D tracking. The code and pretrained models are available at https://github.com/tianweiy/CenterPoint.},
  archivePrefix = {arXiv},
  eprint = {2006.11275},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Center-based 3D Object Detection and Tracking-Yin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/CL2A4K87/2006.html},
  journal = {arXiv:2006.11275 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{yinHierarchicalDiscreteDistribution2018,
  title = {Hierarchical {{Discrete Distribution Decomposition}} for {{Match Density Estimation}}},
  author = {Yin, Zhichao and Darrell, Trevor and Yu, Fisher},
  year = {2018},
  month = dec,
  abstract = {Existing deep learning methods for pixel correspondence output a point estimate of the motion field, but do not represent the full match distribution. Explicit representation of a match distribution is desirable for many applications as it allows direct representation of the correspondence probability. The main difficulty of estimating a full probability distribution with a deep network is the high computational cost of inferring the entire distribution. In this paper, we propose Hierarchical Discrete Distribution Decomposition, dubbed HD3, to learn probabilistic point and region matching. Not only can it model match uncertainty, but also region propagation. To achieve this, we estimate the hierarchical distribution of pixel correspondences at different image scales without multi-hypotheses ensembling. Despite its simplicity, our method can achieve competitive results for both optical flow and stereo matching on established benchmarks, while the estimated uncertainty is a good indicator of errors. Furthermore, the point match distribution within a region can be grouped together to propagate the whole region even if the area changes across images.},
  archivePrefix = {arXiv},
  eprint = {1812.06264},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Hierarchical Discrete Distribution Decomposition for Match Density Estimation-Yin et al-2018.pdf},
  journal = {arXiv:1812.06264 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yinLiDARbasedOnline3D2020,
  title = {{{LiDAR}}-Based {{Online 3D Video Object Detection}} with {{Graph}}-Based {{Message Passing}} and {{Spatiotemporal Transformer Attention}}},
  author = {Yin, Junbo and Shen, Jianbing and Guan, Chenye and Zhou, Dingfu and Yang, Ruigang},
  year = {2020},
  month = apr,
  abstract = {Existing LiDAR-based 3D object detectors usually focus on the single-frame detection, while ignoring the spatiotemporal information in consecutive point cloud frames. In this paper, we propose an end-to-end online 3D video object detector that operates on point cloud sequences. The proposed model comprises a spatial feature encoding component and a spatiotemporal feature aggregation component. In the former component, a novel Pillar Message Passing Network (PMPNet) is proposed to encode each discrete point cloud frame. It adaptively collects information for a pillar node from its neighbors by iterative message passing, which effectively enlarges the receptive field of the pillar feature. In the latter component, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU) to aggregate the spatiotemporal information, which enhances the conventional ConvGRU with an attentive memory gating mechanism. AST-GRU contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module, which can emphasize the foreground objects and align the dynamic objects, respectively. Experimental results demonstrate that the proposed 3D video object detector achieves state-of-the-art performance on the large-scale nuScenes benchmark.},
  archivePrefix = {arXiv},
  eprint = {2004.01389},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing-Yin et al-2020.pdf;/Users/sunjiaming/Zotero/storage/4NJTM29M/2004.html},
  journal = {arXiv:2004.01389 [cs]},
  primaryClass = {cs}
}

@article{yokozukaVITAMINEVIsualTracking2019,
  title = {{{VITAMIN}}-{{E}}: {{VIsual Tracking And MappINg}} with {{Extremely Dense Feature Points}}},
  shorttitle = {{{VITAMIN}}-{{E}}},
  author = {Yokozuka, Masashi and Oishi, Shuji and Simon, Thompson and Banno, Atsuhiko},
  year = {2019},
  month = apr,
  abstract = {In this paper, we propose a novel indirect monocular SLAM algorithm called "VITAMIN-E," which is highly accurate and robust as a result of tracking extremely dense feature points. Typical indirect methods have difficulty in reconstructing dense geometry because of their careful feature point selection for accurate matching. Unlike conventional methods, the proposed method processes an enormous number of feature points by tracking the local extrema of curvature informed by dominant flow estimation. Because this may lead to high computational cost during bundle adjustment, we propose a novel optimization technique, the "subspace Gauss--Newton method", that significantly improves the computational efficiency of bundle adjustment by partially updating the variables. We concurrently generate meshes from the reconstructed points and merge them for an entire 3D model. The experimental results on the SLAM benchmark dataset EuRoC demonstrated that the proposed method outperformed state-of-the-art SLAM methods, such as DSO, ORB-SLAM, and LSD-SLAM, both in terms of accuracy and robustness in trajectory estimation. The proposed method simultaneously generated significantly detailed 3D geometry from the dense feature points in real time using only a CPU.},
  archivePrefix = {arXiv},
  eprint = {1904.10324},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/VITAMIN-E-Yokozuka et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9H6K5PN6/1904.html},
  journal = {arXiv:1904.10324 [cs]},
  keywords = {slam},
  primaryClass = {cs}
}

@article{youGraphStructureNeural2020,
  title = {Graph {{Structure}} of {{Neural Networks}}},
  author = {You, Jiaxuan and Leskovec, Jure and He, Kaiming and Xie, Saining},
  year = {2020},
  month = jul,
  abstract = {Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.},
  archivePrefix = {arXiv},
  eprint = {2007.06559},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Graph Structure of Neural Networks-You et al-2020.pdf;/Users/sunjiaming/Zotero/storage/9WDEP9N7/2007.html},
  journal = {arXiv:2007.06559 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{youPseudoLiDARAccurateDepth2019,
  title = {Pseudo-{{LiDAR}}++: {{Accurate Depth}} for {{3D Object Detection}} in {{Autonomous Driving}}},
  shorttitle = {Pseudo-{{LiDAR}}++},
  author = {You, Yurong and Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Pleiss, Geoff and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  year = {2019},
  month = jun,
  abstract = {Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of far away objects (currently the primary weakness of pseudo-LiDAR). Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for far-away objects by 40\%. Our code will be publicly available at https://github.com/mileyan/Pseudo\_Lidar\_V2.},
  archivePrefix = {arXiv},
  eprint = {1906.06310},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Pseudo-LiDAR++-You et al-2019.pdf;/Users/sunjiaming/Zotero/storage/TRGLI2CQ/1906.html},
  journal = {arXiv:1906.06310 [cs]},
  primaryClass = {cs}
}

@article{yuanOCNetObjectContext2018,
  title = {{{OCNet}}: {{Object Context Network}} for {{Scene Parsing}}},
  shorttitle = {{{OCNet}}},
  author = {Yuan, Yuhui and Wang, Jingdong},
  year = {2018},
  month = sep,
  abstract = {In this paper, we address the problem of scene parsing with deep learning and focus on the context aggregation strategy for robust segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we introduce an \textbackslash emph\{object context pooling (OCP)\} scheme, which represents each pixel by exploiting the set of pixels that belong to the same object category with such a pixel, and we call the set of pixels as object context. Our implementation, inspired by the self-attention approach, consists of two steps: (i) compute the similarities between each pixel and all the pixels, forming a so-called object context map for each pixel served as a surrogate for the true object context, and (ii) represent the pixel by aggregating the features of all the pixels weighted by the similarities. The resulting representation is more robust compared to existing context aggregation schemes, e.g., pyramid pooling modules (PPM) in PSPNet and atrous spatial pyramid pooling (ASPP), which do not differentiate the context pixels belonging to the same object category or not, making the reliability of contextually aggregated representations limited. We empirically demonstrate our approach and two pyramid extensions with state-of-the-art performance on three semantic segmentation benchmarks: Cityscapes, ADE20K and LIP. Code has been made available at: https://github.com/PkuRainBow/OCNet.},
  archivePrefix = {arXiv},
  eprint = {1809.00916},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/OCNet-Yuan_Wang-2018.pdf;/Users/sunjiaming/Zotero/storage/NSMRDM67/1809.html},
  journal = {arXiv:1809.00916 [cs]},
  primaryClass = {cs}
}

@article{yuanPCNPointCompletion2018,
  title = {{{PCN}}: {{Point Completion Network}}},
  shorttitle = {{{PCN}}},
  author = {Yuan, Wentao and Khot, Tejas and Held, David and Mertz, Christoph and Hebert, Martial},
  year = {2018},
  month = aug,
  abstract = {Shape completion, the problem of estimating the complete geometry of objects from partial observations, lies at the core of many vision and robotics applications. In this work, we propose Point Completion Network (PCN), a novel learning-based approach for shape completion. Unlike existing shape completion methods, PCN directly operates on raw point clouds without any structural assumption (e.g. symmetry) or annotation (e.g. semantic class) about the underlying shape. It features a decoder design that enables the generation of fine-grained completions while maintaining a small number of parameters. Our experiments show that PCN produces dense, complete point clouds with realistic structures in the missing regions on inputs with various levels of incompleteness and noise, including cars from LiDAR scans in the KITTI dataset.},
  archivePrefix = {arXiv},
  eprint = {1808.00671},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/PCN-Yuan et al-2018.pdf;/Users/sunjiaming/Zotero/storage/HQRR6R9I/1808.html},
  journal = {arXiv:1808.00671 [cs]},
  primaryClass = {cs}
}

@article{yuLearningDense3D,
  title = {Learning {{Dense 3D Models}} from {{Monocular Video}}},
  author = {Yu, Rui},
  pages = {133},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Learning Dense 3D Models from Monocular Video-Yu-.pdf},
  language = {en}
}

@article{yuLinebasedCameraPose2019,
  title = {Line-Based {{Camera Pose Estimation}} in {{Point Cloud}} of {{Structured Environments}}},
  author = {Yu, Huai and Zhen, Weikun and Yang, Wen and Scherer, Sebastian},
  year = {2019},
  month = nov,
  abstract = {Accurate registration of 2D imagery with point clouds is a key technology for imagery-LiDAR point cloud fusion, camera to laser scanner calibration and camera localization. Despite continuous improvements, automatic registration of 2D and 3D data without using additional textured information still faces great challenges. In this paper, we propose a new 2D-3D registration method to estimate 2D-3D line feature correspondences and the camera pose in untextured point clouds of structured environments. Specifically, we first use geometric constraints between vanishing points and 3D parallel lines to compute all feasible camera rotations. Then, we utilize a hypothesis testing strategy to estimate the 2D-3D line correspondences and the translation vector. By checking the consistency with computed correspondences, the best rotation matrix can be found. Finally, the camera pose is further refined using non-linear optimization with all the 2D-3D line correspondences. The experiments demonstrate the effectiveness of the proposed method on the synthetic and real dataset (outdoors and indoors) with repeated structures and rapid depth changes.},
  archivePrefix = {arXiv},
  eprint = {1912.05013},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Line-based Camera Pose Estimation in Point Cloud of Structured Environments-Yu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/V8IQ3K2F/1912.html},
  journal = {arXiv:1912.05013 [cs]},
  primaryClass = {cs}
}

@article{yuNotOnlyLook2019,
  title = {Not {{Only Look But Observe}}: {{Variational Observation Model}} of {{Scene}}-{{Level 3D Multi}}-{{Object Understanding}} for {{Probabilistic SLAM}}},
  shorttitle = {Not {{Only Look But Observe}}},
  author = {Yu, Hyeonwoo and Lee, Beomhee},
  year = {2019},
  month = jul,
  abstract = {We present NOLBO, a variational observation model estimation for 3D multi-object from 2D single shot. Previous probabilistic instance-level understandings mainly consider the single-object image, not single shot with multi-object; relations between objects and the entire scene are out of their focus. The objectness of each observation also hardly join their model. Therefore, we propose a method to approximate the Bayesian observation model of scene-level 3D multi-object understanding. By exploiting variational auto-encoder (VAE), we estimate latent variables from the entire scene, which follow tractable distributions and concurrently imply 3D full shape and pose. To perform object-oriented data association and probabilistic simultaneous localization and mapping (SLAM), our observation models can easily be adopted to probabilistic inference by replacing object-oriented features with latent variables.},
  archivePrefix = {arXiv},
  eprint = {1907.09760},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Not Only Look But Observe-Yu_Lee-2019.pdf;/Users/sunjiaming/Zotero/storage/E72MVWZJ/1907.html},
  journal = {arXiv:1907.09760 [cs]},
  primaryClass = {cs}
}

@article{zadehVariationalAutoDecoder2019,
  ids = {zadehVariationalAutoDecoder2019a},
  title = {Variational {{Auto}}-{{Decoder}}},
  author = {Zadeh, Amir and Lim, Yao-Chong and Liang, Paul Pu and Morency, Louis-Philippe},
  year = {2019},
  month = mar,
  abstract = {Learning a generative model from partial data (data with missingness) is a challenging area of machine learning research. We study a specific implementation of the Auto-Encoding Variational Bayes (AEVB) algorithm, named in this paper as a Variational Auto-Decoder (VAD). VAD is a generic framework which uses Variational Bayes and Markov Chain Monte Carlo (MCMC) methods to learn a generative model from partial data. The main distinction between VAD and Variational Auto-Encoder (VAE) is the encoder component, as VAD does not have one. Using a proposed efficient inference method from a multivariate Gaussian approximate posterior, VAD models allow inference to be performed via simple gradient ascent rather than MCMC sampling from a probabilistic decoder. This technique reduces the inference computational cost, allows for using more complex optimization techniques during latent space inference (which are shown to be crucial due to a high degree of freedom in the VAD latent space), and keeps the framework simple to implement. Through extensive experiments over several datasets and different missing ratios, we show that encoders cannot efficiently marginalize the input volatility caused by imputed missing values. We study multimodal datasets in this paper, which is a particular area of impact for VAD models.},
  archivePrefix = {arXiv},
  eprint = {1903.00840},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Variational Auto-Decoder-Zadeh et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Variational Auto-Decoder-Zadeh et al-22.pdf;/Users/sunjiaming/Zotero/storage/C6QU6L79/1903.html;/Users/sunjiaming/Zotero/storage/QV4CJMPH/1903.html},
  journal = {arXiv:1903.00840 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zakharovAutolabeling3DObjects2019,
  title = {Autolabeling {{3D Objects}} with {{Differentiable Rendering}} of {{SDF Shape Priors}}},
  author = {Zakharov, Sergey and Kehl, Wadim and Bhargava, Arjun and Gaidon, Adrien},
  year = {2019},
  month = nov,
  abstract = {We present an automatic annotation pipeline to recover 9D cuboids and 3D shape from pre-trained off-the-shelf 2D detectors and sparse LIDAR data. Our autolabeling method solves this challenging ill-posed inverse problem by relying on learned shape priors and optimization of geometric and physical parameters. To that end, we propose a novel differentiable shape renderer over signed distance fields (SDF), which we leverage in combination with normalized object coordinate spaces (NOCS). Initially trained on synthetic data to predict shape and coordinates, our method uses these predictions for projective and geometrical alignment over real samples. We also propose a curriculum learning strategy, iteratively retraining on samples of increasing difficulty for subsequent self-improving annotation rounds. Our experiments on the KITTI3D dataset show that we can recover a substantial amount of accurate cuboids, and that these autolabels can be used to train 3D vehicle detectors with state-of-the-art results. We will make the code publicly available soon.},
  archivePrefix = {arXiv},
  eprint = {1911.11288},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors-Zakharov et al-2019.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors-Zakharov et al-22.pdf;/Users/sunjiaming/Zotero/storage/WUZ4I5A7/1911.html},
  journal = {arXiv:1911.11288 [cs]},
  primaryClass = {cs}
}

@article{zakharovDPOD6DPose2019,
  title = {{{DPOD}}: {{6D Pose Object Detector}} and {{Refiner}}},
  shorttitle = {{{DPOD}}},
  author = {Zakharov, Sergey and Shugurov, Ivan and Ilic, Slobodan},
  year = {2019},
  month = aug,
  abstract = {In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.},
  archivePrefix = {arXiv},
  eprint = {1902.11020},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DPOD-Zakharov et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9CJJGIUQ/1902.html},
  journal = {arXiv:1902.11020 [cs]},
  primaryClass = {cs}
}

@article{zamirRobustLearningCrossTask2020,
  title = {Robust {{Learning Through Cross}}-{{Task Consistency}}},
  author = {Zamir, Amir and Sax, Alexander and Yeo, Teresa and Kar, O{\u g}uzhan and Cheerla, Nikhil and Suri, Rohan and Cao, Zhangjie and Malik, Jitendra and Guibas, Leonidas},
  year = {2020},
  month = jun,
  abstract = {Visual perception entails solving a wide set of tasks, e.g., object detection, depth estimation, etc. The predictions made for multiple tasks from the same image are not independent, and therefore, are expected to be `consistent'. We propose a broadly applicable and fully computational method for augmenting learning with Cross-Task Consistency.1 The proposed formulation is based on inference-path invariance over a graph of arbitrary tasks. We observe that learning with cross-task consistency leads to more accurate predictions and better generalization to out-of-distribution inputs. This framework also leads to an informative unsupervised quantity, called Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy correlates well with the supervised error (r=0.67), thus it can be employed as an unsupervised confidence metric as well as for detection of out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and they benchmark cross-task consistency versus various baselines including conventional multi-task learning, cycle consistency, and analytical consistency.},
  archivePrefix = {arXiv},
  eprint = {2006.04096},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/NHWSW55S/Zamir et al. - 2020 - Robust Learning Through Cross-Task Consistency.pdf},
  journal = {arXiv:2006.04096 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{zamorskiAdversarialAutoencodersGenerating2018,
  title = {Adversarial {{Autoencoders}} for {{Generating 3D Point Clouds}}},
  author = {Zamorski, Maciej and Zi{\k{e}}ba, Maciej and Nowak, Rafa{\l} and Stokowiec, Wojciech and Trzci{\'n}ski, Tomasz},
  year = {2018},
  month = nov,
  abstract = {Deep generative architectures provide a way to model not only images, but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for clustering and reconstruction. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers much wider portion of training data distribution, hence allowing smooth interpolation between the shapes. Finally, our extensive quantitative evaluation shows that 3dAAE provides state-of-the-art results on a set of benchmark tasks.},
  archivePrefix = {arXiv},
  eprint = {1811.07605},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Adversarial Autoencoders for Generating 3D Point Clouds-Zamorski et al-2018.pdf;/Users/sunjiaming/Zotero/storage/E7XSZNHC/1811.html},
  journal = {arXiv:1811.07605 [cs, stat]},
  keywords = {3d feature learning},
  primaryClass = {cs, stat}
}

@article{zarzarEfficientTrackingProposals2019,
  title = {Efficient {{Tracking Proposals}} Using {{2D}}-{{3D Siamese Networks}} on {{LIDAR}}},
  author = {Zarzar, Jesus and Giancola, Silvio and Ghanem, Bernard},
  year = {2019},
  month = mar,
  abstract = {Tracking vehicles in LIDAR point clouds is a challenging task due to the sparsity of the data and the dense search space. The lack of structure in point clouds impedes the use of convolution and correlation filters usually employed in 2D object tracking. In addition, structuring point clouds is cumbersome and implies losing fine-grained information. As a result, generating proposals in 3D space is expensive and inefficient. In this paper, we leverage the dense and structured Bird Eye View (BEV) representation of LIDAR point clouds to efficiently search for objects of interest. We use an efficient Region Proposal Network and generate a small number of object proposals in 3D. Successively, we refine our selection of 3D object candidates by exploiting the similarity capability of a 3D Siamese network. We regularize the latter 3D Siamese network for shape completion to enhance its discrimination capability. Our method attempts to solve both for an efficient search space in the BEV space and a meaningful selection using 3D LIDAR point cloud. We show that the Region Proposal in the BEV outperforms Bayesian methods such as Kalman and Particle Filters in providing proposal by a significant margin and that such candidates are suitable for the 3D Siamese network. By training our method end-to-end, we outperform the previous baseline in vehicle tracking by 12\% / 18\% in Success and Precision when using only 16 candidates.},
  archivePrefix = {arXiv},
  eprint = {1903.10168},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDAR-Zarzar et al-2019.pdf;/Users/sunjiaming/Zotero/storage/5T32PMQV/1903.html},
  journal = {arXiv:1903.10168 [cs]},
  keywords = {3d tracking},
  primaryClass = {cs}
}

@article{zeng3DCarRecogCarRecognition2019,
  title = {{{3DCarRecog}}: {{Car Recognition Using 3D Bounding Box}}},
  shorttitle = {{{3DCarRecog}}},
  author = {Zeng, Rui and Ge, Zongyuan and Denman, Simon and Sridharan, Sridha and Fookes, Clinton},
  year = {2019},
  month = mar,
  abstract = {We present a novel learning framework for vehicle recognition from a single RGB image. Unlike existing methods which only use attention mechanisms to locate 2D discriminative information, our unified framework learns a 2D global texture and a 3D-bounding-box based feature representation in a mutually correlated and reinforced way. These two kinds of feature representation are combined by a novel fusion network, which predicts the vehicle's category. The 2D global feature is extracted using an off-the-shelf detection network, where the estimated 2D bounding box assists in finding the region of interest (RoI). With the assistance of the RoI, the 3D bounding box and its corresponding features are generated in a geometrically correctly way using a novel 3D perspective Network (3DPN). The 3DPN consists of a convolutional neural network (CNN), a vanishing point loss, and RoI perspective layers. The CNN regresses the 3D bounding box under the guidance of the proposed vanishing point loss, which provides a perspective geometry constraint. Thanks to the proposed RoI perspective layer, the variation caused by viewpoint changes is corrected via the estimated geometry, enhancing feature representation. We present qualitative and quantitative results for our approach on the vehicle classification and verification tasks in the BoxCars dataset. The results demonstrate that, by learning how to extract features from the 3D bounding box, we can achieve comparable or superior performance to methods that only use 2D information.},
  archivePrefix = {arXiv},
  eprint = {1903.07916},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/48PLUW9P/Zeng et al. - 2019 - 3DCarRecog Car Recognition Using 3D Bounding Box.pdf},
  journal = {arXiv:1903.07916 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zeng3DMatchLearningLocal2016,
  title = {{{3DMatch}}: {{Learning Local Geometric Descriptors}} from {{RGB}}-{{D Reconstructions}}},
  shorttitle = {{{3DMatch}}},
  author = {Zeng, Andy and Song, Shuran and Nie{\ss}ner, Matthias and Fisher, Matthew and Xiao, Jianxiong and Funkhouser, Thomas},
  year = {2016},
  month = mar,
  abstract = {Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu},
  archivePrefix = {arXiv},
  eprint = {1603.08182},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3DMatch-Zeng et al-2016.pdf;/Users/sunjiaming/Zotero/storage/MB54AYAH/1603.html},
  journal = {arXiv:1603.08182 [cs]},
  keywords = {3d registration},
  primaryClass = {cs}
}

@article{zengDSDNetDeepStructured2020,
  title = {{{DSDNet}}: {{Deep Structured}} Self-{{Driving Network}}},
  shorttitle = {{{DSDNet}}},
  author = {Zeng, Wenyuan and Wang, Shenlong and Liao, Renjie and Chen, Yun and Yang, Bin and Urtasun, Raquel},
  year = {2020},
  month = aug,
  abstract = {In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {2008.06041},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/DSDNet-Zeng et al-2020.pdf;/Users/sunjiaming/Zotero/storage/4FANKK9D/2008.html},
  journal = {arXiv:2008.06041 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangAlignedReIDSurpassingHumanLevel2017,
  title = {{{AlignedReID}}: {{Surpassing Human}}-{{Level Performance}} in {{Person Re}}-{{Identification}}},
  shorttitle = {{{AlignedReID}}},
  author = {Zhang, Xuan and Luo, Hao and Fan, Xing and Xiang, Weilai and Sun, Yixiao and Xiao, Qiqi and Jiang, Wei and Zhang, Chi and Sun, Jian},
  year = {2017},
  month = nov,
  abstract = {In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.4\% on Market1501 and 97.8\% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.},
  archivePrefix = {arXiv},
  eprint = {1711.08184},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/AlignedReID-Zhang et al-2017.pdf},
  journal = {arXiv:1711.08184 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhangCascadedContextPyramid2019,
  title = {Cascaded {{Context Pyramid}} for {{Full}}-{{Resolution 3D Semantic Scene Completion}}},
  author = {Zhang, Pingping and Liu, Wei and Lei, Yinjie and Lu, Huchuan and Yang, Xiaoyun},
  year = {2019},
  month = aug,
  abstract = {Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multi-scale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details; (3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.},
  archivePrefix = {arXiv},
  eprint = {1908.00382},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/P2R6J2IZ/1908.html},
  journal = {arXiv:1908.00382 [cs]},
  primaryClass = {cs}
}

@article{zhangDeepGenerativeModeling2018,
  title = {Deep {{Generative Modeling}} for {{Scene Synthesis}} via {{Hybrid Representations}}},
  author = {Zhang, Zaiwei and Yang, Zhenpei and Ma, Chongyang and Luo, Linjie and Huth, Alexander and Vouga, Etienne and Huang, Qixing},
  year = {2018},
  month = aug,
  abstract = {We present a deep generative scene modeling technique for indoor environments. Our goal is to train a generative model using a feed-forward neural network that maps a prior distribution (e.g., a normal distribution) to the distribution of primary objects in indoor scenes. We introduce a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes. Moreover, our scene representation is applicable for 3D objects with different multiplicities (repetition counts), selected from a database. We show a principled way to train this model by combining discriminator losses for both a 3D object arrangement representation and a 2D image-based representation. We demonstrate the effectiveness of our scene representation and the deep learning method on benchmark datasets. We also show the applications of this generative model in scene interpolation and scene completion.},
  archivePrefix = {arXiv},
  eprint = {1808.02084},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/BFY43LWB/Zhang et al. - 2018 - Deep Generative Modeling for Scene Synthesis via H.pdf},
  journal = {arXiv:1808.02084 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{zhangDomaininvariantStereoMatching2019,
  title = {Domain-Invariant {{Stereo Matching Networks}}},
  author = {Zhang, Feihu and Qi, Xiaojuan and Yang, Ruigang and Prisacariu, Victor and Wah, Benjamin and Torr, Philip},
  year = {2019},
  month = nov,
  abstract = {State-of-the-art stereo matching networks have difficulties in generalizing to new unseen environments due to significant domain differences, such as color, illumination, contrast, and texture. In this paper, we aim at designing a domain-invariant stereo matching network (DSMNet) that generalizes well to unseen scenes. To achieve this goal, we propose i) a novel "domain normalization" approach that regularizes the distribution of learned representations to allow them to be invariant to domain differences, and ii) a trainable non-local graph-based filter for extracting robust structural and geometric representations that can further enhance domain-invariant generalizations. When trained on synthetic data and generalized to real test sets, our model performs significantly better than all state-of-the-art models. It even outperforms some deep learning models (e.g. MC-CNN) fine-tuned with test-domain data.},
  archivePrefix = {arXiv},
  eprint = {1911.13287},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Domain-invariant Stereo Matching Networks-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/YXCX4I52/1911.html},
  journal = {arXiv:1911.13287 [cs]},
  primaryClass = {cs}
}

@article{zhangFreeAnchorLearningMatch2019,
  title = {{{FreeAnchor}}: {{Learning}} to {{Match Anchors}} for {{Visual Object Detection}}},
  shorttitle = {{{FreeAnchor}}},
  author = {Zhang, Xiaosong and Wan, Fang and Liu, Chang and Ji, Rongrong and Ye, Qixiang},
  year = {2019},
  month = sep,
  abstract = {Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on MS-COCO demonstrate that FreeAnchor consistently outperforms their counterparts with significant margins.},
  archivePrefix = {arXiv},
  eprint = {1909.02466},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/FreeAnchor-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AP5WHAUZ/1909.html},
  journal = {arXiv:1909.02466 [cs]},
  primaryClass = {cs}
}

@article{zhangFundamentalPrinciplesLearning2018,
  title = {Fundamental {{Principles}} on {{Learning New Features}} for {{Effective Dense Matching}}},
  author = {Zhang, Feihu and Wah, Benjamin W.},
  year = {2018},
  month = feb,
  volume = {27},
  pages = {822--836},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2017.2752370},
  abstract = {In dense matching (including stereo matching and optical flow), nearly all existing approaches are based on simple features, such as gray or RGB color, gradient or simple transformations like census, to calculate matching costs. These features do not perform well in complex scenes that may involve radiometric changes, noises, overexposure and/or textureless regions. Various problems may appear, such as wrong matching at the pixel or region level, flattening/breaking of edges and/or even entire structural collapse. In this paper, we propose two fundamental principles based on the consistency and the distinctiveness of features. We show that almost all existing problems in dense matching are caused by features that violate one or both of these principles. To systematically learn good features for dense matching, we develop a general multi-objective optimization based on these two principles and apply convolutional neural networks to find new features that lie on the Pareto frontier. By using two-frame optical flow and stereo matching as applications, our experimental results show that the features learned can significantly improve the performance of state-of-the-art approaches. Based on the KITTI benchmarks, our method ranks first on the two stereo benchmarks and is the best among existing two-frame optical-flow algorithms on flow benchmarks.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Fundamental Principles on Learning New Features for Effective Dense Matching-Zhang_Wah-2018.pdf},
  journal = {IEEE Transactions on Image Processing},
  language = {en},
  number = {2}
}

@article{zhangGANetGuidedAggregation2019,
  title = {{{GA}}-{{Net}}: {{Guided Aggregation Net}} for {{End}}-to-End {{Stereo Matching}}},
  shorttitle = {{{GA}}-{{Net}}},
  author = {Zhang, Feihu and Prisacariu, Victor and Yang, Ruigang and Torr, Philip H. S.},
  year = {2019},
  month = apr,
  abstract = {In the stereo matching task, matching cost aggregation is crucial in both traditional methods and deep neural network models in order to accurately estimate disparities. We propose two novel neural net layers, aimed at capturing local and the whole-image cost dependencies respectively. The first is a semi-global aggregation layer which is a differentiable approximation of the semi-global matching, the second is the local guided aggregation layer which follows a traditional cost filtering strategy to refine thin structures. These two layers can be used to replace the widely used 3D convolutional layer which is computationally costly and memory-consuming as it has cubic computational/memory complexity. In the experiments, we show that nets with a two-layer guided aggregation block easily outperform the state-of-the-art GC-Net which has nineteen 3D convolutional layers. We also train a deep guided aggregation network (GA-Net) which gets better accuracies than state-of-the-art methods on both Scene Flow dataset and KITTI benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1904.06587},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/GA-Net-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/9I6S2PCN/1904.html},
  journal = {arXiv:1904.06587 [cs]},
  primaryClass = {cs}
}

@article{zhangGenerating3DPeople2019,
  title = {Generating {{3D People}} in {{Scenes}} without {{People}}},
  author = {Zhang, Yan and Hassan, Mohamed and Neumann, Heiko and Black, Michael J. and Tang, Siyu},
  year = {2019},
  month = dec,
  abstract = {We present a fully-automatic system that takes a 3D scene and generates plausible 3D human bodies that are posed naturally in that 3D scene. Given a 3D scene without people, humans can easily imagine how people could interact with the scene and the objects in it. However, this is a challenging task for a computer as solving it requires (1) the generated human bodies should be semantically plausible with the 3D environment, e.g. people sitting on the sofa or cooking near the stove; (2) the generated human-scene interaction should be physically feasible in the way that the human body and scene do not interpenetrate while, at the same time, body-scene contact supports physical interactions. To that end, we make use of the surface-based 3D human model SMPL-X. We first train a conditional variational autoencoder to predict semantically plausible 3D human pose conditioned on latent scene representations, then we further refine the generated 3D bodies using scene constraints to enforce feasible physical interaction. We show that our approach is able to synthesize realistic and expressive 3D human bodies that naturally interact with 3D environment. We perform extensive experiments demonstrating that our generative framework compares favorably with existing methods, both qualitatively and quantitatively. We believe that our scene-conditioned 3D human generation pipeline will be useful for numerous applications; e.g. to generate training data for human pose estimation, in video games and in VR/AR.},
  archivePrefix = {arXiv},
  eprint = {1912.02923},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Generating 3D People in Scenes without People-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/P2NDQHZN/1912.html},
  journal = {arXiv:1912.02923 [cs]},
  primaryClass = {cs}
}

@article{zhangH3DNet3DObject2020,
  title = {{{H3DNet}}: {{3D Object Detection Using Hybrid Geometric Primitives}}},
  shorttitle = {{{H3DNet}}},
  author = {Zhang, Zaiwei and Sun, Bo and Yang, Haitao and Huang, Qixing},
  year = {2020},
  month = jun,
  abstract = {We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D.},
  archivePrefix = {arXiv},
  eprint = {2006.05682},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/H3DNet-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/H9NICDNG/2006.html},
  journal = {arXiv:2006.05682 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangImageGANsMeet2020,
  title = {Image {{GANs}} Meet {{Differentiable Rendering}} for {{Inverse Graphics}} and {{Interpretable 3D Neural Rendering}}},
  author = {Zhang, Yuxuan and Chen, Wenzheng and Ling, Huan and Gao, Jun and Zhang, Yinan and Torralba, Antonio and Fidler, Sanja},
  year = {2020},
  month = oct,
  abstract = {Differentiable rendering has paved the way to training neural networks to perform "inverse graphics" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D "neural renderer", complementing traditional graphics renderers.},
  archivePrefix = {arXiv},
  eprint = {2010.09125},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/6FCBVGNT/2010.html},
  journal = {arXiv:2010.09125 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{zhangIntegratedObjectDetection2018,
  title = {Integrated {{Object Detection}} and {{Tracking}} with {{Tracklet}}-{{Conditioned Detection}}},
  author = {Zhang, Zheng and Cheng, Dazhi and Zhu, Xizhou and Lin, Stephen and Dai, Jifeng},
  year = {2018},
  month = nov,
  abstract = {Accurate detection and tracking of objects is vital for effective video understanding. In previous work, the two tasks have been combined in a way that tracking is based heavily on detection, but the detection benefits marginally from the tracking. To increase synergy, we propose to more tightly integrate the tasks by conditioning the object detection in the current frame on tracklets computed in prior frames. With this approach, the object detection results not only have high detection responses, but also improved coherence with the existing tracklets. This greater coherence leads to estimated object trajectories that are smoother and more stable than the jittered paths obtained without tracklet-conditioned detection. Over extensive experiments, this approach is shown to achieve state-of-the-art performance in terms of both detection and tracking accuracy, as well as noticeable improvements in tracking stability.},
  archivePrefix = {arXiv},
  eprint = {1811.11167},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Integrated Object Detection and Tracking with Tracklet-Conditioned Detection-Zhang et al-2018.pdf},
  journal = {arXiv:1811.11167 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhangJointLearningNeural2019,
  title = {Joint {{Learning}} of {{Neural Networks}} via {{Iterative Reweighted Least Squares}}},
  author = {Zhang, Zaiwei and Huang, Xiangru and Huang, Qixing and Zhang, Xiao and Li, Yuan},
  year = {2019},
  month = may,
  abstract = {In this paper, we introduce the problem of jointly learning feed-forward neural networks across a set of relevant but diverse datasets. Compared to learning a separate network from each dataset in isolation, joint learning enables us to extract correlated information across multiple datasets to significantly improve the quality of learned networks. We formulate this problem as joint learning of multiple copies of the same network architecture and enforce the network weights to be shared across these networks. Instead of hand-encoding the shared network layers, we solve an optimization problem to automatically determine how layers should be shared between each pair of datasets. Experimental results show that our approach outperforms baselines without joint learning and those using pretraining-and-fine-tuning. We show the effectiveness of our approach on three tasks: image classification, learning auto-encoders, and image generation.},
  archivePrefix = {arXiv},
  eprint = {1905.06526},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Joint Learning of Neural Networks via Iterative Reweighted Least Squares-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/AXDPAW8W/1905.html},
  journal = {arXiv:1905.06526 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhangLaservisualinertialOdometryMapping2018,
  title = {Laser-Visual-Inertial Odometry and Mapping with High Robustness and Low Drift: {{ZHANG}} {\textsc{and}} {{SINGH}}},
  shorttitle = {Laser-Visual-Inertial Odometry and Mapping with High Robustness and Low Drift},
  author = {Zhang, Ji and Singh, Sanjiv},
  year = {2018},
  month = dec,
  volume = {35},
  pages = {1242--1264},
  issn = {15564959},
  doi = {10.1002/rob.21809},
  abstract = {We present a data processing pipeline to online estimate ego-motion and build a map of the traversed environment, leveraging data from a 3D laser scanner, a camera, and an inertial measurement unit (IMU). Different from traditional methods that use a Kalman filter or factor-graph optimization, the proposed method employs a sequential, multilayer processing pipeline, solving for motion from coarse to fine. Starting with IMU mechanization for motion prediction, a visual\textendash inertial coupled method estimates motion; then, a scan matching method further refines the motion estimates and registers maps. The resulting system enables high-frequency, lowlatency ego-motion estimation, along with dense, accurate 3D map registration. Further, the method is capable of handling sensor degradation by automatic reconfiguration bypassing failure modules. Therefore, it can operate in the presence of highly dynamic motion as well as in the dark, texture-less, and structure-less environments. During experiments, the method demonstrates 0.22\% of relative position drift over 9.3 km of navigation and robustness w.r.t. running, jumping, and even highway speed driving (up to 33 m/s).},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Laser-visual-inertial odometry and mapping with high robustness and low drift-Zhang_Singh-2018.pdf},
  journal = {Journal of Field Robotics},
  language = {en},
  number = {8}
}

@article{zhangLearningReconstructShapes,
  title = {Learning to {{Reconstruct Shapes}} from {{Unseen Classes}}},
  author = {Zhang, Xiuming and Tenenbaum, Joshua B and Zhang, Zhoutong and Zhang, Chengkai and Freeman, William T and Wu, Jiajun},
  pages = {12},
  abstract = {From a single image, humans are able to perceive the full 3D shape of an object by exploiting learned shape priors from everyday life. Contemporary single-image 3D reconstruction algorithms aim to solve this task in a similar fashion, but often end up with priors that are highly biased by training classes. Here we present an algorithm, Generalizable Reconstruction (GenRe), designed to capture more generic, class-agnostic shape priors. We achieve this with an inference network and training procedure that combine 2.5D representations of visible surfaces (depth and silhouette), spherical shape representations of both visible and non-visible surfaces, and 3D voxel-based representations, in a principled manner that exploits the causal structure of how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe performs well on single-view shape reconstruction, and generalizes to diverse novel objects from categories not seen during training.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Reconstruct Shapes from Unseen Classes-Zhang et al-.pdf},
  language = {en}
}

@article{zhangLearningTwoViewCorrespondences2019,
  title = {Learning {{Two}}-{{View Correspondences}} and {{Geometry Using Order}}-{{Aware Network}}},
  author = {Zhang, Jiahui and Sun, Dawei and Luo, Zixin and Yao, Anbang and Zhou, Lei and Shen, Tianwei and Chen, Yurong and Quan, Long and Liao, Hongen},
  year = {2019},
  month = aug,
  abstract = {Establishing correspondences between two images requires both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential matrix. Specifically, this proposed network is built hierarchically and comprises three novel operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in a canonical order and invariant to input permutations. Next, the clusters are spatially correlated to form the global context of correspondences. After that, the context-encoded clusters are recovered back to the original size through a proposed upsampling operator. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts. Code will be available at https://github.com/zjhthu/OANet.git.},
  archivePrefix = {arXiv},
  eprint = {1908.04964},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning Two-View Correspondences and Geometry Using Order-Aware Network-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/MF652B5V/1908.html},
  journal = {arXiv:1908.04964 [cs]},
  primaryClass = {cs}
}

@article{zhangLeveragingLocalGlobal2020,
  title = {Leveraging {{Local}} and {{Global Descriptors}} in {{Parallel}} to {{Search Correspondences}} for {{Visual Localization}}},
  author = {Zhang, Pengju and Wu, Yihong and Liu, Bingxi},
  year = {2020},
  month = sep,
  abstract = {Visual localization to compute 6DoF camera pose from a given image has wide applications such as in robotics, virtual reality, augmented reality, etc. Two kinds of descriptors are important for the visual localization. One is global descriptors that extract the whole feature from each image. The other is local descriptors that extract the local feature from each image patch usually enclosing a key point. More and more methods of the visual localization have two stages: at first to perform image retrieval by global descriptors and then from the retrieval feedback to make 2D-3D point correspondences by local descriptors. The two stages are in serial for most of the methods. This simple combination has not achieved superiority of fusing local and global descriptors. The 3D points obtained from the retrieval feedback are as the nearest neighbor candidates of the 2D image points only by global descriptors. Each of the 2D image points is also called a query local feature when performing the 2D-3D point correspondences. In this paper, we propose a novel parallel search framework, which leverages advantages of both local and global descriptors to get nearest neighbor candidates of a query local feature. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the query local feature. We propose a new probabilistic model and a new deep learning based local descriptor when constructing the random trees. A weighted Hamming regularization term to keep discriminativeness after binarization is given in the loss function for the proposed local descriptor. The loss function co-trains both real and binary descriptors of which the results are integrated into the random trees.},
  archivePrefix = {arXiv},
  eprint = {2009.10891},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Leveraging Local and Global Descriptors in Parallel to Search Correspondences-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GJTEEDPX/2009.html},
  journal = {arXiv:2009.10891 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangNeRFAnalyzingImproving2020,
  title = {{{NeRF}}++: {{Analyzing}} and {{Improving Neural Radiance Fields}}},
  shorttitle = {{{NeRF}}++},
  author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
  year = {2020},
  month = oct,
  abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
  archivePrefix = {arXiv},
  eprint = {2010.07492},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/NeRF++-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/NRKUJ7ZP/2010.html},
  journal = {arXiv:2010.07492 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangNeuralLightTransport,
  title = {Neural {{Light Transport}} for {{Relighting}} and {{View Synthesis}}},
  author = {Zhang, Xiuming and Fanello, Sean and Tsai, Yun-Ta and Sun, Tiancheng and Xue, Tianfan and Pandey, Rohit and {Orts-Escolano}, Sergio and Davidson, Philip and Rhemann, Christoph and Debevec, Paul and Barron, Jonathan T and Ramamoorthi, Ravi and Freeman, William T},
  volume = {1},
  pages = {16},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Neural Light Transport for Relighting and View Synthesis-Zhang et al-.pdf},
  language = {en},
  number = {1}
}

@article{zhangPerceiving3DHumanObject2020,
  title = {Perceiving {{3D Human}}-{{Object Spatial Arrangements}} from a {{Single Image}} in the {{Wild}}},
  author = {Zhang, Jason Y. and Pepose, Sam and Joo, Hanbyul and Ramanan, Deva and Malik, Jitendra and Kanazawa, Angjoo},
  year = {2020},
  month = jul,
  abstract = {We present a method that infers spatial arrangements and shapes of humans and objects in a globally consistent 3D scene, all from a single image in-the-wild captured in an uncontrolled environment. Notably, our method runs on datasets without any scene- or object-level 3D supervision. Our key insight is that considering humans and objects jointly gives rise to "3D common sense" constraints that can be used to resolve ambiguity. In particular, we introduce a scale loss that learns the distribution of object size from data; an occlusion-aware silhouette re-projection loss to optimize object pose; and a human-object interaction loss to capture the spatial layout of objects with which humans interact. We empirically validate that our constraints dramatically reduce the space of likely 3D spatial configurations. We demonstrate our approach on challenging, in-the-wild images of humans interacting with large objects (such as bicycles, motorcycles, and surfboards) and handheld objects (such as laptops, tennis rackets, and skateboards). We quantify the ability of our approach to recover human-object arrangements and outline remaining challenges in this relatively domain. The project webpage can be found at https://jasonyzhang.com/phosa.},
  archivePrefix = {arXiv},
  eprint = {2007.15649},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/GNZXXZBT/2007.html},
  journal = {arXiv:2007.15649 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangRobustEgoObject2020,
  title = {Robust {{Ego}} and {{Object}} 6-{{DoF Motion Estimation}} and {{Tracking}}},
  author = {Zhang, Jun and Henein, Mina and Mahony, Robert and Ila, Viorela},
  year = {2020},
  month = jul,
  abstract = {The problem of tracking self-motion as well as motion of objects in the scene using information from a camera is known as multi-body visual odometry and is a challenging task. This paper proposes a robust solution to achieve accurate estimation and consistent track-ability for dynamic multi-body visual odometry. A compact and effective framework is proposed leveraging recent advances in semantic instance-level segmentation and accurate optical flow estimation. A novel formulation, jointly optimizing SE(3) motion and optical flow is introduced that improves the quality of the tracked points and the motion estimation accuracy. The proposed approach is evaluated on the virtual KITTI Dataset and tested on the real KITTI Dataset, demonstrating its applicability to autonomous driving applications. For the benefit of the community, we make the source code public.},
  archivePrefix = {arXiv},
  eprint = {2007.13993},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust Ego and Object 6-DoF Motion Estimation and Tracking-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/2ATMY7TV/2007.html},
  journal = {arXiv:2007.13993 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{zhangRobustMultiModalityMultiObject2019,
  title = {Robust {{Multi}}-{{Modality Multi}}-{{Object Tracking}}},
  author = {Zhang, Wenwei and Zhou, Hui and Sun, Shuyang and Wang, Zhe and Shi, Jianping and Loy, Chen Change},
  year = {2019},
  month = sep,
  abstract = {Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT.},
  archivePrefix = {arXiv},
  eprint = {1909.03850},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Robust Multi-Modality Multi-Object Tracking-Zhang et al-2019.pdf;/Users/sunjiaming/Zotero/storage/K6TDWPK4/1909.html},
  journal = {arXiv:1909.03850 [cs]},
  primaryClass = {cs}
}

@article{zhangSpatialSemanticEmbedding2020,
  title = {Spatial {{Semantic Embedding Network}}: {{Fast 3D Instance Segmentation}} with {{Deep Metric Learning}}},
  shorttitle = {Spatial {{Semantic Embedding Network}}},
  author = {Zhang, Dongsu and Chun, Junha and Cha, Sang Kyun and Kim, Young Min},
  year = {2020},
  month = jul,
  abstract = {We propose spatial semantic embedding network (SSEN), a simple, yet efficient algorithm for 3D instance segmentation using deep metric learning. The raw 3D reconstruction of an indoor environment suffers from occlusions, noise, and is produced without any meaningful distinction between individual entities. For high-level intelligent tasks from a large scale scene, 3D instance segmentation recognizes individual instances of objects. We approach the instance segmentation by simply learning the correct embedding space that maps individual instances of objects into distinct clusters that reflect both spatial and semantic information. Unlike previous approaches that require complex pre-processing or post-processing, our implementation is compact and fast with competitive performance, maintaining scalability on large scenes with high resolution voxels. We demonstrate the state-of-the-art performance of our algorithm in the ScanNet 3D instance segmentation benchmark on AP score.},
  archivePrefix = {arXiv},
  eprint = {2007.03169},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Spatial Semantic Embedding Network-Zhang et al-2020.pdf;/Users/sunjiaming/Zotero/storage/F4FJFUC5/2007.html},
  journal = {arXiv:2007.03169 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhangStabilityVideoDetection2016,
  title = {On {{The Stability}} of {{Video Detection}} and {{Tracking}}},
  author = {Zhang, Hong and Wang, Naiyan},
  year = {2016},
  month = nov,
  abstract = {In this paper, we study an important yet less explored aspect in video detection and tracking -- stability. Surprisingly, there is no prior work that tried to study it. As a result, we start our work by proposing a novel evaluation metric for video detection which considers both stability and accuracy. For accuracy, we extend the existing accuracy metric mean Average Precision (mAP). For stability, we decompose it into three terms: fragment error, center position error, scale and ratio error. Each error represents one aspect of stability. Furthermore, we demonstrate that the stability metric has low correlation with accuracy metric. Thus, it indeed captures a different perspective of quality. Lastly, based on this metric, we evaluate several existing methods for video detection and show how they affect accuracy and stability. We believe our work can provide guidance and solid baselines for future researches in the related areas.},
  archivePrefix = {arXiv},
  eprint = {1611.06467},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On The Stability of Video Detection and Tracking-Zhang_Wang-2016.pdf;/Users/sunjiaming/Zotero/storage/5IXN8H27/1611.html},
  journal = {arXiv:1611.06467 [cs]},
  primaryClass = {cs}
}

@article{zhaoMaskFlownetAsymmetricFeature2020,
  title = {{{MaskFlownet}}: {{Asymmetric Feature Matching}} with {{Learnable Occlusion Mask}}},
  shorttitle = {{{MaskFlownet}}},
  author = {Zhao, Shengyu and Sheng, Yilun and Dong, Yue and Chang, Eric I.-Chao and Xu, Yan},
  year = {2020},
  month = mar,
  abstract = {Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https://github.com/microsoft/MaskFlownet.},
  archivePrefix = {arXiv},
  eprint = {2003.10955},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/MaskFlownet-Zhao et al-2020.pdf;/Users/sunjiaming/Zotero/storage/USMUHTN7/2003.html},
  journal = {arXiv:2003.10955 [cs]},
  primaryClass = {cs}
}

@article{zhengOdometryVisionBasedGroundVehicle2018,
  title = {Odometry-{{Vision}}-{{Based Ground Vehicle Motion Estimation With SE}}(2)-{{Constrained SE}}(3) {{Poses}}},
  author = {Zheng, Fan and Tang, Hengbo and Liu, Yun-Hui},
  year = {2018},
  pages = {1--12},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2018.2831900},
  abstract = {This paper focuses on the motion estimation problem of ground vehicles using odometry and monocular visual sensors. While the keyframe-based batch optimization methods become the mainstream approach in mobile vehicle localization and mapping, the keyframe poses are usually represented by SE(3) in vision-based methods or SE(2) in methods based on range scanners. For a ground vehicle, this paper proposes a new SE(2)-constrained SE(3) parameterization of its poses, which can be easily achieved in the batch optimization framework using specially formulated edges. Utilizing such a parameterization of poses, a complete odometry-vision-based motion estimation system is developed. The system is designed in a commonly used structure of graph optimization, providing high modularity and flexibility for further implementation or adaptation. Its superior performance in terms of accuracy on a ground vehicle platform is validated by real-world experiments in industrial indoor environments.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Odometry-Vision-Based Ground Vehicle Motion Estimation With SE(2)-Constrained-Zheng et al-2018.pdf},
  journal = {IEEE Transactions on Cybernetics},
  language = {en}
}

@inproceedings{zhengPatchMatchBasedJoint2014,
  title = {{{PatchMatch Based Joint View Selection}} and {{Depthmap Estimation}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zheng, Enliang and Dunn, Enrique and Jojic, Vladimir and Frahm, Jan-Michael},
  year = {2014},
  month = jun,
  pages = {1510--1517},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.196},
  abstract = {We propose a multi-view depthmap estimation approach aimed at adaptively ascertaining the pixel level data associations between a reference image and all the elements of a source image set. Namely, we address the question, what aggregation subset of the source image set should we use to estimate the depth of a particular pixel in the reference image? We pose the problem within a probabilistic framework that jointly models pixel-level view selection and depthmap estimation given the local pairwise image photoconsistency. The corresponding graphical model is solved by EM-based view selection probability inference and PatchMatch-like depth sampling and propagation. Experimental results on standard multi-view benchmarks convey the state-of-the art estimation accuracy afforded by mitigating spurious pixellevel data associations. Additionally, experiments on large Internet crowd sourced data demonstrate the robustness of our approach against unstructured and heterogeneous image capture characteristics. Moreover, the linear computational and storage requirements of our formulation, as well as its inherent parallelism, enables an efficient and scalable GPU-based implementation.},
  file = {/Users/sunjiaming/Zotero/storage/K9N2U3AS/Zheng et al. - 2014 - PatchMatch Based Joint View Selection and Depthmap.pdf},
  isbn = {978-1-4799-5118-5},
  keywords = {neufu_paper},
  language = {en}
}

@article{zhiSceneCodeMonocularDense2019,
  title = {{{SceneCode}}: {{Monocular Dense Semantic Reconstruction}} Using {{Learned Encoded Scene Representations}}},
  shorttitle = {{{SceneCode}}},
  author = {Zhi, Shuaifeng and Bloesch, Michael and Leutenegger, Stefan and Davison, Andrew J.},
  year = {2019},
  month = mar,
  abstract = {Systems which incrementally create 3D semantic maps from image sequences must store and update representations of both geometry and semantic entities. However, while there has been much work on the correct formulation for geometrical estimation, state-of-the-art systems usually rely on simple semantic representations which store and update independent label estimates for each surface element (depth pixels, surfels, or voxels). Spatial correlation is discarded, and fused label maps are incoherent and noisy. We introduce a new compact and optimisable semantic representation by training a variational auto-encoder that is conditioned on a colour image. Using this learned latent space, we can tackle semantic label fusion by jointly optimising the low-dimenional codes associated with each of a set of overlapping images, producing consistent fused label maps which preserve spatial correlation. We also show how this approach can be used within a monocular keyframe based semantic mapping system where a similar code approach is used for geometry. The probabilistic formulation allows a flexible formulation where we can jointly estimate motion, geometry and semantics in a unified optimisation.},
  archivePrefix = {arXiv},
  eprint = {1903.06482},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/SceneCode-Zhi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/B4ZPKNHU/1903.html},
  journal = {arXiv:1903.06482 [cs]},
  primaryClass = {cs}
}

@inproceedings{zhongDetectSLAMMakingObject2018,
  title = {Detect-{{SLAM}}: {{Making Object Detection}} and {{SLAM Mutually Beneficial}}},
  shorttitle = {Detect-{{SLAM}}},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Zhong, Fangwei and Wang, Sheng and Zhang, Ziqi and Chen, China and Wang, Yizhou},
  year = {2018},
  month = mar,
  pages = {1001--1010},
  publisher = {{IEEE}},
  address = {{Lake Tahoe, NV}},
  doi = {10.1109/WACV.2018.00115},
  abstract = {Although significant progress has been made in SLAM and object detection in recent years, there are still a series of challenges for both tasks, e.g., SLAM in dynamic environments and detecting objects in complex environments. To address these challenges, we present a novel robotic vision system, which integrates SLAM with a deep neural networkbased object detector to make the two functions mutually beneficial. The proposed system facilitates a robot to accomplish tasks reliably and efficiently in an unknown and dynamic environment. Experimental results show that compare to the state-of-the-art robotic vision systems, the proposed system has three advantages: i) it greatly improves the accuracy and robustness of SLAM in dynamic environments by removing unreliable features from moving objects leveraging the object detector, ii) it builds an instance-level semantic map of the environment in an online fashion using the synergy of the two functions for further semantic applications; and iii) it improves the object detector so that it can detect/recognize objects effectively under more challenging conditions such as unusual viewpoints, poor lighting condition, and motion blur, by leveraging the object map.},
  file = {/Users/sunjiaming/Zotero/storage/SPV2VITY/Zhong et al. - 2018 - Detect-SLAM Making Object Detection and SLAM Mutu.pdf},
  isbn = {978-1-5386-4886-5},
  language = {en}
}

@article{zhongUncertaintyAwareVoxelBased2020,
  title = {Uncertainty-{{Aware Voxel}} Based {{3D Object Detection}} and {{Tracking}} with von-{{Mises Loss}}},
  author = {Zhong, Yuanxin and Zhu, Minghan and Peng, Huei},
  year = {2020},
  month = nov,
  abstract = {Object detection and tracking is a key task in autonomy. Specifically, 3D object detection and tracking have been an emerging hot topic recently. Although various methods have been proposed for object detection, uncertainty in the 3D detection and tracking tasks has been less explored. Uncertainty helps us tackle the error in the perception system and improve robustness. In this paper, we propose a method for improving target tracking performance by adding uncertainty regression to the SECOND detector, which is one of the most representative algorithms of 3D object detection. Our method estimates positional and dimensional uncertainties with Gaussian Negative Log-Likelihood (NLL) Loss for estimation and introduces von-Mises NLL Loss for angular uncertainty estimation. We fed the uncertainty output into a classical object tracking framework and proved that our method increased the tracking performance compared against the vanilla tracker with constant covariance assumption.},
  archivePrefix = {arXiv},
  eprint = {2011.02553},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Uncertainty-Aware Voxel based 3D Object Detection and Tracking with von-Mises-Zhong et al-2020.pdf;/Users/sunjiaming/Zotero/storage/EL2QZ7SV/2011.html},
  journal = {arXiv:2011.02553 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{zhongUnsupervisedDeepEpipolar2019,
  title = {Unsupervised {{Deep Epipolar Flow}} for {{Stationary}} or {{Dynamic Scenes}}},
  author = {Zhong, Yiran and Ji, Pan and Wang, Jianyuan and Dai, Yuchao and Li, Hongdong},
  year = {2019},
  month = apr,
  abstract = {Unsupervised deep learning for optical flow computation has achieved promising results. Most existing deep-net based methods rely on image brightness consistency and local smoothness constraint to train the networks. Their performance degrades at regions where repetitive textures or occlusions occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical flow method which incorporates global geometric constraints into network learning. In particular, we investigate multiple ways of enforcing the epipolar constraint in flow estimation. To alleviate a ``chicken-and-egg'' type of problem encountered in dynamic scenes where multiple motions may be present, we propose a low-rank constraint as well as a union-of-subspaces constraint for training. Experimental results on various benchmarking datasets show that our method achieves competitive performance compared with supervised methods and outperforms state-of-the-art unsupervised deep-learning methods.},
  archivePrefix = {arXiv},
  eprint = {1904.03848},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes-Zhong et al-2019.pdf;/Users/sunjiaming/Zotero/storage/UCH35TKQ/1904.html},
  journal = {arXiv:1904.03848 [cs]},
  primaryClass = {cs}
}

@article{zhouBottomupObjectDetection2019,
  title = {Bottom-up {{Object Detection}} by {{Grouping Extreme}} and {{Center Points}}},
  author = {Zhou, Xingyi and Zhuo, Jiacheng and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2019},
  month = jan,
  abstract = {With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, leftmost, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2\% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9\%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6\% Mask AP.},
  archivePrefix = {arXiv},
  eprint = {1901.08043},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Bottom-up Object Detection by Grouping Extreme and Center Points-Zhou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/D6ETE4KE/Zhou et al. - 2019 - Bottom-up Object Detection by Grouping Extreme and.pdf;/Users/sunjiaming/Zotero/storage/NSWHG5D9/1901.html},
  journal = {arXiv:1901.08043 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhouColorMapOptimization2014,
  title = {Color Map Optimization for {{3D}} Reconstruction with Consumer Depth Cameras},
  author = {Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2014},
  month = jul,
  volume = {33},
  pages = {1--10},
  issn = {07300301},
  doi = {10.1145/2601097.2601134},
  abstract = {We present a global optimization approach for mapping color images onto geometric reconstructions. Range and color videos produced by consumer-grade RGB-D cameras suffer from noise and optical distortions, which impede accurate mapping of the acquired color data to the reconstructed geometry. Our approach addresses these sources of error by optimizing camera poses in tandem with non-rigid correction functions for all images. All parameters are optimized jointly to maximize the photometric consistency of the reconstructed mapping. We show that this optimization can be performed efficiently by an alternating optimization algorithm that interleaves analytical updates of the color map with decoupled parameter updates for all images. Experimental results demonstrate that our approach substantially improves color mapping fidelity.},
  file = {/Users/sunjiaming/Zotero/storage/PK2B2YLF/Zhou and Koltun - 2014 - Color map optimization for 3D reconstruction with .pdf},
  journal = {ACM Transactions on Graphics},
  keywords = {reconstruction},
  language = {en},
  number = {4}
}

@article{zhouContinuityRotationRepresentations2018,
  title = {On the {{Continuity}} of {{Rotation Representations}} in {{Neural Networks}}},
  author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  year = {2018},
  month = dec,
  abstract = {In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.},
  archivePrefix = {arXiv},
  eprint = {1812.07035},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/On the Continuity of Rotation Representations in Neural Networks-Zhou et al-2018.pdf;/Users/sunjiaming/Zotero/storage/92BH7H8V/1812.html},
  journal = {arXiv:1812.07035 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhouDeepTAMDeepTracking2018,
  title = {{{DeepTAM}}: {{Deep Tracking}} and {{Mapping}}},
  shorttitle = {{{DeepTAM}}},
  author = {Zhou, Huizhong and Ummenhofer, Benjamin and Brox, Thomas},
  year = {2018},
  month = aug,
  abstract = {We present a system for keyframe-based dense camera tracking and depth map estimation that is entirely learned. For tracking, we estimate small pose increments between the current camera image and a synthetic viewpoint. This significantly simplifies the learning problem and alleviates the dataset bias for camera motions. Further, we show that generating a large number of pose hypotheses leads to more accurate predictions. For mapping, we accumulate information in a cost volume centered at the current depth estimate. The mapping network then combines the cost volume and the keyframe image to update the depth prediction, thereby effectively making use of depth measurements and image-based priors. Our approach yields state-of-the-art results with few images and is robust with respect to noisy camera poses.},
  archivePrefix = {arXiv},
  eprint = {1808.01900},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/DeepTAM-Zhou et al-2018.pdf},
  journal = {arXiv:1808.01900 [cs]},
  keywords = {neufu_paper},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{zhouElasticFragmentsDense2013,
  ids = {zhouElasticFragmentsDense2013a},
  title = {Elastic {{Fragments}} for {{Dense Scene Reconstruction}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Zhou, Qian-Yi and Miller, Stephen and Koltun, Vladlen},
  year = {2013},
  month = dec,
  pages = {473--480},
  publisher = {{IEEE}},
  address = {{Sydney, Australia}},
  doi = {10.1109/ICCV.2013.65},
  abstract = {We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Elastic Fragments for Dense Scene Reconstruction-Zhou et al-2013.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Elastic Fragments for Dense Scene Reconstruction-Zhou et al-22.pdf;/Users/sunjiaming/Zotero/storage/6GCFKZE5/Zhou et al. - 2013 - Elastic Fragments for Dense Scene Reconstruction.pdf},
  isbn = {978-1-4799-2840-8},
  keywords = {reconstruction},
  language = {en}
}

@article{zhouEndtoEndMultiViewFusion2019,
  title = {End-to-{{End Multi}}-{{View Fusion}} for {{3D Object Detection}} in {{LiDAR Point Clouds}}},
  author = {Zhou, Yin and Sun, Pei and Zhang, Yu and Anguelov, Dragomir and Gao, Jiyang and Ouyang, Tom and Guo, James and Ngiam, Jiquan and Vasudevan, Vijay},
  year = {2019},
  month = oct,
  abstract = {Recent work on 3D object detection advocates point cloud voxelization in birds-eye view, where objects preserve their physical dimensions and are naturally separable. When represented in this view, however, point clouds are sparse and have highly variable point density, which may cause detectors difficulties in detecting distant or small objects (pedestrians, traffic signs, etc.). On the other hand, perspective view provides dense observations, which could allow more favorable feature encoding for such cases. In this paper, we aim to synergize the birds-eye view and the perspective view and propose a novel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn to utilize the complementary information from both. Specifically, we introduce dynamic voxelization, which has four merits compared to existing voxelization methods, i) removing the need of pre-allocating a tensor with fixed size; ii) overcoming the information loss due to stochastic point/voxel dropout; iii) yielding deterministic voxel embeddings and more stable detection outcomes; iv) establishing the bi-directional relationship between points and voxels, which potentially lays a natural foundation for cross-view feature fusion. By employing dynamic voxelization, the proposed feature fusion architecture enables each point to learn to fuse context information from different views. MVF operates on points and can be naturally extended to other approaches using LiDAR point clouds. We evaluate our MVF model extensively on the newly released Waymo Open Dataset and on the KITTI dataset and demonstrate that it significantly improves detection accuracy over the comparable single-view PointPillars baseline.},
  archivePrefix = {arXiv},
  eprint = {1910.06528},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds-Zhou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/LX634KSL/1910.html},
  journal = {arXiv:1910.06528 [cs]},
  primaryClass = {cs}
}

@article{zhouEndtoEndMultiViewFusion2019a,
  title = {End-to-{{End Multi}}-{{View Fusion}} for {{3D Object Detection}} in {{LiDAR Point Clouds}}},
  author = {Zhou, Yin and Sun, Pei and Zhang, Yu and Anguelov, Dragomir and Gao, Jiyang and Ouyang, Tom and Guo, James and Ngiam, Jiquan and Vasudevan, Vijay},
  year = {2019},
  month = oct,
  abstract = {Recent work on 3D object detection advocates point cloud voxelization in birds-eye view, where objects preserve their physical dimensions and are naturally separable. When represented in this view, however, point clouds are sparse and have highly variable point density, which may cause detectors difficulties in detecting distant or small objects (pedestrians, traffic signs, etc.). On the other hand, perspective view provides dense observations, which could allow more favorable feature encoding for such cases. In this paper, we aim to synergize the birds-eye view and the perspective view and propose a novel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn to utilize the complementary information from both. Specifically, we introduce dynamic voxelization, which has four merits compared to existing voxelization methods, i) removing the need of pre-allocating a tensor with fixed size; ii) overcoming the information loss due to stochastic point/voxel dropout; iii) yielding deterministic voxel embeddings and more stable detection outcomes; iv) establishing the bi-directional relationship between points and voxels, which potentially lays a natural foundation for cross-view feature fusion. By employing dynamic voxelization, the proposed feature fusion architecture enables each point to learn to fuse context information from different views. MVF operates on points and can be naturally extended to other approaches using LiDAR point clouds. We evaluate our MVF model extensively on the newly released Waymo Open Dataset and on the KITTI dataset and demonstrate that it significantly improves detection accuracy over the comparable single-view PointPillars baseline.},
  archivePrefix = {arXiv},
  eprint = {1910.06528},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds-Zhou et al-22.pdf;/Users/sunjiaming/Zotero/storage/TLFLQ6FJ/1910.html},
  journal = {arXiv:1910.06528 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhouKFNetLearningTemporal2020,
  title = {{{KFNet}}: {{Learning Temporal Camera Relocalization}} Using {{Kalman Filtering}}},
  shorttitle = {{{KFNet}}},
  author = {Zhou, Lei and Luo, Zixin and Shen, Tianwei and Zhang, Jiahui and Zhen, Mingmin and Yao, Yao and Fang, Tian and Quan, Long},
  year = {2020},
  month = mar,
  abstract = {Temporal camera relocalization estimates the pose with respect to each video frame in sequence, as opposed to one-shot relocalization which focuses on a still image. Even though the time dependency has been taken into account, current temporal relocalization methods still generally underperform the state-of-the-art one-shot approaches in terms of accuracy. In this work, we improve the temporal relocalization method by using a network architecture that incorporates Kalman filtering (KFNet) for online camera relocalization. In particular, KFNet extends the scene coordinate regression problem to the time domain in order to recursively establish 2D and 3D correspondences for the pose determination. The network architecture design and the loss formulation are based on Kalman filtering in the context of Bayesian learning. Extensive experiments on multiple relocalization benchmarks demonstrate the high accuracy of KFNet at the top of both one-shot and temporal relocalization approaches. Our codes are released at https://github.com/zlthinker/KFNet.},
  archivePrefix = {arXiv},
  eprint = {2003.10629},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/KFNet-Zhou et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MJZ57EJ6/2003.html},
  journal = {arXiv:2003.10629 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhouLearningReconstruct3D2019,
  title = {Learning to {{Reconstruct 3D Manhattan Wireframes}} from a {{Single Image}}},
  author = {Zhou, Yichao and Qi, Haozhi and Zhai, Yuexiang and Sun, Qi and Chen, Zhili and Wei, Li-Yi and Ma, Yi},
  year = {2019},
  month = may,
  abstract = {In this paper, we propose a method to obtain a compact and accurate 3D wireframe representation from a single image by effectively exploiting global structural regularities. Our method trains a convolutional neural network to simultaneously detect salient junctions and straight lines, as well as predict their 3D depth and vanishing points. Compared with the state-of-the-art learning-based wireframe detection methods, our network is much simpler and more unified, leading to better 2D wireframe detection. With global structural priors such as Manhattan assumption, our method further reconstructs a full 3D wireframe model, a compact vector representation suitable for a variety of high-level vision tasks such as AR and CAD. We conduct extensive evaluations on a large synthetic dataset of urban scenes as well as real images. Our code and datasets will be released.},
  archivePrefix = {arXiv},
  eprint = {1905.07482},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Learning to Reconstruct 3D Manhattan Wireframes from a Single Image-Zhou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/DIIU83QQ/1905.html},
  journal = {arXiv:1905.07482 [cs]},
  primaryClass = {cs}
}

@article{zhouNeurVPSNeuralVanishing2019,
  title = {{{NeurVPS}}: {{Neural Vanishing Point Scanning}} via {{Conic Convolution}}},
  shorttitle = {{{NeurVPS}}},
  author = {Zhou, Yichao and Qi, Haozhi and Huang, Jingwei and Ma, Yi},
  year = {2019},
  month = oct,
  abstract = {We present a simple yet effective end-to-end trainable deep network with geometry-inspired convolutional operators for detecting vanishing points in images. Traditional convolutional neural networks rely on aggregating edge features and do not have mechanisms to directly exploit the geometric properties of vanishing points as the intersections of parallel lines. In this work, we identify a canonical conic space in which the neural network can effectively compute the global geometric information of vanishing points locally, and we propose a novel operator named conic convolution that can be implemented as regular convolutions in this space. This new operator explicitly enforces feature extractions and aggregations along the structural lines and yet has the same number of parameters as the regular 2D convolution. Our extensive experiments on both synthetic and real-world datasets show that the proposed operator significantly improves the performance of vanishing point detection over traditional methods. The code and dataset have been made publicly available at https://github.com/zhou13/neurvps.},
  archivePrefix = {arXiv},
  eprint = {1910.06316},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/NeurVPS-Zhou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/X3VYVANW/1910.html},
  journal = {arXiv:1910.06316 [cs]},
  primaryClass = {cs}
}

@article{zhouObjectsPoints2019,
  title = {Objects as {{Points}}},
  author = {Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2019},
  month = apr,
  abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
  archivePrefix = {arXiv},
  eprint = {1904.07850},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Objects as Points-Zhou et al-2019.pdf;/Users/sunjiaming/Zotero/storage/QA5GD2SU/1904.html},
  journal = {arXiv:1904.07850 [cs]},
  primaryClass = {cs}
}

@article{zhouOpen3DModernLibrary2018,
  title = {{{Open3D}}: {{A Modern Library}} for {{3D Data Processing}}},
  shorttitle = {{{Open3D}}},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  year = {2018},
  month = jan,
  abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
  archivePrefix = {arXiv},
  eprint = {1801.09847},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Open3D-Zhou et al-2018.pdf;/Users/sunjiaming/Zotero/storage/UL9I6BC2/1801.html},
  journal = {arXiv:1801.09847 [cs]},
  primaryClass = {cs}
}

@inproceedings{zhouSimultaneousLocalizationCalibration2014,
  title = {Simultaneous {{Localization}} and {{Calibration}}: {{Self}}-{{Calibration}} of {{Consumer Depth Cameras}}},
  shorttitle = {Simultaneous {{Localization}} and {{Calibration}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhou, Qian-Yi and Koltun, Vladlen},
  year = {2014},
  month = jun,
  pages = {454--460},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.65},
  abstract = {We describe an approach for simultaneous localization and calibration of a stream of range images. Our approach jointly optimizes the camera trajectory and a calibration function that corrects the camera's unknown nonlinear distortion. Experiments with real-world benchmark data and synthetic data show that our approach increases the accuracy of camera trajectories and geometric models estimated from range video produced by consumer-grade cameras.},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Simultaneous Localization and Calibration-Zhou_Koltun-2014.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@article{zhouTrackingObjectsPoints2020,
  title = {Tracking {{Objects}} as {{Points}}},
  author = {Zhou, Xingyi and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
  year = {2020},
  month = apr,
  abstract = {Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. In this paper, we present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves 67.3\% MOTA on the MOT17 challenge at 22 FPS and 89.4\% MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves 28.3\% AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS.},
  archivePrefix = {arXiv},
  eprint = {2004.01177},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Tracking Objects as Points-Zhou et al-2020.pdf;/Users/sunjiaming/Zotero/storage/MJXC6Y55/2004.html},
  journal = {arXiv:2004.01177 [cs]},
  primaryClass = {cs}
}

@article{zhouUnsupervisedLearningMonocular2018,
  title = {Unsupervised {{Learning}} of {{Monocular Depth Estimation}} with {{Bundle Adjustment}}, {{Super}}-{{Resolution}} and {{Clip Loss}}},
  author = {Zhou, Lipu and Ye, Jiamin and Abello, Montiel and Wang, Shengze and Kaess, Michael},
  year = {2018},
  month = dec,
  abstract = {We present a novel unsupervised learning framework for single view depth estimation using monocular videos. It is well known in 3D vision that enlarging the baseline can increase the depth estimation accuracy, and jointly optimizing a set of camera poses and landmarks is essential. In previous monocular unsupervised learning frameworks, only part of the photometric and geometric constraints within a sequence are used as supervisory signals. This may result in a short baseline and overfitting. Besides, previous works generally estimate a low resolution depth from a low resolution impute image. The low resolution depth is then interpolated to recover the original resolution. This strategy may generate large errors on object boundaries, as the depth of background and foreground are mixed to yield the high resolution depth. In this paper, we introduce a bundle adjustment framework and a super-resolution network to solve the above two problems. In bundle adjustment, depths and poses of an image sequence are jointly optimized, which increases the baseline by establishing the relationship between farther frames. The super resolution network learns to estimate a high resolution depth from a low resolution image. Additionally, we introduce the clip loss to deal with moving objects and occlusion. Experimental results on the KITTI dataset show that the proposed algorithm outperforms the state-of-the-art unsupervised methods using monocular sequences, and achieves comparable or even better result compared to unsupervised methods using stereo sequences.},
  archivePrefix = {arXiv},
  eprint = {1812.03368},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Unsupervised Learning of Monocular Depth Estimation with Bundle Adjustment,-Zhou et al-2018.pdf},
  journal = {arXiv:1812.03368 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhouVisualLearningDirect,
  title = {Visual {{Learning Beyond Direct Supervision}}},
  author = {Zhou, Tinghui},
  pages = {132},
  file = {/Users/sunjiaming/Zotero/storage/J573PBGH/Zhou - Visual Learning Beyond Direct Supervision.pdf},
  language = {en}
}

@article{zhouVoxelNetEndtoEndLearning2017,
  title = {{{VoxelNet}}: {{End}}-to-{{End Learning}} for {{Point Cloud Based 3D Object Detection}}},
  shorttitle = {{{VoxelNet}}},
  author = {Zhou, Yin and Tuzel, Oncel},
  year = {2017},
  month = nov,
  abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
  archivePrefix = {arXiv},
  eprint = {1711.06396},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/VoxelNet-Zhou_Tuzel-2017.pdf;/Users/sunjiaming/Zotero/storage/A6SKNCMT/VoxelNet-Zhou_Tuzel-2017.pdf;/Users/sunjiaming/Zotero/storage/PDCANW7C/1711.html},
  journal = {arXiv:1711.06396 [cs]},
  keywords = {3d detection},
  language = {en},
  primaryClass = {cs}
}

@article{zhuDeepFeatureFlow2016,
  title = {Deep {{Feature Flow}} for {{Video Recognition}}},
  author = {Zhu, Xizhou and Xiong, Yuwen and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
  year = {2016},
  month = nov,
  abstract = {Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition.},
  archivePrefix = {arXiv},
  eprint = {1611.07715},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Deep Feature Flow for Video Recognition-Zhu et al-2016.pdf;/Users/sunjiaming/Zotero/storage/AR9VNTW3/1611.html},
  journal = {arXiv:1611.07715 [cs]},
  primaryClass = {cs}
}

@article{zhuDeformableConvNetsV22018,
  title = {Deformable {{ConvNets}} v2: {{More Deformable}}, {{Better Results}}},
  shorttitle = {Deformable {{ConvNets}} V2},
  author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
  year = {2018},
  month = nov,
  abstract = {The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of RCNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.},
  archivePrefix = {arXiv},
  eprint = {1811.11168},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Deformable ConvNets v2-Zhu et al-2018.pdf},
  journal = {arXiv:1811.11168 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhuDeformableDETRDeformable2020,
  title = {Deformable {{DETR}}: {{Deformable Transformers}} for {{End}}-to-{{End Object Detection}}},
  shorttitle = {Deformable {{DETR}}},
  author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  year = {2020},
  month = oct,
  abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10\$\textbackslash times\$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released.},
  archivePrefix = {arXiv},
  eprint = {2010.04159},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/Zotero/storage/DYAXR9SQ/Zhu et al. - 2020 - Deformable DETR Deformable Transformers for End-t.pdf;/Users/sunjiaming/Zotero/storage/ZKMSM973/2010.html},
  journal = {arXiv:2010.04159 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhuDistractorawareSiameseNetworks2018,
  title = {Distractor-Aware {{Siamese Networks}} for {{Visual Object Tracking}}},
  author = {Zhu, Zheng and Wang, Qiang and Li, Bo and Wu, Wei and Yan, Junjie and Hu, Weiming},
  year = {2018},
  month = aug,
  abstract = {Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-thearts, yielding 9.6\% relative gain in VOT2016 dataset and 35.9\% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks. The code is available at https://github.com/foolwood/DaSiamRPN.},
  archivePrefix = {arXiv},
  eprint = {1808.06048},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/Distractor-aware Siamese Networks for Visual Object Tracking-Zhu et al-2018.pdf},
  journal = {arXiv:1808.06048 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhuFlowGuidedFeatureAggregation2017,
  title = {Flow-{{Guided Feature Aggregation}} for {{Video Object Detection}}},
  author = {Zhu, Xizhou and Wang, Yujie and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
  year = {2017},
  month = mar,
  abstract = {Extending state-of-the-art object detectors from image to video is challenging. The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. Existing work attempts to exploit temporal information on box level, but such methods are not trained end-to-end. We present flow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. It improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. Our method significantly improves upon strong single-frame baselines in ImageNet VID, especially for more challenging fast moving objects. Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges 2016, without additional bells-and-whistles. The proposed method, together with Deep Feature Flow, powered the winning entry of ImageNet VID challenges 2017. The code is available at https://github.com/msracver/Flow-Guided-Feature-Aggregation.},
  archivePrefix = {arXiv},
  eprint = {1703.10025},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Flow-Guided Feature Aggregation for Video Object Detection-Zhu et al-2017.pdf;/Users/sunjiaming/Zotero/storage/AXM6K5H6/1703.html},
  journal = {arXiv:1703.10025 [cs]},
  primaryClass = {cs}
}

@article{zhuHighPerformanceVideo2017,
  title = {Towards {{High Performance Video Object Detection}}},
  author = {Zhu, Xizhou and Dai, Jifeng and Yuan, Lu and Wei, Yichen},
  year = {2017},
  month = nov,
  abstract = {There has been significant progresses for image object detection in recent years. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios.},
  archivePrefix = {arXiv},
  eprint = {1711.11577},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Towards High Performance Video Object Detection-Zhu et al-2017.pdf},
  journal = {arXiv:1711.11577 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{zhuObjectCentricPhotometricBundle2018,
  title = {Object-{{Centric Photometric Bundle Adjustment}} with {{Deep Shape Prior}}},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Zhu, Rui and Wang, Chaoyang and Lin, Chen-Hsuan and Wang, Ziyan and Lucey, Simon},
  year = {2018},
  month = mar,
  pages = {894--902},
  publisher = {{IEEE}},
  address = {{Lake Tahoe, NV}},
  doi = {10.1109/WACV.2018.00103},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object-Centric Photometric Bundle Adjustment with Deep Shape Prior-Zhu et al-2018.pdf;/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Object-Centric Photometric Bundle Adjustment with Deep Shape Prior-Zhu et al-2018.pdf},
  isbn = {978-1-5386-4886-5},
  language = {en}
}

@article{zhuSoftAnchorPointObject2019,
  title = {Soft {{Anchor}}-{{Point Object Detection}}},
  author = {Zhu, Chenchen and Chen, Fangyi and Shen, Zhiqiang and Savvides, Marios},
  year = {2019},
  month = nov,
  abstract = {Recently, anchor-free detectors have shown great potential to outperform anchor-based detectors in terms of both accuracy and speed. In this work, we aim at finding a new balance of speed and accuracy for anchor-free detectors. Two questions are studied: 1) how to make the anchor-free detection head better? 2) how to utilize the power of feature pyramid better? We identify attention bias and feature selection as the main issues for these two questions respectively. We propose to address these issues with a novel training strategy that has two soften optimization techniques, i.e. soft-weighted anchor points and soft-selected pyramid levels. To evaluate the effectiveness, we train a single-stage anchor-free detector called Soft Anchor-Point Detector (SAPD). Experiments show that our concise SAPD pushes the envelope of speed/accuracy trade-off to a new level, outperforming recent state-of-the-art anchor-based and anchor-free, single-stage and multi-stage detectors. Without bells and whistles, our best model can achieve a single-model single-scale AP of 47.4\% on COCO. Our fastest version can run up to 5x faster than other detectors with comparable accuracy.},
  archivePrefix = {arXiv},
  eprint = {1911.12448},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Soft Anchor-Point Object Detection-Zhu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/UPUL49AZ/1911.html},
  journal = {arXiv:1911.12448 [cs]},
  primaryClass = {cs}
}

@article{zhuUnifiedINT8Training2019,
  title = {Towards {{Unified INT8 Training}} for {{Convolutional Neural Network}}},
  author = {Zhu, Feng and Gong, Ruihao and Yu, Fengwei and Liu, Xianglong and Wang, Yanfei and Li, Zhelong and Yang, Xiuqi and Yan, Junjie},
  year = {2019},
  month = dec,
  abstract = {Recently low-bit (e.g., 8-bit) network quantization has been extensively studied to accelerate the inference. Besides inference, low-bit training with quantized gradients can further bring more considerable acceleration, since the backward process is often computation-intensive. Unfortunately, the inappropriate quantization of backward propagation usually makes the training unstable and even crash. There lacks a successful unified low-bit training framework that can support diverse networks on various tasks. In this paper, we give an attempt to build a unified 8-bit (INT8) training framework for common convolutional neural networks from the aspects of both accuracy and speed. First, we empirically find the four distinctive characteristics of gradients, which provide us insightful clues for gradient quantization. Then, we theoretically give an in-depth analysis of the convergence bound and derive two principles for stable INT8 training. Finally, we propose two universal techniques, including Direction Sensitive Gradient Clipping that reduces the direction deviation of gradients and Deviation Counteractive Learning Rate Scaling that avoids illegal gradient update along the wrong direction. The experiments show that our unified solution promises accurate and efficient INT8 training for a variety of networks and tasks, including MobileNetV2, InceptionV3 and object detection that prior studies have never succeeded. Moreover, it enjoys a strong flexibility to run on off-the-shelf hardware, and reduces the training time by 22\% on Pascal GPU without too much optimization effort. We believe that this pioneering study will help lead the community towards a fully unified INT8 training for convolutional neural networks.},
  archivePrefix = {arXiv},
  eprint = {1912.12607},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Towards Unified INT8 Training for Convolutional Neural Network-Zhu et al-2019.pdf;/Users/sunjiaming/Zotero/storage/JSDUXHP9/1912.html},
  journal = {arXiv:1912.12607 [cs]},
  primaryClass = {cs}
}

@article{zimmer3DBATSemiAutomatic2019,
  title = {{{3D BAT}}: {{A Semi}}-{{Automatic}}, {{Web}}-Based {{3D Annotation Toolbox}} for {{Full}}-{{Surround}}, {{Multi}}-{{Modal Data Streams}}},
  shorttitle = {{{3D BAT}}},
  author = {Zimmer, Walter and Rangesh, Akshay and Trivedi, Mohan},
  year = {2019},
  month = may,
  abstract = {In this paper, we focus on obtaining 2D and 3D labels, as well as track IDs for objects on the road with the help of a novel 3D Bounding Box Annotation Toolbox (3D BAT). Our open source, web-based 3D BAT incorporates several smart features to improve usability and efficiency. For instance, this annotation toolbox supports semi-automatic labeling of tracks using interpolation, which is vital for downstream tasks like tracking, motion planning and motion prediction. Moreover, annotations for all camera images are automatically obtained by projecting annotations from 3D space into the image domain. In addition to the raw image and point cloud feeds, a Masterview consisting of the top view (bird's-eye-view), side view and front views is made available to observe objects of interest from different perspectives. Comparisons of our method with other publicly available annotation tools reveal that 3D annotations can be obtained faster and more efficiently by using our toolbox.},
  archivePrefix = {arXiv},
  eprint = {1905.00525},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D BAT-Zimmer et al-2019.pdf;/Users/sunjiaming/Zotero/storage/BKXV9P8I/1905.html},
  journal = {arXiv:1905.00525 [cs]},
  primaryClass = {cs}
}

@article{zollhoferStateArt3D2018,
  title = {State of the {{Art}} on {{3D Reconstruction}} with {{RGB}}-{{D Cameras}}},
  author = {Zollh{\"o}fer, Michael and Stotko, Patrick and G{\"o}rlitz, Andreas and Theobalt, Christian and Nie{\ss}ner, Matthias and Klein, Reinhard and Kolb, Andreas},
  year = {2018},
  month = may,
  volume = {37},
  pages = {625--652},
  issn = {01677055},
  doi = {10.1111/cgf.13386},
  abstract = {The advent of affordable consumer grade RGB-D cameras has brought about a profound advancement of visual scene reconstruction methods. Both computer graphics and computer vision researchers spend significant effort to develop entirely new algorithms to capture comprehensive shape models of static and dynamic scenes with RGB-D cameras. This led to significant advances of the state of the art along several dimensions. Some methods achieve very high reconstruction detail, despite limited sensor resolution. Others even achieve real-time performance, yet possibly at lower quality. New concepts were developed to capture scenes at larger spatial and temporal extent. Other recent algorithms flank shape reconstruction with concurrent material and lighting estimation, even in general scenes and unconstrained conditions. In this state-of-the-art report, we analyze these recent developments in RGB-D scene reconstruction in detail and review essential related work. We explain, compare, and critically analyze the common underlying algorithmic concepts that enabled these recent advancements. Furthermore, we show how algorithms are designed to best exploit the benefits of RGB-D data while suppressing their often non-trivial data distortions. In addition, this report identifies and discusses important open research questions and suggests relevant directions for future work.},
  file = {/Users/sunjiaming/OneDrive/OneDrive\\ -\\ 南方科技大学/Zotero-Papers/State of the Art on 3D Reconstruction with RGB-D Cameras-Zollhöfer et al-2018.pdf},
  journal = {Computer Graphics Forum},
  language = {en},
  number = {2}
}

@misc{zollhoferStateArt3D2018a,
  title = {State of the {{Art}} on {{3D Reconstruction}} with {{RGB}}-{{D Cameras}}},
  author = {Zollh{\"o}fer, Michael and Stotko, Patrick and G{\"o}rlitz, Andreas and Theobalt, Christian and Nie{\ss}ner, Matthias and Klein, Reinhard and Kolb, Andreas},
  year = {2018},
  abstract = {The advent of affordable consumer grade RGB-D cameras has brought about a profound advancement of visual scene reconstruction methods. Both computer graphics and computer vision researchers spend significant effort to develop entirely new algorithms to capture comprehensive shape models of static and dynamic scenes with RGB-D cameras. This led to significant advances of the state of the art along several dimensions. Some methods achieve very high reconstruction detail, despite limited sensor resolution. Others even achieve real-time performance, yet possibly at lower quality. New concepts were developed to capture scenes at larger spatial and temporal extent. Other recent algorithms flank shape reconstruction with concurrent material and lighting estimation, even in general scenes and unconstrained conditions. In this state-of-the-art report, we analyze these recent developments in RGB-D scene reconstruction in detail and review essential related work. We explain, compare, and critically analyze the common underlying algorithmic concepts that enabled these recent advancements. Furthermore, we show how algorithms are designed to best exploit the benefits of RGB-D data while suppressing their often non-trivial data distortions. In addition, this report identifies and discusses important open research questions and suggests relevant directions for future work. CCS Concepts \textbullet Computing methodologies , . . ., Reconstruction; Appearance and texture representations; Motion capture;},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/State of the Art on 3D Reconstruction with RGB-D Cameras-Zollhöfer et al-2018.pdf;/Users/sunjiaming/Zotero/storage/LI5BC25Z/484024a91c1cb97b358c2cabd9fa427fcc6d52e3.html},
  howpublished = {/paper/State-of-the-Art-on-3D-Reconstruction-with-RGB-D-Zollh\%C3\%B6fer-Stotko/484024a91c1cb97b358c2cabd9fa427fcc6d52e3},
  journal = {undefined},
  language = {en}
}

@article{zuffi3DMenagerieModeling2017,
  title = {{{3D Menagerie}}: {{Modeling}} the {{3D}} Shape and Pose of Animals},
  shorttitle = {{{3D Menagerie}}},
  author = {Zuffi, Silvia and Kanazawa, Angjoo and Jacobs, David and Black, Michael J.},
  year = {2017},
  month = apr,
  abstract = {There has been significant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fit to data. We demonstrate generalization by fitting it to images of real animals including species not seen in training.},
  archivePrefix = {arXiv},
  eprint = {1611.07700},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/3D Menagerie-Zuffi et al-2017.pdf;/Users/sunjiaming/Zotero/storage/R97WMYSL/1611.html},
  journal = {arXiv:1611.07700 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zuffiThreeDSafariLearning2019,
  title = {Three-{{D Safari}}: {{Learning}} to {{Estimate Zebra Pose}}, {{Shape}}, and {{Texture}} from {{Images}} "{{In}} the {{Wild}}"},
  shorttitle = {Three-{{D Safari}}},
  author = {Zuffi, Silvia and Kanazawa, Angjoo and {Berger-Wolf}, Tanya and Black, Michael J.},
  year = {2019},
  month = aug,
  abstract = {We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other. To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. Learning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision. Moreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. We show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. Code and data are available at https://github.com/silviazuffi/smalst.},
  archivePrefix = {arXiv},
  eprint = {1908.07201},
  eprinttype = {arxiv},
  file = {/Users/sunjiaming/OneDrive/OneDrive - 南方科技大学/Zotero-Papers/Three-D Safari-Zuffi et al-2019.pdf;/Users/sunjiaming/Zotero/storage/HMJYKJVT/1908.html},
  journal = {arXiv:1908.07201 [cs]},
  primaryClass = {cs}
}


